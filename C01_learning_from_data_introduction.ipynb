{
 "metadata": {
  "name": "",
  "signature": "sha256:29228d33929715613a2336bbf2057717e9a81b14ee55323413e734ab8487efad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id='TOP'> </a> Introduction\n",
      "There are huge amount of learning methods and it is easy to get lost. This notebook present selected aspects of learning theory that helped me in organizing the various methods. This notebooks is heavily inspired by two excellent online courses: [Machine learning](https://www.coursera.org/course/machlearning)  by [**Pedro Domingos**](http://homes.cs.washington.edu/~pedrod/)  and \n",
      "[Learnind from data](http://work.caltech.edu/telecourse.html) by [**Yaser Abu-Mostafa**](https://work.caltech.edu/index.html). I present here only the part of the theory that I find to be useful to my subjective understanding. Look at the [resources](#LearningTheoryResources) section for more references. \n",
      "\n",
      "\n",
      "# Contents\n",
      "* [<font color=\"red\">Definitions and terminology</font>](#Definitions)\n",
      "    * [The learning problem](#LearningProb) \n",
      "    * [Anatomy of a learning machine](#AnatomyOfLearningMachine)\n",
      "    * [Overfitting](#overfitting)\n",
      "      * [<font color=\"green\">Numerical example</font>](#overfittingExamplePolynomial)\n",
      "      * [<font color=\"green\">Detecting overfitting in learning curves</font>](#OverfittingExampleLearningCurves)\n",
      "    * [The curse of dimensionality](#TheCurseOfDimentionality)   \n",
      "    * [Types of learning problems](#ClassificationOfLearningMethods) \n",
      "* [<font color=\"red\">Theories of learning</font>](#TheoryLearning) \n",
      "    * [No free lunch theorems](#NoFreeLunchTheories)\n",
      "    * [Bias-Variance tradeoff](#BiasVariance)\n",
      "        * [<font color=\"green\">Bias-Variance demonstration</font>](#BiasVarianceExample)  \n",
      "        * [<font color=\"green\">Direct estimation of Bias and Variance</font>](#ApproximatingSine1)\n",
      "    * [PAC learning: Consistent hypothesis space, agnostic learning, VC-dimension](#PACLearning)\n",
      "        * [<font color=\"green\">Numerical example: PAC learning</font>](#PACLearningExample)\n",
      "* [<font color=\"red\">Random thoughts on selecting and using learning models</font>](#RandomThoughts)\n",
      "    * [Model selection](#ModelSelection) \n",
      "        * [Procedures for controlling the complexity of learning models](#ComplexityControl)\n",
      "        * [Validation and cross-validation](#validation)\n",
      "           * [<font color=\"green\">Illustrative example: Why validation is optimistic estimation of error</font>](#validationExample)\n",
      "    * [Selecting the loss function](#LossFunctionSelection)\n",
      "      *  [Popular loss function for regression](#PopularLossFunctionRegression)\n",
      "      *  [Popular loss function for classification](#PopularLossFunctionClassification) \n",
      "    * [Combating overfitting](#CombatingOverFitting)\n",
      "        * [Reducing the complexity of the hypothesis set by regularization](#regularization)\n",
      "            * [<font color=\"green\">Example of regulatization</font>](#regularizationExamplePolynomial)\n",
      "        * [Reducing the number of features](#ReducingNumberOfFeatures)\n",
      "    * [Combating the curse of dimentionality](#CombatingCurseDim)\n",
      "        * [Feature engineering](#FeatureEng)\n",
      "        * [Feature selection](#FeatureSel)\n",
      "        * [Dimensionality reduction](#DimRed)\n",
      "    * [Assesing model performance](#AssesingModelQuality)\n",
      "    * [Features scaling](#FeaturesScaling)\n",
      "    * [Ensemble learning](#EnsembleLearning)\n",
      "* [Resources](#LearningTheoryResources)\n",
      "  * [Courses](#ResourcesCourses) \n",
      "  * [Books](#ResourcesBooks) \n",
      "  * [Papers, Slides and sites](#ResourcesPapersSlidesSites)\n",
      "  * [Competitions](#ResourcesCompetitions)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <font color=\"red\"><a id=\"Definitions\"></a> Definitions and terminology</font>\n",
      "Machine learning is an interdeciplinary field studied by researchers coming from many different fields. One unfortunate consequence is that the same concept is often referred to by different names. In this section I will present the basic definitions of the learning problems. I will try, as much as I can, to give the different names for each concept that I present. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id=\"LearningProb\"></a> The learning problem\n",
      "The are many definitions of the learning-from-data problem, each from a slightly different perspective. All definitions are quite similar and composed of the following ingredients:\n",
      "\n",
      "### <font color=\"brown\">A data generator </font>\n",
      "> Generates $N$ input points $\\mathbf{x_i} \\in \\mathcal{X} \\subset \\mathbb{R}^p\\quad, i =1,\\ldots,N$ that are independently and idendically distributed according to an **unknown** fixed distribution $P_X(x)$. The $\\mathbf{x_i}$'s are also called **instances** (and then $\\mathcal{X}$ is called the instance space). The coordinates of $\\mathbf{x_i}$ are called\n",
      "**features**, **predictors** or **attributes**.\n",
      "\n",
      "### <font color=\"brown\">A target operator</font>\n",
      "\n",
      "> Transforms input values into a **response**, or **target**, value $y \\in \\mathcal{Y}$ according to an **unknown** conditional probability distribution $P_{Y\\mid X}(y\\mid x)$. \n",
      "\n",
      "> The learning problem is a **regression** problem when $\\mathcal{Y} \\subset \\mathbb{R}$ and a **classification** problem when the values of $y$ are from a discrete set $\\mathcal{Y}$ called the **label set**. The predictor $y$ in this case is, quite naturally, called a **label**. Binary classification, where $\\mathcal{Y}$ consists of just two values, is a particularly interesting special case often referred to as **concept learning**. The label will then referred to as **concept**.\n",
      "\n",
      ">The target operator is sometimes realized by a simpler model with $y=f(x)$ where $f(x)$ is a deterministic function or by $y=f(x)+\\epsilon$ where $\\epsilon$ is a noise term with zero mean. This makes sense when we have reson to believe that the data is indeed generated from some function, perhaps with a known functional form or having known smoothness properties.\n",
      "    \n",
      "   \n",
      "\n",
      "### <font color=\"brown\">A training set</font>\n",
      ">$\\mathcal{D} = \\{(\\mathbf{x_i},y_i) ,i = 1,\\ldots,N\\}$ where  $(\\mathbf{x_i},y_i)$ are independetly and identically  distributed according to the **unknown**  joint distribution $P_{X,Y}(x,y)=P_{Y\\mid X}(y\\mid x)P_X(x)$. Elements of training set are referred to as **samples** or **examples**.\n",
      "\n",
      "### <font color=\"brown\">A learning machine</font>\n",
      ">An algorithm which, on the basis of the training set, returns a **prediction rule** $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ that estimates the value of the target function for each value of an input point $\\mathbf{x}$. This function is called a **predictor**, **hypothesis**, **model** or a **classifier**. Learning machine is also often referred to as just a **learner**, \n",
      "\n",
      "### What is actually learning?\n",
      "Intuitivly we would like the hypothesis $h$ to be \"close\" to the function $f$ or, more generally, we would like $(\\mathbf{x},h(\\mathbf{x}))$ to be close to the joint distribution $P_{X,Y}(x,y)$ on <font, color=\"red\">**new**</font> unseen, data from $\\mathcal{X}$. This is  what we call <font, color=\"blue\">**generalization**</font> or <font, color=\"blue\">**learning**</font> ."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"AnatomyOfLearningMachine\">Anatomy</span> of a learning machine\n",
      "\n",
      "Pedro Domingos argues in his paper **\"A Few Useful Things to Know about Machine Learning\"** ([download here](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)) that any learning machine can be considered as being composed of just three components: \n",
      "\n",
      "> A **Representation** of the **Hypothesis set (or space)**, **Evaluation** of a **loss function** and an **Optimization** procedure for finding the best candidate out of the hypothesis set that minimizes the loss function.\n",
      "\n",
      "### <font color=\"brown\">An hypothesis set</font>\n",
      "A set $\\mathcal{H}$ of predictors $h : \\mathcal{X} \\mapsto \\mathcal{Y}$ from which we select a particular estimator to the target operator. Many times the selection will be performed by minimizing the empirical risk, a term described below.\n",
      "\n",
      "###<font color=\"brown\">The evaluation (loss) functions</font>\n",
      "An evaluation function, also called **objective** or **scoring** function is a function that is used to rank different hypetheses from the hypothesis set. Many evaluation functions are proposed in the literature including [Precision and recall](http://en.wikipedia.org/wiki/Precision_and_recall), [likelihood](http://en.wikipedia.org/wiki/Maximum_likelihood). [posteriori probability](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), [information gain](http://www.autonlab.org/tutorials/infogain.html), [K-L divergence](http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) and [margin](http://en.wikipedia.org/wiki/Margin_classifier) to name a few.\n",
      "\n",
      "I will concentrate below on a special type of evaluation functions called **loss functions**. A loss function is a non-negative function $L(h({\\mathbf x}),y))$ defined for functions $h : \\mathcal{X} \\mapsto \\mathcal{Y}$ that measure the discrepancy between the value predicted by $h$ for point ${\\mathbf x}$ and $y$. Loss function is sometimes also called a **utility** , **penalty** or **cost** function.\n",
      "\n",
      "The loss function is used to define **the expected risk** \n",
      "> $R(h) = \\int L(h(\\mathbf x),y) P_{X,Y}(x,y)dxdy$ \n",
      "\n",
      "also called **out-of-sample**, **test**, **generalization** or **true** error and denoted by $E_{\\tt{out}}$.  Ideally we would like to select the \"best\" predictor $g_o \\in \\mathcal{G}$ so that \n",
      "> $g_o = \\tt{arg min}_{g \\in \\mathcal{G}} R(g)$.\n",
      "\n",
      "where $\\mathcal{G}$ is the space of measurable functions for which $R(g)$ is well defined. In some cases the loss function [have specific semantics](http://hunch.net/?p=269) which allow us to precisely characterize $g_o$. For example, for a square loss function $L(h({\\mathbf x}),y))=(h(x)-y)^2$ we have that $g_o(x)=\\mathbb{E}( Y\\mid X= x)$, Thus the best prediction of $Y$ for any $X=x$ is the conditional mean. \n",
      "\n",
      "Where the learning problem is formulated in terms of a target function $f$ then we have that  $g_o = f$.\n",
      "\n",
      "A unique characteristic of the learning problem is that we wish to minimize a quantity that we can not compute. In practice we will replace the expected risk by **the empirical risk**, namely the risk computed on the data set\n",
      "> $R_{\\tt emp}(h) = \\frac{1}{N}\\sum_{i=1}^N L(h(\\mathbf x_i),y_i)$  \n",
      "\n",
      "also called **in-sample** or **training**  error and denoted by $E_{\\tt in}$.\n",
      "\n",
      " The loss function determine the nature of the learning algorithm and is thus of great interest. In <a href=\"#LossFunctionSelection\" data-ajax=\"false\">this section below</a> we further discuss the question selecting the loss function and present some popular alternatives.\n",
      "\n",
      "### <font color=\"brown\">The learning algorithm - Empirical risk minimization</font>\n",
      "We will search for the \"best\" candidate from the hypothesis set, that is $g\\in \\mathcal{H}$ so that \n",
      ">$g = \\tt{arg min}_{h \\in \\mathcal{H}} R_{\\tt emp}(h)$\n",
      "\n",
      "A central problem is to find conditions under which $g$ mimics the behavior of $g_o$. However, the above minimization problem is often ill posed so we use the [regularization theory](http://en.wikipedia.org/wiki/Regularization_(mathematics) to make the problem solvable. In practice we will add smoothness constraints on the solution: Instead of minimizing the empirical risk we minimize\n",
      "> $\\frac{1}{N}\\sum_{i=1}^N L(h(\\mathbf x_i),y_i) + \\lambda J(h)$ \n",
      "\n",
      "where $J(h)$ is a smoothness functional. For example consider and hypothesis set $\\mathcal{H}$ consisting of linear functions of the form $h=\\mathbf{ w^T x}$. In this case, $J(h) = ||\\mathbf{w}||^2$ and the resulting problem is called **ridge regression**.\n",
      "    \n",
      "### <font color=\"brown\">The optimization method</font>\n",
      "Once a loss function is specified we need to search the hypothesis space for the hypothesis that minimizes the loss. Optimization for machine learning is a very active research area and there are many methods investigated depending on the type of learning problem and nature of the loss function. There is a [Workshop](http://opt-ml.org) dedicated to optimization for machine learning and great part of most machine learning algorithms are devoted to efficient implementations of optimization techniques. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id=\"overfitting\"></a>Overfitting\n",
      "\n",
      "The whole purpose of machine learning is to find a learner that performs well on yet **unseen** data. [Overfitting](http://en.wikipedia.org/wiki/Overfitting) is the most severe problem when trying to delevop a good learning model. \n",
      "\n",
      "Overfitting is a **comparative** term that arises when evaluating the performance of competing models. The problem is that when looking at the in-sample error we might think that a model is doing extremly well  while the out-of-sample error actually increases as compared to another model. In the extreme case, we might have a model that provide a predictor that attains a zero in-sample error but its performance on the test set is much worse than another model which attains some error on the training set but much lower error on the test set (we will see a computational example below).\n",
      "\n",
      "In the case of overfitting it is \n",
      "[Inductive bias](http://en.wikipedia.org/wiki/Inductive_bias) that comes to our rescue. We use our knowledge of the domain to properly select the hypothesis set and the loss function. This shoulde be done **without** viewing the data (bacause if we view the data we will tune our selection to the samples that we see and thus we will be likely overfit). An extreme case of inductive bias is when we know exactly what program to write to predict. Thus the hypothesis set will consist of just one fixed function. As [Eliezer Yudkowsky note](http://lesswrong.com/lw/hg/inductive_bias/): *\"Inductive biases can be probabilistically correct or probabilistically incorrect, and if they are correct, it is good to have as much of them as possible, and if they are incorrect, you are left worse off than if you had no inductive bias at all.\"*. \n",
      "\n",
      "The purpose of the theory of learning is to provide us tools to help in selecting the hypothesis set and loss function and to give us some idea as to what size of training set do we need to get descent generalization error. As we will see, learning theory actually guarantees that no single method consistently outperforms other methods. Thus we need deep domain knowledge and rich set of learning methods so we can hope to get a method that is good for our problem. We need to accept that it well may be that no existing method provide good generalization for our problem and we will have to work hard on developing such a method.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"green\"><a id=\"overfittingExamplePolynomial\"></a>Numerical example</font>\n",
      "We will consider a test function that is a polynomial of degree 15 from which we sample 12 points. The graph of  the test function is plotted below in <font color=\"blue\">**blue color**</font> and the sample points are plotted as blue dots superimposed on this graph."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.polynomial.legendre as lg\n",
      "import matplotlib.pyplot as plt     \n",
      "%pylab inline\n",
      "\n",
      "\n",
      "def poly15(x):\n",
      "    coef = [ 1.33164947 , -0.48120244, -1.93454292 , 0.79024579 , 2.33241528 , 0.27261564,\\\n",
      "              1.92751477 , 2.44851315 , 1.20674012,-3.04869176 , 0.21597876 , 1.08214072, \\\n",
      "              1.13487548 , 0.6082264 , -2.91413442]\n",
      "    return lg.legval(x,coef)\n",
      "    \n",
      "xp = np.linspace(-1, 1, 100)\n",
      "yp = poly15(xp)\n",
      "plt.plot(xp,poly15(xp))\n",
      "\n",
      "# Sampled\n",
      "xsampled = np.linspace(-1, 1, 12)\n",
      "ysampled = poly15(xsampled)\n",
      "plt.scatter(xsampled,ysampled,c='b',marker='o',s=30)\n",
      "\n",
      "plt.title('Test funcion: Polynomial of order 15')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<matplotlib.text.Text at 0x9d56a90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC+jGGJMnLKAbY0ye+P/dOHQsDNPYRQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x9913fd0>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We consider two competing models, or hypothesis, for learning our test function:\n",
      "1. A polynomial of the second degree (the <font color=\"green\">**green curve**</font>) and \n",
      "2. A polynomial of order 12 (the <font color=\"red\">**red curve**</font>)\n",
      "\n",
      "and use ordinary least squares (i.e. a square loss function) to estimate the parameters of the model. \n",
      "\n",
      "The twelve order polynomial interpolates the data so its in-sample error is zero. But the graph of this model is very wiggly. On the other hand the in-sample error of the parabola approximation is far from zero but the approximation is very smooth and its out-of-sample error is much better that the out-of-sample-error of the 12-order polynomial model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import warnings\n",
      "warnings.simplefilter('ignore', np.RankWarning)\n",
      "\n",
      "def outError(target,approx):\n",
      "    M = 10000\n",
      "    x  = numpy.random.uniform(low=-1.0, high=1.0, size=M)\n",
      "    e  = np.mean( (target(x)-approx(x))**2 )\n",
      "    return e\n",
      "\n",
      "# Plot test function and sample points\n",
      "plt.plot(xp,poly15(xp))\n",
      "plt.scatter(xsampled,ysampled,c='b',marker='o',s=30)\n",
      "\n",
      "# Hyphothesis H2 : Second order polynomial\n",
      "p_coeff_2         = np.polyfit(xsampled,ysampled,2) \n",
      "poly_fit_2        = np.poly1d(p_coeff_2)\n",
      "in_sample_error_2 = np.mean( (poly_fit_2(xsampled)-ysampled)**2 )\n",
      "plt.plot(xp,poly_fit_2(xp),'g',label='poly-2')\n",
      "\n",
      "# Hyphothesis H12 : 12 order polynomial\n",
      "p_coeff_12         = np.polyfit(xsampled,ysampled,12) \n",
      "poly_fit_12        = np.poly1d(p_coeff_12) \n",
      "in_sample_error_12 = np.mean( (poly_fit_12(xsampled)-ysampled)**2 )\n",
      "plt.plot(xp,poly_fit_12(xp),'r',label='poly-12')\n",
      "\n",
      "plt.legend()\n",
      "plt.xlabel('x')\n",
      "plt.title('Approximating 15th order polynomial')\n",
      "plt.ylim([-2.,6.])\n",
      "\n",
      "print 'Train set of 12 points\\n',70*'-'\n",
      "print 'In-sample error for second order polynomial     ',in_sample_error_2\n",
      "print 'In-sample error for 12     order polynomial     ',in_sample_error_12\n",
      "print ''\n",
      "print 'Out-of-sample error for second order polynomial ',outError(poly15,poly_fit_2)\n",
      "print 'Out-of-sample error for 12     order polynomial ',outError(poly15,poly_fit_12)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train set of 12 points\n",
        "----------------------------------------------------------------------\n",
        "In-sample error for second order polynomial      3.52762386015\n",
        "In-sample error for 12     order polynomial      1.02870814956e-25\n",
        "\n",
        "Out-of-sample error for second order polynomial  3.12549233925\n",
        "Out-of-sample error for 12     order polynomial  19.4024344311\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEZCAYAAACHCd7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa134ba8>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now increase the size of the data set from 12 to 120. Now we can see that the additional complexity of the 12-order polynomial model pays off and it achieves a much improved out-of-sample error as compared to the out-of-sample error of the second error polynomial model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xsampled120     = np.linspace(-1, 1, 120)\n",
      "ysampled120     = poly15(xsampled120)\n",
      "p_coeff_2       = np.polyfit(xsampled120,ysampled120,2) \n",
      "poly_fit_2      = np.poly1d(p_coeff_2) \n",
      "in_sample_error_2_120 = np.mean( (poly_fit_2(xsampled120)-ysampled120)**2 )\n",
      "\n",
      "p_coeff_12      = np.polyfit(xsampled120,ysampled120,12) \n",
      "poly_fit_12     = np.poly1d(p_coeff_12) \n",
      "in_sample_error_12_120 = np.mean( (poly_fit_12(xsampled120)-ysampled120)**2 )\n",
      "\n",
      "print '\\nTrain set of =120 points\\n',70*'-'\n",
      "print 'Out-of-sample error for second order polynomial ',outError(poly15,poly_fit_2)\n",
      "print 'Out-of-sample error for 12     order polynomial ',outError(poly15,poly_fit_12)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train set of =120 points\n",
        "----------------------------------------------------------------------\n",
        "Out-of-sample error for second order polynomial  2.31732177219\n",
        "Out-of-sample error for 12     order polynomial  0.330674244824\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "> ## Lesson learned: <font color=\"blue\">One should select the complexity of the model based on the size of the data set and not on the complexity of the test function that is approximated</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"DD\"></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <font color=\"green\"><span id=\"OverfittingExampleLearningCurves\">Detecting</span> overfitting using learning curves</font>\n",
      "In this demosntration the data set is composed of 60 points sampled from a polynomial of order 15. we compare the in-sample and out-of-sample errors by different polynomial models of orders ranging from 3 to 20. As the order of the model increases the in-sample error always decreases **but** at some point the out-of-sample error starts to increase. This is the point where the problem of overfitting manifests itself."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N           = []\n",
      "Ein         = []\n",
      "Eout        = []\n",
      "\n",
      "target_func = poly15\n",
      "deg         = 12\n",
      "\n",
      "Ntest  = 10000\n",
      "numpy.random.seed(seed=7)\n",
      "perm   = np.random.permutation(Ntest)\n",
      "xtest  = np.linspace(-1, 1, Ntest)[perm]\n",
      "ytest  = target_func(xtest)[perm]\n",
      "\n",
      "n_samples = 60\n",
      "for deg in xrange(3,20,1):    \n",
      "    xsample  =   xtest[:n_samples]\n",
      "    ysample  =   ytest[:n_samples]\n",
      "     \n",
      "    p_coeff           = np.polyfit(xsample,ysample,deg) \n",
      "    poly_fit          = np.poly1d(p_coeff) \n",
      "\n",
      "    poly_fit_y_sample = poly_fit(xsample)\n",
      "    poly_fit_y_test   = poly_fit(xtest)\n",
      "\n",
      "    err_out = np.sum( (ytest-poly_fit_y_test)**2)/float(len(ytest))\n",
      "    err_in  = np.sum( (ysample-poly_fit_y_sample)**2)/float(len(ysample))\n",
      "\n",
      "    N.append( deg )\n",
      "    Ein.append( err_in )\n",
      "    Eout.append(err_out )\n",
      "    \n",
      "plt.plot(N,Ein, 'g',label='Ein')\n",
      "plt.plot(N,Eout,'b',label='Eout')\n",
      "plt.title('Approximation by polynom of order 15, 60 sample points')\n",
      "plt.xlabel('Degree of approximating polynomial')\n",
      "plt.ylabel('Square error')\n",
      "plt.legend()\n",
      "plt.ylim([0,16])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "(0, 16)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+v8Irl0kAk6mMQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa099f60>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"TheCurseOfDimentionality\">The</span> curse of dimensionality\n",
      "\n",
      "Implementing machine learning methods becomes, sometimes exponentially harder, as the number of predictors, or attributes, increase. This [curse of dimentionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality) is a serious problem that should be addressed whenever implementing a machine learning procedure. The problem is that we often use our intuition, that is based on the three dimensional space in which we live, to reason about machine learning algorithms. This intuition fails to carry us onto high dimensional spaces. \n",
      "\n",
      "The most striking fact, IMHO,  is that most of the volume of high dimensional hyper-cube is located at its boundaries while our intuition tell us that most of the volume is at the interior of the hyper-cube. Let us consider some concrete examples:  \n",
      " * The volume of the unit sphere in $d$ dimensions is $\\frac{2\\pi^{d/2}}{\\Gamma(n/2)}$ which goes to zero as $d \\rightarrow \\infty$\n",
      " * Thus, the ratio between the volume of a circimscribing circle and the unit cube and goes to zero with the dimension which means that most of the volume of the unit cube is concenrated at its \"corners\" \n",
      " * Suppose that we have points uniformly sampled and we ask what is the lenght $l$ of the side of a hyper-cube that contains 10% of the points. We get that $l=0.1^{1/d}$ so for $d=1$ we have $l=0.1$ and for $d=10$ we have $l=0.83$. That means that in order to cover 10% of the points we need the length of each side of the hyper-cube to cover 83% of the range in each direction. \n",
      "\n",
      "There are also straighforward consequences of high dimensional spaces:\n",
      "* The number of parameters of many learning methods grow with the dimension which leads to:\n",
      "  * Increase in computation time for learning\n",
      "  * Need to increase the size of the training set to avoid overfitting\n",
      "* Number of training points have to grow exponentially with the dimension in order to keep a given sample density. For example, 10 points are needed to sample a unit interval so that adjacent points are at a distance of 0.1. If we want the same spacing between adjacent points for a $d$-dimensional cube then we will need $10^d$ points.\n",
      "* The notion of \"nearest neighbor\" in high dimension is very unintuitive. For example, the number of closest neighbors to a point on a $d$-dimensional grid is $2d$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"ClassificationOfLearningMethods\">Types</span> of learning problems\n",
      "There are many learning methods and they can be classified along different axes. In the following we present some of the approaches for grouping different learning methods:\n",
      "\n",
      "## <font, color=\"green\">  Main learning paradigms</font>\n",
      "\n",
      "* [Supervised learning](http://en.wikipedia.org/wiki/Supervised_learning), also called **inductive learning**. In this type of learning the training data includes desired labels. This is the most studied part of machine learning. One of the drawbacks of supervised learning is that when the creation of the labels is a costly operation, it limits the applicability of this approach only to data sets of modest size. Supervised learning can come in three main flavors:\n",
      "  * **Classification** - The output of learner is discrete.\n",
      "  * **Regression** - The output of the learner is continuous.\n",
      "  * **Probability estimation** - The output of the learner is probability.\n",
      "* [Unsupervised learning](http://en.wikipedia.org/wiki/Unsupervised_learning) is a type of learning where the training data does not include desired labels. \n",
      "* [Semi-supervised learning](http://en.wikipedia.org/wiki/Semi-supervised_learning) is a type of learning where some of the training samples include the desired labels. \n",
      "* [Active learning](http://en.wikipedia.org/wiki/Active_learning_(machine_learning) is a special case of semi-supervised learning in which the learning machine can requests the user to provide labels for desired samples.\n",
      "* [Reinforcement learning](http://en.wikipedia.org/wiki/Reinforcement_learning). This is a kind of a play where the player takes an action and is rewarded for success or punished for failure.\n",
      "\n",
      "## <font, color=\"green\"> Modeling approaches for classification</font>  \n",
      "* **Discriminative models** are models that directly estimate the **conditional**, or **posterior**, probability $P_{Y\\mid X}(y\\mid \\mathbf{x})$ for an instance $\\mathbf{x}$ over all class labels to predict the label $y_o$  as\n",
      " > $y_o = g(\\mathbf{x}) = \\arg \\max_y P_{Y\\mid X}(y\\mid \\mathbf{x})$ \n",
      "\n",
      "     Another interpretation is that discriminative methods model *the decision boundary* between classes and assign a class label to the class that is the most probable given the data. No attempt to model the underlying probability distribution.\n",
      "  \n",
      "  \n",
      "   > <font, color=\"blue\">**Examples:** SVM, Neural networks, Logistic regression, Nearest neighbors, Conditional random fields</font>\n",
      "\n",
      "\n",
      "* **Generative models** model the **joint**  distribution $P_{X,Y}(x,y)$ so that the label $y_o$ for an instance $\\mathbf{x}$ is predicted by \n",
      "  >  > $y_o = g(\\mathbf{x}) = \\arg \\max_y P_{X\\mid Y}(\\mathbf{x} \\mid y )P_Y(y) = \\arg \\max_y P_{X,Y}(x,y)$ \n",
      "\n",
      "> <font, color=\"blue\">**Examples:** Naive Bayes,Hidden Markov Models, Bayesian networks, Markov random fields, Mixture models</font>\n",
      " \n",
      "    The name come from the ability to sample such models to generate new data sets. \n",
      "\n",
      "A nice discussion of Discriminative and Generative is given in [those useful](http://cs229.stanford.edu/notes/cs229-notes2.pdf) notes by Andrew Ng's. A comparison between generative and descriminative approaches for classification is presented in [a paper](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf) by Andrew Ng and Michael Jordan.\n",
      "\n",
      "\n",
      "## <font, color=\"green\"> Learning methods by distribution of work between learning and prediction step</font> \n",
      "* [Eager learning](http://en.wikipedia.org/wiki/Eager_learning) - The system tries to construct a general, input independent hypothesis function during training of the system. This is an example of the **offline learning** approach in which the system do not change its approximation to the target function once the initial training phase is completed. The training phase is the part where the best instance of the hypothesis space is selected based on the available data.\n",
      "* [Lazy learning](http://en.wikipedia.org/wiki/Lazy_learning) - generalization beyond the training data is delayed until a query is made to the system, also called **instance based larning**.\n",
      "* [online learning](http://en.wikipedia.org/wiki/Online_machine_learning) - methods that assume that the data comes in a **sequential** (or **streaming**) form, This form of learning is useful for the \"big data\" regime where the data can not be stored in memory and when the predicting model have to change over time. The case where the entire training set can be accessed is called **batch learning**. There is also a **mini-batch** scenario where parts of the training set are available. \n",
      "\n",
      "## <font, color=\"green\"> Search procedure</font> \n",
      "Learning methods can also be grouped based on the optimization algorithm used to search for a good hypothesis:\n",
      "* **One step computation** - Solving for the parameters of the hypothesis, or model, in one step. \n",
      "    > <font, color=\"blue\">**Examples:** Linear regression, Naive Bayes,..</font>\n",
      "* **Local search** - start with an initial hypothesis, making small improvments until reaching a local minimum. \n",
      "    > <font, color=\"blue\">**Examples:** (Stochastic) Gradient descent for Neural Network</font> \n",
      "* **Constructive search** - Start with an empty hypothesis, gradually add more structure to it untill reaching a local minimum. \n",
      "    > <font, color=\"blue\">**Example:** Decision trees, Boosting</font>\n",
      "\n",
      "Often there will be more than one optimization approach for a given learning procedure so we can, for example, decide to switch from direct computation for solving linear regression problems and switch to gradient descent once the problem becomes very large. \n",
      "\n",
      "## <font, color=\"green\"> Interpretable machine learning</font> \n",
      "The main obstacle in the deployment of machine learning models is that humans many times do not trust them. Thus the interpretabiliy power of machine learning techniques is very important to their acceptance. Unfortunatly, many times interpretable learning models have less predictive power than more complex but un-interpretable models. The construction of accurate interpretable machine learning models is an active reserach area, see e.g. [this paper](http://link.springer.com/chapter/10.1007%2F978-3-642-32378-2_8) or download [those slides](https://chenhaot.com/pubs/mldg-interpretability.pdf). An interesting approach are methods that produce [decision lists](http://en.wikipedia.org/wiki/Decision_list). The work of [Cynthia Rudin](http://web.mit.edu/rudin/www/Mypage.html) as manifested in her [research overview](http://web.mit.edu/rudin/www/researchsummary.pdf) seems very relevant.\n",
      "\n",
      "## <font, color=\"green\"> Mix and match</font> \n",
      "Nearly any specific method have several variants, corresponding to different choices of the different axes discussed above. As an example we consider learning by decision trees: The original decision three learner is an eager learner but we also have [lazy decision trees](http://www.aaai.org/Papers/AAAI/1996/AAAI96-107.pdf) and [online decision trees](http://www.mitpressjournals.org/doi/pdf/10.1162/0899766041336396)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <font color=\"red\"><span id=\"TheoryLearning\">Theories</span> of learning</font>\n",
      "The term \"learning\" is actually referred to **generalization**. We would like our selected hypothesis $g^{(D)}$, learned on the data set $D$,  to perform well on data it has not seen. We would like the **in-sample** error $E_{\\tt in}(g^{(D)}) $ to be close to the **out-of-sample** error $E_{\\tt out}(g^{(D)})$. In other words, we would like $||R(g^{(D)}) - R_{\\tt{emp}}(g^{(D)}) ||$ ) to be small. There are some interesting topics covered in the theory of learning such as\n",
      "* Is learning feasible?\n",
      "* How many sample points do we need to get good generalizations?\n",
      "* What learning method should we use - what are the tradeoffs?\n",
      "\n",
      "There are more than one learning theory covering the questions above from different viewpoints. Those theories arer beatiful in their own right and provide many useful hints for selecting, using and evaluating a learning method. \n",
      "\n",
      "My, very informal, summary of the main \"results\" of the theory is that:\n",
      "1. Learning from data is, in general, impossible if we are not given any additional information. This is the consequence of the no-free-lunch theorems. Fortunatly real learning problems are never stated in the general terms of this theorem and the implicit end explicit restrictions make learning possible.\n",
      "2. The more powerful a learner is, the more sample points are needed to get good generalization. Providing too few sample points to powerful learners causes **overfitting**. This is expressed, from different angles by PAC learning, Bias-Variance decompositions and the theory of VC dimension."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"NoFreeLunchTheories\">No</span> free lunch theorems\n",
      "\n",
      "[No free lunch theorems](http://en.wikipedia.org/wiki/No_free_lunch_theorem). We consider the statement for a simple case of binary classification model (that is, concept learning) based on binary predictors. Let  $\\mathcal{F}$ be the set of all boolean functions. Then:\n",
      ">  $\\frac{1}{|\\mathcal{F}|} $$\\sum_{f \\in \\mathcal{F}} E_{\\tt{out}}(g^{(D)},f) = \\frac{1}{2}$ \n",
      "\n",
      "where $E_{\\tt{out}}(g^{(D)},f)$ is the out-of-sample error (fraction of instances of the test set that are wrongly classified) for the best hypothesis $g^{(D)}$ consructed by the learner $L$ over the training set $D$. This is true for any training set and learner. \n",
      "\n",
      "The proof is very simple: Suppose that we have a learner that achieves a generalization error of $\\frac{1}{2}-\\epsilon$ for a given target function $f$. then there is a target function $f^*$ that is equal to $f(x)$ when $x$ is in the training data and is equal to $\\neg f(x)$ when $x$ is in the test set. Then the generalization error for $f^*$ would be $\\frac{1}{2}+ \\epsilon$. Summing over all pairs of functions, we get the result.\n",
      "\n",
      "This suggests that no learner can in principle be better than a random coin-flip learner. Thus no learning model or inductive bias is better than others. Of course, in reality a target function is not randomly uniformly selected out of the space of target functions. In practice we impose other (unknown but not uniform) probability distribution for specific problem domains, exploiting regularities, smoothness and other domain characteristics. However the theory do explain, at least qualitativly, why the premise of the single jack-of-all-trades algorithm in machine learning was never fulfiled. In many cases simple algorithms outperformed more complex alternatives in specific problem domains. The NFL theorems also explain the need for a large tool box of different algorithms in order to be a good practitionner in machinbe learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"BiasVariance\">Bias</span>-Variance tradeoff\n",
      "The notion of **Bias** and **Variance** are welll established in the machine learning community and are used to explain, evaluate and compare the performance of various algorithms. Informaly speaking, the out-of-sample error of a learner is decomposed into three parts: \n",
      "* The **Bias** represent how well the best predictor from our hypothesis set fits the target function, on average for many randomly selected training sets of the same size. \n",
      "* The **Variance** represents how **stable** is the optimal predictor when we vary the training set. \n",
      "* The **irreducible error**, sometimes loosly called the **noise**, represents the parts of the target function that can not be approximated by our hypothesis set. \n",
      "\n",
      "This notion was found to be useful in developing and evaluating machine learning algorithms: While the Noise term is outside of our control, we have control over the Bias and Variance. Learners with low bias and high variance are the flexible models that can approximate well complex target functions but are unstable and may vary widely from data set to data set. Models with high bias and low variance are the opposite: They have low approximation power but are very stable and do not deviate much from one dataset to another.\n",
      "\n",
      "\n",
      "### Bias-Variance decomposition for square loss\n",
      "\n",
      "We consider an hypothesis set $\\mathcal{H}$ from which the learning algorithm selects the best hypothesis $g^{(D)}=g^{(D)}(x)$. The Bias-Variance decompositions is the following: \n",
      "> $\\mathbb{E}_D[E_{\\tt out}(g^{(D)})]= \\tt{Bias}^2 + \\tt{Variance} + \\tt{Noise}$\n",
      " \n",
      "where $\\mathbb{E}_D[E_{\\tt out}(g^{(D)})]$ is the average out-of-sample error over all data sets of size $N$ and the out-of-sample error for the square loss is \n",
      "> $E_{\\tt out}(g^{(D)}) = E_x[(g^{(D)}-y)^2]$ \n",
      "\n",
      "where $ y = f(x) + \\epsilon$ and $\\epsilon$ is a \"noise\" term with zero mean ($\\mathbb{E}_D[\\epsilon]=0$) and variance $\\tt{Var}(\\epsilon)=\\mathbb{E}_D[\\epsilon^2]$ independent of $x$.\n",
      "Here $E_x$ denots averaging over all domain $\\mathcal{X}$ (this is just the definition of the risk). Thus\n",
      ">$E_D[E_{\\tt out}(g^{(D)})] = E_x[E_D[(g^{(D)}-y)^2]]$. \n",
      "\n",
      "To evaluate the expectation $E_D [ (g^{(D)}-y)^2 ]$ we will use the 'average' hypothesis $\\bar{g} = E_D[g^{(D)}]$ where, for each x, we average over **many** data sets $D_1,D_2,...D_K$ so  $\\bar{g} = \\bar{g}(x) = \\frac{1}{K} \\sum_{i=1}^K g^{(D_K)}$. Using $\\bar{g}$ we get the decomposition:\n",
      "\n",
      ">$E_D [ (y-g^{(D)})^2 ] = E_D [ ( y-f + f - g^{(D)} )^2 ] \\\\\n",
      "\\quad = E_D [( y-f)^2] + E_D [(f-g^{(D)} )^2 ] + 2 E_D [(y-f)(f-g^{(D)} ) ] \\\\\n",
      "\\quad = \\tt{Var}(\\epsilon) + E_D [(f-g^{(D)} )^2 ] \\\\\n",
      "\\quad = \\tt{Var}(\\epsilon) + E_D [(f-\\bar{g}+\\bar{g}-g^{(D)} )^2 ] \\\\\n",
      "\\quad = \\tt{Var}(\\epsilon) + E_D [(f-\\bar{g})^2] + E_D [(\\bar{g}-g^{(D)} )^2] + 2E_D [(f-\\bar{g}))(\\bar{g}-g^{(D)} )] \\\\\n",
      "\\quad = \\tt{Var}(\\epsilon) + E_D [ (g^{(D)}-\\bar{g})^2 ] + (\\bar{g}-f )^2 \\\\  \n",
      "\\quad \\equiv \\tt{Var}(\\epsilon)  + {\\tt var}(x) + {\\tt bias}^2(x)$ \n",
      "\n",
      "where we note that the cross terms $2 E_D [(y-f)(f-g^{(D)} ) ]$ and $2E_D [(f-\\bar{g}))(\\bar{g}-g^{(D)} )] $ are zero. This is because  $E_D[y]=E_D[f]=f$, $\\bar{g}$ is independent of $\\epsilon$ and thus $E_D[\\epsilon \\bar{g}] = E_D[\\epsilon] E_D[\\bar{g}]=0$,  $E_D[g^{(D)}]=\\bar{g}$ and $(f-\\bar{g})$ does not depend on $D$. Integrating over the domain $\\mathcal{X}$, we get the Bias-Variance decomposition:\n",
      "\n",
      "> $E_D[E_{\\tt out}(g^{(D)})] = \\tt{Var}(\\epsilon) + E_x[{\\tt var}(x)] + E_x[{\\tt bias}^2(x)] = \\tt{Noise}  + \\tt{Variance} + {\\tt Bias}^2$\n",
      "\n",
      "### Bias-Variance decopmposition for other loss functions\n",
      "Extending Bias-Variance to other loss functions and in particular to the 0-1 loss function (frequency of prediction errors) of classification problems turned out to be not as straightforward as prehaps expected.  Gareth James in his paper entitled *\"Variance and bias for general loss functions\"* ([download paper](http://www-bcf.usc.edu/~gareth/research/bv.pdf)) argues that there are two characteristics of a plausible definitions of a Bias-Variance decomposition that coincide for the square loss functions but can not be simultanously met for the 0-1 loss. Those are:\n",
      "1. Providing an additive decomposition of the out-of-sample error and \n",
      "2. Reproduction of the standard characteristics of Bias and Variance, for example that a variance cannot be negative. \n",
      "\n",
      "We will not discuss this issue further in this section. The interested reader is encouraged to look at some papers listed in the <a href=\"#LearningTheoryResources\" data-ajax=\"false\">Resources</a> <span id=\"\"> section below.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <font color=\"green\"><span id=\"BiasVarianceExample\">Bias-Variance</span> demonstration</font>\n",
      "In this interactive demonstration, you approximate a fourth order polynomial augmented with randon Gaussian noise. You approximate the target function by an hypothesis set composed of polynomials. You can select the degree of the hypothesis space and the number of points in the train set. The display show the original target function in <font color=\"green\">**green color**</font> and the best hypothesis in <font color=\"red\">**red color**</font>. On each run of the demonstration, another set of *n_samples* train points is computed.\n",
      "\n",
      "**Variance** is demonstrated by the change in the graph of the best hypothesis as you rerun the demonstration. If the degree of the hypothesis space is large with respect to the number of sample points then you will see great changes in the shape of the best hypothesis from one run to another. You can compare high variance versus low variance hypothesis sets.\n",
      "\n",
      "**Bias** is demonstrated by quality of fit of the best hypothesis to the target function. In particular it is worth noting that as the number of points increase the graph of the model approaches the graph of the target function for any hypothesis coming from an hypothesis set of degree greater than three. This is because of the Gaussian nature of the error."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from IPython.html.widgets import interact, interactive, fixed\n",
      "from IPython.html import widgets\n",
      "\n",
      "import random\n",
      "import numpy.polynomial.legendre as lg\n",
      "import matplotlib.pyplot as plt     ## matplotlib - plots\n",
      "\n",
      "\n",
      "def polyWithNoise(x,err_factor):\n",
      "    coef5 = [ 2.33164947 , -0.48120244, -1.93454292 , 0.79024579 , 2.33241528]\n",
      "    p     = lg.legval(x,coef5)\n",
      "    p += err_factor*np.random.randn(len(x))\n",
      "    return p\n",
      "\n",
      "def poly_fit_deg(xp,xsampled,ysampled,deg):\n",
      "    p_coeff      = np.polyfit(xsampled,ysampled,deg) \n",
      "    poly_fit     = np.poly1d(p_coeff) \n",
      "    plt.plot(xp,poly_fit(xp),'r',label='hypothesis')\n",
      "\n",
      "\n",
      "def plotOverfit(n_sample,deg,show_data_points):\n",
      "    # PLot target function\n",
      "    err_factor = 0.0\n",
      "    xp = np.linspace(-1, 1, 100)\n",
      "    yp = polyWithNoise(xp,err_factor)\n",
      "    plt.plot(xp,yp,'g',label='target function')\n",
      "\n",
      "    # Sample n_samples points\n",
      "    err_factor = 0.3\n",
      "    xsampled = np.linspace(-1, 1, n_sample)\n",
      "    ysampled = polyWithNoise(xsampled,err_factor)\n",
      "    \n",
      "    if show_data_points:\n",
      "        plt.scatter(xsampled,ysampled,c='b',marker='o',s=30)\n",
      "\n",
      "    # Fit polynomial by OLS\n",
      "    poly_fit_deg(xp,xsampled,ysampled,deg)\n",
      "\n",
      "    plt.title('Approximating polynomial of order 4')\n",
      "    plt.ylim([0.,5.])\n",
      "    plt.legend()\n",
      "\n",
      "\n",
      "def show_args(**kwargs):\n",
      "    plotOverfit(kwargs['n_samples'],kwargs['deg_hypothesis'],kwargs['show_data_points'])\n",
      "\n",
      "i = interact(show_args,\n",
      "         show_data_points   = True,\n",
      "         c = widgets.ToggleButtonWidget(description=\"Run\"),\n",
      "         a=widgets.FloatSliderWidget(min=2, max=32, step=1, value=2, description=\"deg_hypothesis\"),\n",
      "         b=widgets.FloatSliderWidget(min=12, max=240, step=5, value=12, description=\"n_samples\")      \n",
      "         )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJIIUvEVEIkjBW0Qkgv4PZegl+KmXagsAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa205cf8>"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <font color=\"green\"><span id=\"ApproximatingSine1\">Example</span>: Estimating Bias-Variance</font>\n",
      "Give target function $\\sin(\\pi x)$ which we sample on two points and fit constant and quadratic models. The first model has high bias ansd low variance while the quadratic model have low bias but high variance. The computation takes quite a long time and is meant mainly to demonstrate how Bias-Variance can be directly computed. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# Compute bias-variance for approximating a target function over N training set\n",
      "# by a univariate polynomial of the given degree\n",
      "# Orr toy target function\n",
      "def sin_pix(x):\n",
      "    return np.sin(np.pi*x)\n",
      "\n",
      "\n",
      "def BiasVariance(N,target,deg):\n",
      "    # compute the average hypothesis over K data sets \n",
      "    K          = 1000\n",
      "    coeff_dk   = []\n",
      "    p_coeff_avg = np.zeros(deg+1)\n",
      "    for i in xrange(K):\n",
      "        x            = np.random.uniform(low=-1.0, high=1.0, size=N) # Sample N data point\n",
      "        y            = target(x)                  # A get valui of target\n",
      "        p_coeff      = np.polyfit(x,y,deg)        # Fit apolynomial to the data set\n",
      "        p_coeff_avg += p_coeff                    # and add it scontribution to the \"average fir\"\n",
      "        coeff_dk.append(p_coeff)\n",
      "\n",
      "    #-----------------------------------------------------------\n",
      "    # poly_bar: The mean hypothesis over a set of data points\n",
      "    #------------------------------------------------------------\n",
      "    p_coeff_avg /= float(K)\n",
      "    poly_bar   = np.poly1d(p_coeff_avg) \n",
      "\n",
      "    #-----------------------------------------------------------\n",
      "    # Now estimate expectation of the sample space X, use M points\n",
      "    #-----------------------------------------------------------\n",
      "    bias = 0.0\n",
      "    var  = 0.0\n",
      "    M    = 10000\n",
      "    for j in xrange(M):\n",
      "        xpt    = np.random.uniform(low=-1.0, high=1.0, size=1)\n",
      "\n",
      "        # compute bias(x) and bias = E_x[bias(x)]\n",
      "        bias += (poly_bar(xpt)-target(xpt))**2\n",
      "\n",
      "        # Compute var(x)=E_D( g^(D)(x)-g_bar)x))**2 ) by averaging over \n",
      "        # all values for that x and all data sets\n",
      "        varx = 0.0\n",
      "        for k in xrange(K):\n",
      "            poly_dk =  np.poly1d(coeff_dk[k]) \n",
      "            varx   += (poly_bar(xpt)-poly_dk(xpt))**2\n",
      "        varx /= float(K)\n",
      "\n",
      "        # var = E_x[var(x)]\n",
      "        var += varx\n",
      "\n",
      "    bias /= float(M)\n",
      "    var  /= float(M)\n",
      "    print '\\nSize of training set ',N,' degree of polynomial',deg,'\\n',70*'-'\n",
      "    print 'Bias                              : ',bias\n",
      "    print 'Variance                          : ',var\n",
      "    print 'Expectation of out-of-sample error: ',var + bias\n",
      "    \n",
      "#=========================================================================================\n",
      "# Here are the results comparing polynomial of degree 0\n",
      "# to polynomial of degree 1. The running time is too slow\n",
      "# for online demonstration so I replicate the results below\n",
      "#\n",
      "# N = 2\n",
      "# =============\n",
      "# Const polynomial: High bias/low Variance\n",
      "# ----------------------------------------------------------------------\n",
      "# Bias                              :  0.500743998327\n",
      "# Variance                          :  0.240062235445\n",
      "# Expectation of out-of-sample error:  0.740806233772\n",
      "#\n",
      "#\n",
      "# Second order polynomial: Low bias/hi Variance\n",
      "# ----------------------------------------------------------------------\n",
      "# Bias                              :  0.199169678361\n",
      "# Variance                          :  2.12334940824\n",
      "# Expectation of out-of-sample error:  2.3225190866\n",
      "#=========================================================================================\n",
      "N = 2\n",
      "BiasVariance(N,sin_pix,0)\n",
      "print '\\n'\n",
      "#BiasVariance(N,sin_pix,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Size of training set  2  degree of polynomial 0 \n",
        "----------------------------------------------------------------------\n",
        "Bias                              :  [ 0.49466522]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Variance                          :  [ 0.25630491]\n",
        "Expectation of out-of-sample error:  [ 0.75097013]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"PACLearning\">PAC</span> learning\n",
      "\n",
      "There are many versions of the theory of Probably Approximatly Correct ([PAC](http://en.wikipedia.org/wiki/Probably_approximately_correct_learning)) learning. Jeremy Kun wrote a very [nice post](http://jeremykun.com/2014/09/19/occams-razor-and-pac-learning/) describing PAC learning. PAC learning is an exciting theory because: (a) It provides assurances for inductive learning and (b) It gives us some guidlines on the number of examples needed to learn. \n",
      "\n",
      "In this section I will summarize, somewhat informaly, the main results. For simplicity, I will use the notations of this notebook that are different from the traditional notations of PAC learning. \n",
      "We will present the simplest possible setting, restricting our attention to concept learning, namely a binary classification problems with a binary target function $f(x)$. In the PAC setting we give two constants $ 0 < \\epsilon, \\delta < 0.5$ and we search for the number $N$ of samples in the training set so that \n",
      "> $P( |E_{\\tt{out}}(g) - E_{\\tt{in}}(g)|  > \\epsilon ) \\equiv \\delta$\n",
      "\n",
      "is small. The main result is that the **probability** that the out-of-sample error of the best hypothesis $g$ is  **approximatly** (up to $\\epsilon$) equal to the in-sample error can be made arbitrarily small, provided we are willing to provide many examples.\n",
      "\n",
      "* **Finite dimensional hypothesis space with $E_{\\tt{in}}(g)=0$**\n",
      "\n",
      "  **Claim**\n",
      "  \n",
      "   >  <font color=\"green\">$\\delta \\equiv  P( E_{\\tt{out}}(g) \\geq \\epsilon ) \\leq |\\mathcal{H}|e^{-N\\epsilon} \\Longrightarrow N \\leq  \\frac{1}{\\epsilon}( \\log(|\\mathcal{H}|) + \\log(\\frac{1}{\\delta}))$</font>\n",
      "    \n",
      "  **Proof**\n",
      "    \n",
      "    We define a \"bad\" hypothesis $h$ to be an hypothesis for which $E_{\\tt{out}}(h) > \\epsilon$. The probability that a bad hypothesis is consistent is\n",
      "    >$P(E_{\\tt{in}}(h)=0) = P(\\cap_{i=1}^N\\{h(\\mathbf{x_i})=y_i\\}) = \\Pi_{i=1}^N P(\\{h(\\mathbf{x_i})=y_i\\})\\leq (1-\\epsilon)^N$\n",
      "    \n",
      "  The probability  $P(E_\\tt{out}(g) > \\epsilon)$ that we learned a bad, consistent, hypothesis is not greater than the probability that the set of consistent hypothesis contains a bad hypothesis. By the [Union=Bound](http://en.wikipedia.org/wiki/Boole%27s_inequality) inequality to get\n",
      "    \n",
      "  > $ P(E_\\tt{out}(g) \\geq \\epsilon) < S (1-\\epsilon)^N$ where $S$ is the number of consistent hypothesis.\n",
      "    \n",
      "   Finally we bound $S$ by $|\\mathcal{H}|$, the cardinality of the hypothesis set $\\mathcal{H}$ and use the inequality $1-\\epsilon \\leq e^{-\\epsilon}$ to get\n",
      "   \n",
      "   > $\\delta \\equiv P(E_\\tt{out}(g) > \\epsilon) \\leq |\\mathcal{H}|e^{-N\\epsilon}$\n",
      ".\n",
      "\n",
      "\n",
      "* **Finite dimensional hypothesis space with $E_{\\tt{in}}(g)>0$**\n",
      "  \n",
      "  This case is also called **agnostic** learning since we wish to get results even when the target function is not within the hypothesis set. Clearly we can not hope for the strong assurances of the first case:\n",
      "\n",
      "    **Claim**\n",
      "  \n",
      "  >  <font color=\"green\">$\\delta \\equiv P( E_{\\tt{out}}(g) > E_{\\tt{in}}(g)  + \\epsilon ) \\leq |\\mathcal{H}|e^{-2N\\epsilon^2} \\Longrightarrow  N \\leq  (2/\\epsilon^2)( \\log(|\\mathcal{H}|) + \\log(\\frac{1}{\\delta}))$</font>\n",
      "    \n",
      "  **Proof**\n",
      "   \n",
      "    In this case we use [Hoffe\u001bding's Inequality](http://en.wikipedia.org/wiki/Hoeffding%27s_inequality) that states that for any hypothesis $h$, we have that $P( E_{\\tt{out}}(h) > E_{\\tt{in}}(h)  + \\epsilon ) \\leq e^{-2N\\epsilon^2} $. The rest follows from Union-Bound.\n",
      "\n",
      "\n",
      "* **Infinite dimensional hypothesis space with $E_{\\tt{in}}(g)>0$**\n",
      "\n",
      "    The key idea here is the theory of [VC-dimension](http://en.wikipedia.org/wiki/VC_dimension) which allows us to replace $\\log(|\\mathcal{H}|)$ by  $\\tt{VC}(\\mathcal{H})$, the VC-dimension of the hypothesis space $\\mathcal{H}$. The VC dimension of an hypothesis set is defined as the largest index $k$ for which we can find a set of points $\\mathbf{x_1},\\mathbf{x_2},\\ldots,\\mathbf{x_k}$ in such a way that the learner $g \\in \\mathcal{H}$ is consistent (attains a zero in-sample error) for **any** possible values of the labels $y_1,y_2,\\ldots,y_k$. If we can find a consistent learner for any number of sample points, then the VC-dimension is infinite. The VC dimension turned out to be a good characterization of the complexity, or prediction power of the hypothesis space. The larger is the VC-dimension, the more is the predictive power of $\\mathcal{H}$.\n",
      "    \n",
      "    As an example we note that he VC-dimension of a linear hypothesis space with $d$ predictors is $d+1$ while the VC-dimension of a decision tree is infinite since this is a very flexible learnin model. Note that in some cases the VC-dimension is associated with the number of parameters of a model but this is not true in general. A striking counterexample  due to E. Levin and J. Denker is the hypothesis set $\\tt{sign}( \\sin(\\alpha x) , x,\\alpha \\in \\mathbb{R}$ having just one parameter but an infinite VC-dimension. It is also not difficult to construct examples of hypothesis sets with many parameters but small VC dimension.\n",
      "    \n",
      "  **Claim**\n",
      "  \n",
      "  >  <font color=\"green\">$ N \\leq  (1/\\epsilon)( 8\\tt{VC}(\\mathcal{H})\\log_2(13/\\epsilon) + 4 \\log_2(2/\\delta))$</font>\n",
      "\n",
      "  **Consequence**\n",
      "  \n",
      "  Learning (generalization) is only possible if the VC-dimension is finite.\n",
      "\n",
      "\n",
      "It is indeed exciting the the theory of PAC learning is able to provide bound on the number of samples needed in a very general setting. However, the bounds obtained by the theory are far too conservative to be of practical use. Nevetheless the information provides us good understanding of the tradeoffs involved, mainly between the **complexity** of the learner, as expressed by its VC dimension,  and the number of data points needed to achieve good generalizations.\n",
      "\n",
      "> ## Lesson learned: <font color=\"blue\">The higher is the complexity of an hypothesis space, the more data points are needed to be able to generalize</font>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <font color=\"green\"><span id=\"PACLearningExample\">Example</span> PAC learning</font>\n",
      "In this demosntration we compare the in-sample and out-of-sample errors to a 15th order polynomial that is approximated by a polynomial of order 12. As the number of training points increase, the in-sample error get closer to the out-of-sample error, as expected, by the theory. The out-of-sample error represents the limitation of the hypothesis set of approximating the target function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.polynomial.legendre as lg\n",
      "import matplotlib.pyplot as plt     ## matplotlib - plots\n",
      "import warnings\n",
      "warnings.simplefilter('ignore', np.RankWarning)\n",
      "\n",
      "\n",
      "#import seaborn as sns\n",
      "%pylab inline\n",
      "\n",
      "\n",
      "def poly15(x):\n",
      "    coef = [ 1.33164947 , -0.48120244, -1.93454292 , 0.79024579 , 2.33241528 , 0.27261564,\\\n",
      "              1.92751477 , 2.44851315 , 1.20674012,-3.04869176 , 0.21597876 , 1.08214072, \\\n",
      "              1.13487548 , 0.6082264 , -2.91413442]\n",
      "    return lg.legval(x,coef)\n",
      "    \n",
      "\n",
      "N           = []\n",
      "Ein         = []\n",
      "Eout        = []\n",
      "\n",
      "target_func = poly15\n",
      "deg         = 12\n",
      "\n",
      "Ntest  = 10000\n",
      "numpy.random.seed(seed=7)\n",
      "perm   = np.random.permutation(Ntest)\n",
      "xtest  = np.linspace(-1, 1, Ntest)[perm]\n",
      "ytest  = target_func(xtest)[perm]\n",
      "\n",
      "\n",
      "for n_samples in xrange(3,1320,1):    \n",
      "    xsample  =   xtest[:n_samples]\n",
      "    ysample  =   ytest[:n_samples]\n",
      "     \n",
      "    p_coeff           = np.polyfit(xsample,ysample,deg) \n",
      "    poly_fit          = np.poly1d(p_coeff) \n",
      "\n",
      "    poly_fit_y_sample = poly_fit(xsample)\n",
      "    poly_fit_y_test   = poly_fit(xtest)\n",
      "\n",
      "    err_out = np.sum( (ytest-poly_fit_y_test)**2)/float(len(ytest))\n",
      "    err_in  = np.sum( (ysample-poly_fit_y_sample)**2)/float(len(ysample))\n",
      "\n",
      "    N.append( n_samples )\n",
      "    Ein.append( err_in )\n",
      "    Eout.append(err_out )\n",
      "    \n",
      "plt.plot(N,Ein, 'g',label='Ein')\n",
      "plt.plot(N,Eout,'b',label='Eout')\n",
      "plt.title('Learning curve: Approximation by polynm of order 12')\n",
      "plt.xlabel('Number of training points')\n",
      "plt.ylabel('Square error')\n",
      "plt.legend()\n",
      "plt.ylim([0,6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(0, 6)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEZCAYAAABxbJkKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XeYXVXV+PHvml4yLXUmvRcIpFEiGJxQJKB0kACGACKK\nFEEUUfy9BJH2ooiCIC8gvYQihE5oo7SEAOkhIYGE9D6Z3mf9/tjnztzcTM3cOrM+z3OfOfe0vc65\nd866e+9TRFUxxhjTNcVFOgBjjDGRY0nAGGO6MEsCxhjThVkSMMaYLsySgDHGdGGWBIwxpguzJNAK\nEZkiIisjHYdpnYiUiMjgMJV1n4j8IQTrnSUijwd7vfsRR72IDI10HC0RkSNFZLX3uZ8cgvU/IiI3\nBXu90Saqk4CIrBORYyIZg6p+oKqjIxlDNBCRId6B4d5Ix9IcVc1Q1XXBXq+IXCAiHwSUdamq/inY\nZQF24U7b/RH4u/e5vxyC9StB/DxE5CYRWSoiNSJyQ8C0H4jIhyJSKCJbROQBEekWrLJbEtVJgCB/\nCE0RkWjfB60K0zacDywDzhaRpFAUIJ5QrDuGdPXtb4+BwIpgrKiF/6F2fx4iktDMpNXAb4DX2Pe4\nlolLannAGKAfcEd7y94vqhq1L2AtcHQT4wW4DlgD7ARmAzl+058DtgB7gP8AB/hNewS4D3gdKAWO\nAdYB1wCLvWWeAZK9+fOBDX7LNzuvN/1aYDOwEbgYqAeGNrN93YGHgU3AbuBFb/wFwAcB8zasx28b\nXvO24Vpve+P85j8NWOwNx7W0v9rwOYi37OnAV8AZTcR2BfA1sAP4X0D8tuUj4G5vf33p/5kCBcCf\nvHnKgaHAEcACb/5Pge94854NfANkeO9P8La7RzP76F7vcy4BPgBygb8BhV4c4/3i8O2fYmA5cKo3\nfgxQAdR669ntt/6b/Jb/Ke6ffBcwB8gL2D8/8/ZdIXBPC/v6Btz39xkvls+Bg71pvwGeD5j/78Bd\nzaxrnbddy3Hfr3+x93e1tZiHAocCW32fpzftdGCRNzwLeBZ41It3GTApIIZfA0u8/fcQ0Ad4AygC\n3gayW9gfTcaI+67Ved+ZYiCxiWXH4L5fhV5cJ7VwHDgamAB84a3vGeDpgM/4h8Aib30fAQcFbOe1\n3nZW4Pe/2ERcjwM3tPI/dxqwJJjH02bLCkch+x1c80ngl8DHQF8gEfgn8JTf9AuAdG/aX4GFAR/+\nHhoPLMleOfNwB4kc3K+Ln3nT89k7CbQ07zTcQWkMkAo84X1Rm0sCr3lftCwgAZjiF39rSSBwG9YA\nx/rN/xxwbRv312JgegufwxTcP3AKcDPwchOxvQtkAwOAVcBP/LalxoshHviRF3u2N73A+wcag0tW\nfXD/ZOd576fjDmA53vxP4BJnD1zyPLGFfbQD94+d7MW3DvgxLqndBLznt+yZQK43/CPcgaGP935m\nE5/Hw8AfveGjvbLGA0m4A/N/AuJ6GfdrbwCwHTi+mX09C6jGHWjjcT84vvGG87y4srx5E4BtwIRm\n1rUOd1Dqh/uufoh3UGtjzL59uRyY5jftReBqv3grcN99AW4BPgn4f/kY6IX7/m3DHWjH+X0u/9NM\n/K3F2OTxwZuWiPufuM7bT1NxB/eRzfwPZQLf0vg9PcP7HHyf8QQv9kO97TzfKz/Rb19/4e3r5KZi\n8outLUngLvz+R0P5iviBvpUd0VwSWMHevybzvA9sn+yLOzDV0/jr8RHgkSbKOdfv/e3Afd5wPvsm\ngebm/Rdws9+0YTRTE/BirsP7hw6YdgGtJ4HAbbgJeMgbzsAdLAa0d3818zk86PtCev+Q1UCvgNi+\n7/f+UuAdv23ZFLC++cCPveH3gVl+02YA8wLm/xiY6Q1nef+sS3z7vZl99DBwv9+0y4Hlfu8PAgpb\n2OaFwMktfB7+SeAh4Da/aenePhroF9cRftNnA79tptxZwMd+7wVXszzSe/8GcLE3/ENgWSv/P5f4\nvT8BWNOOmH378rfAE95wd6CMxgQ5C5jrt54DgPKAGM7xe/888I+Az+XFZuJvLcaWksAUYEvAuKfw\nDr4E/A8BRzXxPf3I7zO+zzfsN30ljT/c1gIXtPH/qcUkAByH++EzvC3r6+grVtvDBwMvep0ohbiD\nXC3QR0TiReQ2EVkjIkW4Dwegp/dXgQ1NrHOr33AF0FKnTOC86d5wXsC6N7awjgG4poWiFuZpTlPb\n8BRwutdefzrwuar65hlMM/urtYJEJBX3K/k5AFVdhPvVc27ArP7xrMf96vPZFDDvt7h91dSyfb3l\nA+fv55VfhDuQjAX+0kr42/2GKwPe7/UZi8j5IrLQbx+NxdU22iLPixEvxjJc80U/v3n8vzPltPz9\navjeqDsqbKRxfz6Kq83g/W3tTKLmPpe2xOzzJHCSiKThakn/VdVtftP9h8uBlIA2dv/pFQHvK2l+\nX7QnxkB92fd/5Fsat9+3X/3nb+p76jMIuMb3/fC+I/3Z+3ve1HGlXURkMm5/n6Gqazq6vraI1SSw\nHlc9zfF7panqFtzB6WTgGFXNAoZ4y4Sjw20L7uDuM6C5GXFfmO4iktXEtDIgzfdGRHJbK1hVv8R9\naU/A7YOn/Ca3tL9acxquqny/d9aCbxtnBsw3MGDY/x8q8J92EO7XbUP4fsObvOmB828EEJHxwIW4\n7bu7DfG3SkQGAf8HXAZ0V9UcXBuy7zujzS3r2YxLtL71pdPYXLU/Gr433sG0P437aw5wsIiMBX6A\nO2C0pLnPpc0xq+pGXBPo6eybeFrbN01p6/9iR/brZmBAwIkGgwKW9Y99C01/T33W42r5/v9D3VR1\ndjPra80+84rIBNzne4Gqvt+OdXVILCSBJBFJ8Xsl4Nq0bxGRgQAi0svvPOFuQBWw2/vS3BKwvlAk\nA986nwUuFJHR3q+m/9fcAt4B+A3gXhHJFpFEETnKm7wYOFBExolICq7K3VR5gZ4CrsJVhZ/zG9/S\n/mrNTFy1fCyuHXcccCQwzjsQ+fza244BwJW4Jg+f3iJypbeNZwGjcR1yTW3P68BIETlHRBJE5Gxv\n/le9ffEE8DvgIqCfiFzaTNzt+ZzTcf+UO4E4EbnQ216fbUB/EUkMWL+vjKdxn/s4EUnGfefmqWpg\njaatsU0SkdO87/pVuF/L8wBUtQJ4AfdZz/cO0M0R4Bci0k9EugPX0/i5tDfmx3DNQmOBf7djWzqi\nvTH6m4erlVzrfe/ycc1nz3jTA+P+GKj1+56ejmv/93kA+LmIHOadxJbundbZ5tM4ve9zCq7PIdE7\nnsV508YCbwKXq+rrLa0n2GIhCbyO+zB9r//BneHxMjBXRIqBT4DDvPkfw/0i3oT7NfcJe2ddpfWM\nHThPS/M3zKuqb+I6r97HnQnyiTdPVTPLzsB1mq7EHWiu9NbzFe50sXdwnawftHEbnsa1bb6rqrv9\nxre0vxCRZSJyTuDKRKQfrnPuLlXd7vf6AveFPd9v9jm4M1kWAq/iEofPfGAErpPvJlxVtzBge/C2\nfTfun/Ua3EH518APvfG3At+q6v2qWo37VfonERkWuJ4m9lFT+8z3ua3ANS19gmu2GYvrRPV5F9c5\nulVEtvst61v+XVzCfwH3C3QIrkN7n+1rIRb/aS/hzoTajesgP11V6/zmedSLsbWmIMUli7m4s2lW\n487E2p+Y/42rSbyoqpWtbEtb/r9aWp42xth8Aao1wEm4mvEO4B5ghve/tU+53vyn4/p/duGavV7w\nm/457kyle3Cfy2rc9789v/4fxB3DpuMScjmNTXvX4Go5/xJ38VuJiCxtx7r3m+80vtCsXCQbt+EH\n4nbWRao6L2QFRhkRGQMsBZJUtT7S8YSKiNTjOrG+aWLaBbgzhaaEPbBOyqttrcR1zpa2MN9a3L5/\nL0jlrsadCReU9ZnoEOqawN+A11V1DHAw7tzsTs2rxieLSA7uzKGXO3MCMOHlNR9cAzzdUgIIQbmn\n4/qpLQF0Ms1d2dZhXofnFFWdCaCqtbiLQzq7S3CnD9bhzoH/RUSjCY82NZeZjvH6uLbhznibFsZy\nC3D9MjPCVaYJn5A1B3lncdyPOx1xHK69+JeqWh6SAo0xxrRbKJuDEoCJwL2qOhF32uN1ISzPGGNM\nO4WsOQh3XvdGVV3gvX+egCQgItZMYIwx+0FVg3J6bshqAqq6FdggIiO9UcfiTrMLnK9drzVrlKFD\nQ38pdVteN9xwQ8RjsPgjH4fFH3uvWI5dNbi/nUNZEwB3Z8knvVsZfI270tMYY0yUCGkSUNXF7H3V\nnTHGmCgSC1cMR638/PxIh9AhFn9kWfyRE8uxB1tIrxhutXARbW/5X38N3/+++2uMMV2RiKBB6hgO\ndZ+AMca0iXT5J4s2LdQ/1C0JGGOiRiRbJqJROBKj9QkYY0wXZknAGGO6MEsCxhjThVkSMMaY/fTk\nk09y/PHHRzqMDrEkYIwxrRg8eDBpaWlkZGQ0vK688krOO+883nrrrUiH1yF2dpAxxrRCRHj11Vc5\n+uijIx1K0FlNwBhj9tMjjzzClCmNT06Ni4vj/vvvZ+TIkeTk5HD55ZdHMLq2sSRgjDFt0NZrGF57\n7TU+++wzlixZwrPPPhv1zUXWHGSMiRlyY3AuntIb2ndRmqpy6qmnkpDQeMi84447SExM3Gfe6667\njszMTDIzM5k6dSqLFi2K6s5jSwLGmJjR3oN3sIgIc+bM2adP4JFHHtln3tzc3IbhtLQ0SktLQx1e\nh1hzkDHGdGGWBIwxpg32575GsXAvJEsCxhjTBieddNJe1wmcfvrpiMheN3kLvOFb4PRoZM8TMMZE\nBe8e+ZEOI6o0t0+C+TwBqwkYY0wXZknAGGO6MEsCxhjThVkSMMaYLsySgDHGdGGWBIwxpguzJGCM\nMV2YJQFjjOnCLAkYY0wXZknAGGNa0dzjJTti1qxZzJgxI0gR7r+Q30paRNYBxUAdUKOqh4W6TGOM\nCSZ7vGTHKJCvqhMsARhjOhNV5U9/+hODBw+mT58+zJw5k+LiYgAKCgoYMGDAXvMPHjyYd999lzff\nfJNbb72V2bNnk5GRwYQJEyIRPhC+5qDovo2eMca0oqkbuT388MM8+uijFBQU8M0331BaWtric4V9\ndxWdNm0av//975k+fTolJSUsXLgwlKG3KFw1gXdE5DMR+WkYyjPGdFIiwXm1l+/xkjk5OQ2vBx98\nkKeeeoprrrmGwYMHk56ezq233sozzzxDfX19m9YZDXdNDcfjJY9U1S0i0gt4W0RWquoHYSjXGNPJ\nROqY2dzjJe+8804GDRrU8H7gwIHU1taybdu2cIe430KeBFR1i/d3h4i8CBwGNCSBWbNmNcybn59P\nfn5+qEMyxpig6Nu3L+vWrWt4v379ehISEujTpw8bN26kvLy8YVpdXR07duxoeN+eh80UFBRQUFAQ\njJD3EdIkICJpQLyqlohIOvB94Eb/efyTgDHGRKummm7OOeccbr/9dk444QR69uzZ0M4fFxfHyJEj\nqays5PXXX+e4447jlltuoaqqqmHZ3Nxc3nnnHVS11YQQ+AP5xhtvbH7mdgp1n0Af4AMRWQTMB15V\n1bkdXWkUNKMZY7qYwMdLnnHGGVx00UXMmDGDo446iqFDh5KWlsbdd98NQFZWFvfeey8XX3wx/fv3\np1u3bnudLXTWWWcB0KNHDw455JCIbBPE4OMlN26Eww+HTZtCFJQxJiLs8ZL7ssdLNiEtDSoqIh2F\nMcZ0DjGXBFJTwa+vxRhjTAfEXBJISYHqaqiri3QkxhgT+2IuCYi4juEbboh0JMYYE/tirmPYLQcD\nBsD69SEIyhgTEdYxvK9wdAyH44rhoBs2DCZNinQUxhgT+2IyCcyaBW+8EekojDHB1p6raE1wxGQS\nSEpyncPGmM7DmoIiI+Y6hgGSky0JGGNMMMRkErCagDHGBEfMJoGyMksExhjTUTGbBD74wM4QMsaY\njorZJACwbFlk4zDGmFgXk0nAdxZZYmJk4zDGmFgXk0lg/Hh44gl3+wi/B/UYY4xpp5hMAklJcN55\n7rkCn3wS6WiMMSZ2xWQS8BkxAnbujHQUxhgTu2I6CfTsaUnAGGM6IqaTQFYWFBdHOgpjjIldMZ0E\n0tLsKWPGGNMRlgSMMaYLsyRgjDFdWMwngf/+F4qKIh2JMcbEpphPAt9+CzfdFOlIjDEmNsV0Epg4\n0f2tr49sHMYYE6tiOgnk5sK//gXz5rlbSBhjjGmfmE4CAGPGuFtHxMX8lhhjTPjF/KFz8mS4+WY3\nXFcX2ViMMSbWSCQf7iwiGqzye/eGd96BcePcVcQZGUFZrTHGRB0RQVUlGOsKeU1AROJFZKGIvBLK\ncvLy4I9/dMOZmXZ3UWOMaYtwNAf9ElgBhLTKUVMDL7wARx7p3r/+eihLM8aYziGkSUBE+gMnAg8C\nQam6NOeKK+C3v4WXX4ZLLoEPPwxlacYY0zmEtE9ARJ4DbgEygV+r6kkB04PWJ+CvpMQ1D+3YAamp\nQV+9McZEVDD7BBKCsZKmiMgPge2qulBE8pubb9asWQ3D+fn55Oc3O2ubZWS4DuKPPoJjj+3w6owx\nJqIKCgooKCgIybpDVhMQkVuAGUAtkIKrDbygquf7zROSmgDAX/4CCxbAM8+EZPXGGBMxwawJhOUU\nURH5HmFsDgJYu9bdVmLtWsjODkkRxhgTETF1iqifsF6QMGQIHHGEu62EMcaYpnWai8WasngxfPe7\nsHGjexSlMcZ0BrFaEwi7ceNg9GhYsSLSkRhjTHTq1EkA4PDDXbPQ119HOhJjjIk+LSYBEYkTkR+F\nK5hQ+Pvf4ZxzID/fbiVhjDGBWkwCqloP/DZMsYREXBzMmOH6Ba6+OtLRGGNMdGlLc9DbIvJrERkg\nIt19r5BHFkQnnACffw6VlZGOxBhjoktbrhiejju98zK/cQoMDUlEIZKR4W4nYYwxplGrSUBVB4ch\njpCzJGCMMftqNQmISBJwKXAUrgbwH+CfqloT4tiCKjMT9uyB7dvdA2iMMca04WIxEXkIlywexd0O\negZQq6oXd7jwEF8sFuiyy2DrVvfcAWOMiVVhvXeQiCxR1YNbG7dfhYc5CaxYAccf75LAYYeFrVhj\njAmqcF8xXCsiw/0KH4a7M2jMGTHCnSl05pkwahTMnw/19fC3v0FpaaSjM8aY8GtLTeAY4GFgrTdq\nMHChqr7X4cLDXBPwqax0D5tJS3O1g8GD4f333QVlxhgT7cL2UBkRiQfGASOBUd7oVaoa02fcp6TA\n6tWuZjB+vBt3++3wve+BhPQhmMYYE13aUhNYoKqHhqTwCNUEfMrLYeVK+OADuOoqd/vpww93NYQ5\nc+C+++Cgg2D4cCgudqeZJiZGLFxjjAHC3zH8VyARmA2U4c4QUlX9osOFRzgJ+PviC/cAmi++gFtu\n2Xd6WppLEOef7/oRLroo/DEaYwyEPwkU0MQDYVR1aocLj6IkEEjVNQ1t3Qrp6a4PYcAA99433aeu\nDuLjIxOnMabrCdvZQV6fwMuqOjXwFYzCo5mvbyA31zUDJSS4ZqOnn3YdyaWlsGoVPPigm3bPPS4x\nzJvn+huMMSYWdOk+gf2xfj0MGrT3uLvvhltvdc8y9j3AJjcX/vpXN+6f/4SHH4acHDftmWfg44/h\n+uuhT5/wxm+MiX3WJxBBqvD44zB1KmzY4G5BMXw4LF8Ol14Kd9zhxr34ont+wYIF8O23MHmyu0bh\ngQegutp1NMfHu76Gk06CQw+FsjI4+mj3RLS4Vq7gUHWnuqak7HtGU1UVJCfD0qXu9e23rrzSUlfm\npZe6Jq76eqithaFDXUwASUmh2W/GmOCxPoEYU1zsHm5TVeWali67zB2ElyyBe+91B/z582HSJHju\nOXdAPv54d8C++GKYO9d1WKekwI4d7rbYKSlQU+P6IyZPhp/8BN59F3btgrffdsklJwcmToR+/aBH\nDzdu/nz46CO37qQkqKhwyyQkuPnXrHH3WfJ55RU375AhsHu3i3X1ati0CQYOhBNPbKzhGGPCI6xJ\nIJS6ShJory1b4L334M474Ztv4PLL3QH84IOhVy9X0xBxHdXLlrmaw9SprhYxcKCrWWzdCsOGtf7L\nXhWKilyz1Xe/6xJMerpbZ3y8SygJCW492dluWt++cOCBrulryRK33LhxLhllZLhTb5OSGhPL6tVw\n7LFu2oYN7pWd7ZrV0tNdHL5ajTGmdeGuCeQCNwP9VHWaiBwAfEdVH+pw4ZYEoo6qSzyvvuoO5Oed\n55JBYuK+CcXXEf7lly4ZLF/uahYbN0LPnm6Z3buhe3eXMLKyXJPXiBGuRpKSAqNHu2aqtWtdh/vo\n0W6Z4mI3LjMTvv9917w1YoRbvls397621g2Xlbnyjj4a8vIam8eqq11M69a5+Wtq3Hqzs92yNTWu\njMWLXSzdu7tkW1vrxou4mLt3d7cZqa11ySoryyW0lBRXM9qwwZWVlgZjx7r1+5rp9uxx25CZ2bj/\nrMnNdFS4k8CbuNtGXK+qB4tIIrBQVcd2uHBLAl3Ghg2u2Wn4cHfgVnUHzoUL3QHzoINcMlm1qvHA\nW1/v/s6d6w6+a9e6ZUtLXbKJj3cH8rQ011w2b56bPyHBTS8vh5EjXY2jrs4lpcxMF4evpjJwoCu7\ntNQ115WVuXKHD3fr37PHNcGtWuVOE46Ph8JCN58vkeTlufUWF7s+mN27G/tbcnLcuouL3TbEx7uT\nBgYMaLz40PdKSnJ/U1JcbatvX1fm8OEuVt+pyLm5LqllZrpbo9fWNi6fmWkXNHYF4U4Cn6nqISKy\nUFUneOMWqer4DhduScAEkaqrhcTFucSQluYOltGivt69NmxwcZaWuiTie1VXu78VFbB5s3sVFrqa\nWXy8SwRVVS4pVVa65bOyXJOab/nycpcoExMbk1R9vUsO6emNTXSZma6m4vv3S0hw+8pXI0pK2nta\nYqL7GxfnlouLc2UnJDRuW3W1KzMry83vm9fXdNm7txvnmzc+vvEFrry4OLdO/2m+l6rbntraxldx\nsdvmwH1YW9sYp2/d9fVu2H8bfK/A975xqo37sbbWrb+kpPllVBtfvrJ8++7gg11tNxjCdu8gT6mI\n9PArfDJQFIzCjQkm38EmWvkOFEOGuFdH1dY2rtOnstIdGGtqGg/ecXFuXFmZG79nj3sfuK6qKpeA\nCgvdfP7TfK+6OneAq6tzp0vX1bl5RBprV0uXunnr6xvnXbHCHUDr6928ycluvFtvPTX1NVTUVhBH\nPElxqdTUKNU1dcRLEvV1QlVNLYqSmKiutpSUQGJCPBkZLrn516QSEyE+QRGgrk4a9r2vmdCXEHx/\nfa/A975YfQdxX99Yt25uPYHL1NU1Jj3fy7eNNTVwySXBSwLB1JaawCTgbuBAYDnQCzhTVRd3uHCr\nCRgTMuU15aQlpgGwu2I3RZVF9E7vzdbSrQzNGYo0c7fE2vpavtzxJXkZeewq30VhZSE9UnuweNti\nlm9fTs+0nihKnMRRU1fD2j1r2V2xm9xuuXy04SOKKosY23ssijIgcwCH9j2UDcUbqNd6NhRtoF9m\nPzaXbObtb96mV1ovlm1fRt+MvhzW7zCKq4opWFdATX0NE3InMG/jPKrqqhjTcwwT8ybyTeE37Knc\nw6aSTQzJHkJVXRXDuw+nXuuprXd3uM9OyWbu13OJl3gO7H0gcRJHUnwSOSk5xMfFU1lbyZrdazig\n1wGM6zOOkT1GkhSfxNe7v6ZO65iUN4md5TtZuXMlo3uOJj0pneyUbHqk9mBozlDi41zVpbiqmMKK\nQspryhnefTgLNi9g7tdz6Z7anWnDp7FixwqyU7JJjEskLyOPwdmDiZO23L2/dWE/O8jrBxiFu0Zg\nlapWB6VwSwImiFbsWMHv3v0dFTUVHDHgCCbmTSRe4umZ1pND+h7S8M/bnAWbFpDbLZcBWVFcnWhF\nZW0lzy5/lmXbl/HXeX/le4O+R0ZyBi+vepm8bnlsKtlEQlwCfTP6cuLwEzlu2HGkJqSyuWQzFbUV\nvLb6NVbvWs32su0ApCamkpWcxY7yHeSk5HDyqJPZUb6DwopCUhNTqaytZFjOMIblDOPrwq85dfSp\n1NXXsalkE9V11Wwr3cYnGz8hOSGZuvo6JuROYFfFLnJScji036EkxSdxYK8D99rn9VpPRU0F6Unp\n1NTVUFxVTFZKFglxjQ0XZdVlzNs4j/SkdDaXbGZ3xW56pPagTuvYXbGbkT1GMqrHKBZuXUh1XTUV\nNRXsLN+JiJCRlMGonqP4atdXLNm2hMXbFlNWXca4PuOorKvk002fkhSfxDFDjmHlzpXU1tdSWFnI\n2sK17KrYxfDuw8lIyuDzLZ+T2y0XQdhaupXcbrmcdcBZFFYW8tbXbzG652gqairYUrqF8ppybpp6\nE7849BdB+ZztFFHTKZVWl7Kncg/9MvpRU19DdV01W0u3sqNsB2mJacTHxdMvox85qe7ChHV71vH1\n7q8Z3XM0N/33Ju7//H7OO+g8Dux1ICt3reT5Fc+Tk5LDzvKdjO45mqmDpzK652hOG3MavdPdg6a/\n2vUVn23+jDmr5vDs8mdJS0wjOT6Zmvoaph84nYsnXoyIcNGci4iPi2dyv8kUVxfz4foPmTluJqN6\njKKytpId5TvYWrqVAZkD+HLnlwzLGYaIsKl4E3kZeVTXVVNSVdJQrohweL/DmTqk9cttaupqSIzf\nu7f3mWXP8O4379I/sz8fb/yYeq2ne2p3nl3+LIf3O5xRPUdxwbgL2F62nY82fMSF4y9kQt4Eautr\niZd4Vu1axUsrX+Ldte9SWl1KXrc8iquKqdd6fvWdX3Hs0GNJSbBzdgPtLN/ZkHQm95/csI92V+wm\nOyW7xV/69VofuzWB/V65SAruwfTJQBIwR1V/5zfdkoAB4P7P7ufnr/284X28xJOT6g7gg7MHs27P\nOpLik0iOT2Zi3kS2lW1j5c6Ve63jydOf5Jyx5+zVzKGqKMrTS5/mjTVvUFtfy5tr3iQnNYeclBy+\n3PklQ7KHcMaYM7ji8CtIiEtgQ9EG0pPSeWTRI8xePps1u9dw2zG3kRSfxMqdK+mf2Z/jhh3HPZ/e\nw0srX+KV8VvvAAAaYUlEQVSIAUdQp3VMGTiFraVbyUzOZM3uNRRVFTExdyIbSzayYNMCjhp0FN2S\nulFUVUS3xG7M/cY1HZw2+jTiJI5v93xLXkYeRw85mpdWvsR9n93H2N5jmbdxHkNzhvKd/t/h8y2f\nMyR7CEu3L+XC8RdSWFHIuNxxJMUnsW7POi479DJ6pPXAdG4xkwQARCRNVctFJAH4EPi1qn7oTbMk\nEONUlQ3FG8hMziQ7JbvJeapqq3jhyxdYum0p28u2MyBrQMMv0pSEFHql9+K6d67jtmNvY/rY6SzZ\ntoTa+lo2l2zmRwf+iDiJo7qumqT4JHaU7eCZZc/QN6Mvo3qO4oBeB7ChaAODsgc1WXZTKmoq+HjD\nx1TWVjI+dzz9Mvs1O2+91lNYURiSA2tdfR2vrX6N1756jfi4eNIS0yivKefTTZ8yoscIfj7p5xRV\nufb1neU7+e+3/23YZ2cccEZDrcJ0PeE+RTQOOA8Yoqp/FJGBQK6qftqugkTScLWCmaq6whtnSSBG\nLNm2hN0Vuxnbeyw903oCrpni2rev5a75dwEwNGcoVx1+FfM3zWfVrlWM6D6C3RW7eevrtwDISclh\nxsEzWF+8nk83fUqcxLG5ZDOjeozij1P/yJkHnBmx7TMmloQ7CfwTqAeOVtXRItIdmKuqh7SpAJdE\nvgCGAfep6rV+0ywJxID7FtzHL15v7ND6xSG/ICM5g9s/up04iWPuj+cyPnc8//7y39zy4S3sLN/J\nFYddwfIdywHo260vfzjqD+R2y221c9YY07pwJ4GFqjoh4GKxxao6rl0FiWQBbwHXqWqBN05vuOGG\nhnny8/PJt6e9h11THY8+17x1DXfOu5PVV6xmWM4wVu1axe0f3U5RZRFnjDmDKYOmMDBrYJgjNqZr\nKSgooKCgoOH9jTfeGNYkMB84AvjMSwa9cDWBCe0uTOT/ARWq+mfvvdUEwqykqoQ31rzBKaNO4b21\n77GjfAczX5rJ7DNnM3v5bDKSMshJyaG6rpoNxRuYt3EeX172pXU2GhNFwn3F8N3Ai0BvEbkFOBP4\nQ1tWLiI9gVpV3SMiqcBxwI37G2w02162ndnLZjNz/EwykzNbXyACyqrLyLwtk6zkLM5+/uyG8Xcc\ndwc/feWnFFcVc9LIk3ht9WvkdctjePfhPHrqo5YAjOnEWqwJeO353wF2A8d4o99V1S/btHKRg4BH\ncY+xjAMeV9U7/KZ3iprAW2ve4m/z/8Yba94A4O0ZbzNl4BSSE9yNa5ZtX8Z9C+7j+qOup29GX8Bd\nlbl462Im9Z3U4fK3lGzhtdWvcc7Yc4iTOFITUwHYWrqV37z9G66fcj0fb/iYa+Zew9TBU3nhRy/w\n+JLHGdF9BP0y+zEwayDLti9jybYlnHvQuR2OxxgTWuHuEwjKzeKaWXfMJ4HHFj/GzJdmAjBz3EzG\n9BzDde9eB8BNU29iTM8xPLjwQd5c8yYAK36xgu1l28l/NB+AOInjZ5N+RmZyJjcffXNDx2lLF5Ys\n2baEez69h18e/kve/uZtrn7r6r2mJ8QlNFxC7++YIcfwjxP/waieo4Ky7caYyAh3EvgzMA94IdhH\n7HAngffWvsf6ovWcMeYMMpIz9ns9ZdVlTHvS3Rtkd8VuLj3kUs496Fz6ZvRlaM5QHln0CBfOuXCv\nZW495lZKq0u5+YObAcjrlsffT/g7Tyx5gjmr5gBwY/6NTMqbxLjccYz5xxhyUnJ48ewXmdR3Em9/\n7Q72w7sPZ1fFLj5c/2HDup86/SnOHns2i7YuYnPJZhZtXcSKHSv4/ZTfM6L7CJZuX0pet7wWz4c3\nxsSOcCeBUiANqAMqvdGqqh1u+A53EpAb3T679Zhbue6717V7+V539GJn+c69xl084WIeOPmBJudf\nvWs1X+36itE9R9M3oy+pial8seULUhNSyUjOoH9mf8Bdir69bDvHPnYsW0q3AJCakMq43HHM2ziP\ng3ofxNLtS8lKzuKnE3/KRxs+4tVzX2V72faGC56MMV1HTF0x3GLhYUoCC7csZHvZdqY9OY0/TPkD\nD3zxAJt+tWmfc9abaoKp13oEoaiqiJzbczhxxInkdcvj4okXc3i/w5u9E+P+2Fi8kRU7VvDe2veY\nlT+LlIQU1hau5cWVLzKuzziOGXpM6ysxxnR6kbiLaA4wAmi4o5Sq/rfDhYcpCZz6zKnMWTWHhLgE\nKq6vYMrDU/j1d37NGQecAcDsZbOZ/sJ0ADZevZF+mf1QVe5dcC+Xv3F5w3qOG3occ2fMDXm8xhjT\nkrCeIioiPwWuBAYAC4HJwCfA0cEIIBx8Z+RkJbvb0V4y8RLOfO5M7vz+nfxq7q8AGNt7LKkJqdz5\nyZ3Myp9F5m2NrV0juo9g9e7VTBs+LSLxG2NMqLSlT2AZcCjwiaqOF5HRwK2qelqHCw9TTWD689OZ\ns2oOQ7KHsOKyFRRVFpF9+943O1t9xWrq6usY/Y/RDeMuGn8RVxx+BeNzx/PG6jc4tN+hDffNMcaY\nSAn3xWKVqlohIohIiqquFJGYOMfwn5/9k7TENGYvn82vJv+KJduXAJCVkoXeoNz+4e1kpWTx80Ma\nb2F84fgLeXjRw2y4ekNDxy3ACSNOCHv8xhgTam2pCbwIXAT8EnfBWCGQoKondrjwENYEXl71Mqc8\nc0rD+/Lfl1NdV01WSlary5ZUlXToFFJjjAmliJ0dJCL5QCbwZjAeMRnKJOA7HbRHag9+dOCPuPcH\n94akHGOMCbdwdwz73yLyG+9vLrA+GAGEwvqixtDmTJ/DkQOPjGA0xhgTvdrSJ/A64Pu5ngIMAVYB\nB4YqqI7YVb6LQXe5p0ydP+58JuS1+2anxhjTZbSaBFR1rP97EZkIXBayiDrozx//GXD38Xnk1Eci\nG4wxxkS5/bpiWESWBSaH/So8iH0CO8p2kJGcQerN7g6aJb8roVtSt6Cs2xhjokm4+wSu8XsbB0wE\nNgWj8GDq/efenHXAWQDcfcLdlgCMMaYN2tInkEFjn0At8CrwQsgi6oDnVjwHwPHDjo9wJMYYExva\n0icwKwxxdFj/zP5sLN4IELVP9jLGmGjTluagV3A1AV/7017DqnpyiGJrVW19LeuL1nPxyxcTL/G8\nd/57HP3Y0WSnZLe+sDHGmDY1B60F+gBP4A7+5wDbcM8djqgXVrzQcPdPgEl9J6E3xPaTyowxJpza\nkgSOVFX/B+G+LCKfq+pVoQqqrZLik/Z6n5Fkt3owxpj2aPohtntLE5FhvjciMhT3pLGIq6yt5KSR\nJ3HfD+4DCOoDXowxpitoS03gauB9EVnrvR8MXBKyiNqhoraCHmk9+Nmkn3HCcLvLpzHGtFdbzg56\nU0RGAqNxncIrVbUq5JG1QXlNOWkJaYgIg7IHRTocY4yJOc02B4nIYSKSB6CqlcA44CbgDhHpHqb4\nWlRRU0FqYmqkwzDGmJjVUp/A/UAVgIgcBdwGPAoUA/8X+tBaV15TTlpiVHRPGGNMTGqpOShOVXd7\nw2cD96vqC8ALIrI49KG1rqK2gqzk1h8SY4wxpmkt1QTiRSTRGz4WeN9vWls6lENGVVlftN6ag4wx\npoNaSgJPA/8RkZeBcuADABEZAewJQ2zN+nD9hwy6a5A1BxljTAc1mwRU9WbgGuBh4LuqWu9NEuCK\nMMTWrHovlA/Wf0BqgtUEjDFmf7XYrKOqnzQx7qu2rlxEBgCPAb1xp5f+n6r+vb1BBiqpLgFg1a5V\nVhMwxpgOaMsVwx1RA1ytqgcCk4HLRGRMR1ZYXVfNSU+f1FADKK8p73iUxhjTRYU0CajqVlVd5A2X\nAl8CfTuyzrWF7sLlitoKDul7CCN7jOxwnMYY01WF7SwfERkMTADmd2Q9NfU19M3oy/yL59M/s38w\nQjPGmC4rLElARLoBzwO/9GoEDWbNmtUwnJ+fT35+fovrqq6rpk96H0sAxpguo6CggIKCgpCse78e\nNN+uAty1Bq8Cb6jqXQHT2v2g+Xkb53HVm1cx7+J5QYzSGGNiRzAfNB/SPgFx93Z+CFgRmAD2V3Vd\nNYnxia3PaIwxplWhPjvoSODHwFQRWei9pnVkhdV11fs8TMYYY8z+CWmfgKp+SJATjSUBY4wJnlDX\nBIJq5c6VXPnGlSTGWXOQMcYEQ0RvBNdeY/7hrjMrqymLcCTGGNM5xFRNwGdr6dZIh2CMMZ1CTNUE\nAB479TGKq4ojHYYxxnQKMZMEKmsrSYxL5McH/xh35qkxxpiOipnmoIqaCtKT0i0BGGNMEMVMErBT\nQ40xJvgsCRhjTBdmScAYY7owSwLGGNOFWRIwxpguzJKAMcZ0YZYEjDGmC4uZJLBq1yq7XYQxxgRZ\nyJ8s1mLh7XiymNzoLhLTGyIXrzHGRINgPlksZm4bce5B59otpI0xJshipjkoXuLJH5wf6TCMMaZT\niZkkUF5TTnpieqTDMMaYTiVmkkBZTRnpSZYEjDEmmGImCRRXFdMtqVukwzDGmE4lqpPA0m1L2VG2\ng3qtZ8m2JYztPTbSIRljTKcS1WcHHfzPgwG47sjrSIhLoHtq9whHZIwxnUtU1wR8bvvoNrta2Bhj\nQiCqk0BOSk7D8Pay7RGMxBhjOqeobQ5avWs1Y3uP5ZrvXMNBfQ6iR2qPSIdkjDGdTlTWBDYVb2Lk\nPSMpriomt1suQ3OGkpWSFemwjDGm04nKJLCldAsAi7ctJjUxNcLRGGNM5xWdSaBkS8NwaoIlAWOM\nCZWQ9gmIyL+AHwDbVfWgti5XUl3CKaNOYXL/yQzrPix0ARpjTBcX6prAw8C09i5UXlNOz7SeXPfd\n64iTqKysGGNMpxDSI6yqfgAUtne5suoy0hLTQhCRMcYYf1H5M7uspszuGGqMMWEQnUnAagLGGBMW\nEb9YbNasWQ3D+fn55OfnU1pdSs+0npELyhhjokhBQQEFBQUhWXfInzEsIoOBV5o6O6ipZwxf8NIF\nVNZWcuKIEzl/3Pkhjc0YY2JRMJ8xHNLmIBF5GvgYGCkiG0Tkwpbmr9d6Hl38KLOXz7Y7hhpjTBiE\ntDlIVc9pz/zVddUNw73SegU9HmOMMXuLqo7hqtoqAGYcPIPD+h0W4WiMMabzi3jHsL+quip6pvXk\nsdMei3QoxhjTJURVTaCytpLk+ORIh2GMMV1GVCWBqtoqUhJSIh2GMcZ0GdGVBOqqSE6wmoAxxoRL\ndCWB2iprDjLGmDCKqiRQWVtpNQFjjAmjqEoC5TXlduM4Y4wJo6hKAmU1ZaQnWRIwxphwia4kUG23\nkDbGmHCKqiRgzUHGGBNeUZUErDnIGGPCK6qSQHFVMRlJGZEOwxhjuoyoSgKFFYXkpOZEOgxjjOky\noioJ3DX/LroldYt0GMYY02VETRLwPWGsoqYiwpEYY0zXETVJoLK2EoDLD7s8wpEYY0zXETVJoKiq\niN7pvYmPi490KMYY02VETRLYU7mH7JTsSIdhjDFdStQkgaLKIrKSsyIdhjHGdCnRkwSqishKsSRg\njDHhFD1JoLLImoOMMSbMoiYJ7KncY81BxhgTZlGTBCpqK+zmccYYE2ZRkwSqaqtIik+KdBjGGNOl\nRE0SqK6rtiRgjDFhFjVJoKquyp4vbIwxYRY1ScBqAsYYE34hTQIiMk1EVorIahH5bUvzWhIwxpjw\nC1kSEJF44B5gGnAAcI6IjGlu/qraKpLjY6s5qKCgINIhdIjFH1kWf+TEcuzBFsqawGHAGlVdp6o1\nwDPAKc3NHIs1gVj/Iln8kWXxR04sxx5soUwC/YANfu83euOaVFVnp4gaY0y4JYRw3dqWmU56+iQA\nPlr/EaeNPi2E4RhjjAkkvid6BX3FIpOBWao6zXv/O6BeVW/3myc0hRtjTCenqhKM9YQyCSQAq4Bj\ngM3Ap8A5qvplSAo0xhjTbiFrDlLVWhG5HHgLiAcesgRgjDHRJWQ1AWOMMdEvYlcMt+dCskgRkQEi\n8r6ILBeRZSJypTe+u4i8LSJfichcEcn2W+Z33jatFJHvRy76hnjiRWShiLzivY+l2LNF5HkR+VJE\nVojI4TEW/++8785SEXlKRJKjOX4R+ZeIbBORpX7j2h2viEzytnm1iPwtwvHf4X1/FovIv0Uky29a\n1MTfVOx+064RkXoR6R6S2FU17C9c89AaYDCQCCwCxkQillbizAXGe8PdcH0cY4D/Ba71xv8WuM0b\nPsDblkRv29YAcRHehl8BTwIve+9jKfZHgYu84QQgK1bi92L4Bkj23s8GZkZz/MAUYAKw1G9ce+L1\ntSx8ChzmDb8OTItg/Mf59iNwW7TG31Ts3vgBwJvAWqB7KGKPVE2gXReSRYqqblXVRd5wKfAl7lqH\nk3EHKLy/p3rDpwBPq2qNqq7DfTiHhTVoPyLSHzgReBDwnUkQK7FnAVNU9V/g+phUtYgYiR8oBmqA\nNO8kiTTcCRJRG7+qfgAUBoxuT7yHi0gekKGqn3rzPea3TEg1Fb+qvq2q9d7b+UB/bziq4m9m3wPc\nCVwbMC6osUcqCbTrQrJoICKDcZl6PtBHVbd5k7YBfbzhvrht8Yn0dv0V+A1Q7zcuVmIfAuwQkYdF\n5AsReUBE0omR+FV1N/AXYD3u4L9HVd8mRuL30954A8dvIjq2A+Ai3K9jiIH4ReQUYKOqLgmYFNTY\nI5UEYqo3WkS6AS8Av1TVEv9p6updLW1PRLZVRH4IbFfVhTTWAvYSrbF7EoCJwL2qOhEoA67znyGa\n4xeRYcBVuOp6X6CbiPzYf55ojr8pbYg3aonI9UC1qj4V6VjaQkTSgN8DN/iPDkVZkUoCm3BtXT4D\n2DuDRQ0RScQlgMdV9SVv9DYRyfWm5wHbvfGB29XfGxcJRwAni8ha4GngaBF5nNiIHdz3YaOqLvDe\nP49LCltjJP5DgI9VdZeq1gL/Br5D7MTv057vy0ZvfP+A8RHdDhG5ANcsep7f6GiPfxjuB8Ri73+4\nP/C5iPQhyLFHKgl8BowQkcEikgScDbwcoViaJSICPASsUNW7/Ca9jOvkw/v7kt/46SKSJCJDgBG4\njpqwU9Xfq+oAVR0CTAfeU9UZxEDs4PpjgA0iMtIbdSywHHiFGIgfWAlMFpFU73t0LLCC2Infp13f\nF+9zK/bO5BJght8yYSci03BNoqeoaqXfpKiOX1WXqmofVR3i/Q9vBCZ6TXPBjT3Uvd4t9IafgDvb\nZg3wu0jF0UqM38W1py8CFnqvaUB34B3gK2AukO23zO+9bVoJHB/pbfBi+h6NZwfFTOzAOGABsBj3\nSzorxuK/Fpe4luI6VROjOX5cjXEzUI3rs7twf+IFJnnbvAb4ewTjvwhYDXzr9/97bzTG7xd7lW/f\nB0z/Bu/soGDHbheLGWNMFxY1j5c0xhgTfpYEjDGmC7MkYIwxXZglAWOM6cIsCRhjTBdmScAYY7ow\nSwKmRd4tbP/s9/7XInJDS8u0Y92PiMgZwVhXK+WcJe5W1O8GjB8kIufs5zo/asM8D4jImP1Zf0e1\npWwROSVS8ZnoYUnAtKYaOE1Eenjvg3lhyX6vy7szZ1v9BLhYVY8JGD8EOHd/1q+qR7ZWqKr+VCP0\nNL02ln0a7rbEpguzJGBaUwP8H3B14ITAX/IiUur9zReR/4jISyLytYjcJiIzRORTEVkiIkP9VnOs\niCwQkVUi8gNv+XhxDwP5VNzDQC7xW+8HIjIHdyVuYDzneOtfKiK3eeP+BzgS+JeI/G/AIrcBU8Q9\ndOcqEZkpIi97NYa3RSRdRN4Rkc+99Z7czLYWiMhz4h5e8oTfPAUiMtE3v4j8SUQWicgnItLbGz9M\nROZ56/+TiJQQwLu9ykoRecKr0TwnIqnetGPE3WV1iYg85N2GpdWyReQI4CTgDm/5oSJypbiH4CwW\nkaeb+C6Yzijcl6bbK7ZeQAmQgXuoRSZwDXCDN+1h4Az/eb2/+bh7o/cBknA3sZrlTbsS+Ks3/Ajw\nujc8HHe5fDJwCXC9Nz4Zd+uIwd56S4FBTcTZF3d7gB64hxa9i7tfDMD7uPuuBC7zPeAVv/cXeDFk\ne+/jcfdnB+gJrG5mW/d45QvwMXBEYLm424/8wBu+3W/7XgXO9oZ/5ltvQJyDveW/471/yPscUnC3\nqh7ujX8Ud6fbtpb9MHC6XzmbgERvODPS3z17hedlNQHTKnW3z34MdwBvqwWquk1Vq3H3MXnLG78M\nd1AD1xz0rFfGGtz9UUYD3wfOF5GFwDzc/WuGe8t8qqrfNlHeocD76u7aWYd7mtpRftObug1v4DgF\n5qrqHu99HHCriCwG3gb6+n7BB/hUVTerquLuMzW4iXmqVfU1b/hzv3kmA895wy39+t6gqp94w0/g\n7ms1Eljr7TtwSeCoJpZtrmzYex8sAZ4SkfOAuhZiMZ2IJQHTVnfh2tbT/cbV4n2HRCQO96vfp8pv\nuN7vfT3uWQHN8fUTXK6qE7zXMFV9xxtf1sJy/gc0Ye8+h7b2P5T7DZ+HqwFMVNUJuNsopzSxjP+2\n1tH09tX4Dbe2D5riH3/gtvmPb0pLZfuv5wfAP3C37F4gIvHtjNHEIEsCpk1UtRD3q/0nNB441uHu\nWgjuMYSJ7VytAGeJMwwYirsr4lvAL3ydsyIyUtxDNlqyAPieiPTwDl7Tgf+0skwxrqnLPx5/mbgH\n89SJyFRgUJu2qn3mAWd6w9NbmG+giEz2hs8FPsDdhXewt+/A3Tq4oB1ll+C20Xfb9IGqWoB7eE8W\neyd800lZEjCt8f+l+BfcL2OfB3AH3kW4Zo3SZpYLXJ/6Da/H3Tf/deBnXvPRg7h7738hIkuB+3C/\nXpt9spWqbsEdvN7HNcl8pqqvtLJtS4A6r8P0qibW/yRwiIgswR1g/c+2aW8tI3B+3/urgF95+3AY\nUNTM8quAy0RkBe4AfZ+qVuFu9/ycF2Mt8M92lP0M8BsR+Rx3T/rHvfV8AfxNVYvbsF0mxtmtpI2J\nIBFJVdUKb3g6rpP4tIB5BuM6sA8Kf4Sms2tvu6QxJrgmicg9uKaoQtyDUJpiv9ZMSFhNwBhjujDr\nEzDGmC7MkoAxxnRhlgSMMaYLsyRgjDFdmCUBY4zpwiwJGGNMF/b/Ad8WI2D0hL/SAAAAAElFTkSu\nQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa14eb70>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <font color=\"red\"><span id=\"RandomThoughts\">Random</span> thoughts on selecying and using learning models</font>\n",
      "\n",
      "Suppose that we are given a concrete predictive question and we want to select an appropriate learning model. We need to select the data representation, the hypothesis set, the cost function and the learning method. We may need also to develop strategies to combat overfitting and the curse of dimensionality. The first question to ask is whether we can write a program that directly solves our question. This approach actually boils down to a particular selection of an hypothesis set and learning procedure. If writing such a program is not feasible (we do not know how to write it) or desirable (for example, writing the program takes too much time) then we will use machine learning. The matching of a learning model to a specific problem is more of an art than science and it is the part where the our intuition and domain understanding shows up.\n",
      "\n",
      "Since no single method is guaranteed to perform better than others, we need to have first a strong understanding of available learning methods: \n",
      "\n",
      "* Look at methods described in textbooks on machine learning and leading software libraries. Look at this [Resources](Resources.ipynb) notebook.\n",
      "* J. Baxter suggests is [his paper]((http://people.ee.duke.edu/~lcarin/Baxter2000.pdf) to search for a learner that succeedded in similar learning problems\n",
      "* Look at \n",
      "  * [Top 10 algorithms in data mining](http://ailab.arizona.edu/mis510/slides/15_data_mining.pdf) (2008)\n",
      "  * [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf) (2014) and also [the followup post](http://www.r-bloggers.com/the-geometry-of-classifiers/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29)\n",
      "  * [An Empirical Comparison of Supervised Learning Algorithms (2006)](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf), [Watch the talk ](http://videolectures.net/solomon_caruana_wslmw/)\n",
      "* Look at discussions like [this Quora](http://www.quora.com/What-are-the-top-10-data-mining-or-machine-learning-algorithms) \n",
      "* Consider [this incomplete list at Wikipedia](http://en.wikipedia.org/wiki/List_of_machine_learning_concepts)\n",
      "\n",
      "Now it is time to use a useful inductive bias to guide us in selecting a descebt learning model. Important insight come from our domain knowledge. In the best scenario we know, or can assume, something about the way our data set was generated. For example we may know that the target function is linear and that the noise is Gaussian. This justifies using linear hypothesis space along with a square loss function which lead us straight to a least squares learning procedure. \n",
      "\n",
      "There are other common inductive biases that can guide us. For example researchers often use [Occam's razor](http://en.wikipedia.org/wiki/Occam's_razor) to prefer simple models over more complex alternatives. As is the case with all sorts of inductive biases, we should be careful in our interpretation of this principle. In the paper entitled [The Role of Occam\u2019s Razor in Knowledge Discovery\n",
      "](http://homes.cs.washington.edu/~pedrod/papers/dmkd99.pdf)  **Domingos** argues that there are two different interpretations of Occam's razor eith respect to machine learning: The first interpretation says that between two models with the same generalization error, the simpler model should be chosen because simplicity is a desirable goal by itself. This, Domingos argues, is natural and resonable. The second interpretation is that between two models with the same training error we should select the simpler model  because it will lead to smaller generalization error. This interpretation is much more problematic and many times proven to be false. For example some [ensemble methods](http://en.wikipedia.org/wiki/Ensemble_learning) that combines the results of many simple learners are obviously more complex than any individual learner but also often performs much better than any of the simple learners. \n",
      "\n",
      "\n",
      "Even if you seek for a solution on a domain where there is an \"established\" good learning method you should still be sure to check if this method fits your particular problem. A good example is the learning methods known as [deep learning](http://en.wikipedia.org/wiki/Deep_learning) which are neural networks with many hidden layers. In recent years we witness remarkable success of deep learning in image recognition problems where they now (2014) achieve near-to-human accuracy. This is done insipite of the great comlexity of the model which raises the concern for overfitting. Two new results, one in the paper  entitled [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images](http://arxiv.org/abs/1412.1897) and one in [Intriguing properties of neural networks](http://arxiv.org/abs/1312.6199), demonstrate cases where deep learning fails. So, again, no silver bullet  and you, the implemetor, have to apply your best judgment as to whether to use deep learning for your image recognition problem. \n",
      "\n",
      "> ## Lesson learned: <font color=\"blue\"> Selecting the learning procedure to a specific problem is an art rather than a science</font>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"ModelSelection\">Model</span> selection\n",
      "[Model selection](http://en.wikipedia.org/wiki/Model_selection) is the task of selecting a learning model from a set of candidate models. It is used, for example, to select the degree of polynomial in least square regression problem. The main problem here is that we should base our selection criterion on reduction of the **test** or **out-of-sample** error which is an unknown quantity when all that we have at our disposal is the **train** or in-sample error. \n",
      "\n",
      "## <font color=\"brown\"><span id=\"ComplexityControl\">Procedures</span> for controlling the complexity of learning models</font>\n",
      "\n",
      "When fitting models, it is possible to decrease the bias by adding parameters, but doing so may increase vastly the variance and thus result in overfitting. The idea behind some of the methods for model selection is to add a term that penalize the complexity of the model. A model with more parameters will only be selected if the penalize performance measure is better. Examples for such penalized measures are:\n",
      "* [Akaike Information Criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion) (AIC). See [Facts and fallacies of the AIC](http://robjhyndman.com/hyndsight/aic/) for a good discussion,\n",
      "* [Bayesian Information Criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion)(BIC), also called Schwarz criterion and \n",
      "* [Minimum description length](http://en.wikipedia.org/wiki/Minimum_description_length)(MDL).\n",
      "* [Structure Risk Minimization](http://en.wikipedia.org/wiki/Structural_risk_minimization) (SRM) is an approach by which we consider a nested family of hypothesis sets (say polynomials of increased order) and select the hypothesis model in the series whose sum of empirical risk and VC confidence is minimal. \n",
      "\n",
      "For more information, please consider [This StackExchange discussion](http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other)\n",
      "and also [this AIC vs. BIC discussion](http://methodology.psu.edu/eresources/ask/sp07).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"brown\"><span id=\"validation\">Validation</span></font>\n",
      "The idea of validation is to take a set ${\\mathcal K}$ of $K$ data points and to estimate the out-of-sample error by $E_{\\tt val} = \\frac{1}{K} \\sum_{{\\bf x_k} \\in {\\mathcal K}} e(h({\\mathbf x_k}),y_k)$ where $h$ is an estimator from the hypothesis set that is constructed based on the $N-K$ remaining data points. Provided that the erros are independent, $E_{\\tt val}$ provides an unbiased estimation to the out-of-sample error $E_{\\tt out}$. As $K$ increases, the variance of the estimation decreases but it measures the out-of-sample-error of a worse estimator since it is based on only $N-K$ points.\n",
      "\n",
      "\n",
      "Validation set is used to select a good model out of several alternatives or to fix the value of a parameter of a learning method such as, for example, regularization constant. After validation is finished, we can compute the final approximation based on the entire data set. In this case $E_{\\tt val}$ will be a biased **optimistic** estimator of $E_{\\tt out}$, as we [explain](#validationExample) below.\n",
      "\n",
      "#### <font color=\"brown\"><span id=\"sizeOfK\">How</span> large should $K$ be?</font>\n",
      "There is an inherent conflict here. If $K$ is large then the variance in the estimation of the out-of-sample error is small but the variance of the optimal estimator is large because it is based on $N-K$ points. \n",
      "\n",
      "We can analyse this process as a learning problem: The hypothesis set consists of all optimal solutions to different values of the parameter we wish to estimate. It might be discrete (like selecting between completly different models) or continuous like selecting the value of the regularization parameter. The sample set is the set of points in the validation set since we base our choice on $E_{\\tt val}$. [Guyon](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1337&rep=rep1&type=pdf) studied this problem and concluded that $\\frac{K}{N}$ should be inversly proportional to the square root of the number of free adjustable parameters. Of course $N$ should be large enough in the first place so the results make sense.\n",
      "\n",
      "\n",
      "> ### <font color=\"blue\"> Rule of thumb: $K \\approx \\frac{N}{5}$ usually works well</font>\n",
      "\n",
      "\n",
      "### <font color=\"brown\"><span id=\"CrossValidation\">Cross</span> validation</font>\n",
      "IN many practical situations, the size of the data set is limited and we do not want to \"waste\" a large number of points for validation. This is where the technique of [cross validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) is useful. k-fold cross validation works as follows: We randomly split the data set into K-folds of size $\\frac{K}{N}$. Then we iterate $K$ times. On each iteration we train on $K-1$ folds and validate on the fold left untouched. We compute $E_{\\tt val}$ by averaging the validation errors on the $K$ iterations. This metodology allows us not to have to set aside a large set of points for validation at the expence of increased computation time. We should note that this procedute result in an optimisticly biased estimation of the out-of-sample-error. \n",
      "\n",
      "> ### <font color=\"blue\"> Rule of thumb: 10-fold cross-validation usually works well</font>\n",
      "\n",
      "\n",
      "A discussion of cross-validation and its relation to AIC, BIC is presented in the blog entitled [Why every statistician should know about cross-validation](http://robjhyndman.com/hyndsight/crossvalidation/). Another interesting discussion concerns [comparing different dpecies of dross-validation](http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm). Sylvain Arlot wrote or a comprehensive survey of cross validation techniques for model selection ([download PDF](http://www.di.ens.fr/willow/pdfs/2010_Arlot_Celisse_SS.pdf)).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"green\"><span id=\"validationExample\">Illustrative</span> example - why validation errors are optimistic</font>\n",
      "We consider using a validation process for selcting the best model out of $L$ models. This is done by looking at $L$ validation errors $e_1,e_2,\\ldots,e_L$ and selecting the model for which the valudation error is minimal. Assume that the each model errors are unbiased estimation of the generalization error. Then the expected error of the model selected by validation wil be smaller. [For example](http://math.stackexchange.com/questions/786392/expectation-of-minimum-of-n-i-i-d-uniform-random-variables) the expectation of $L$ uniform random variables in $[0,1]$ is $\\frac{1}{n+1}$ as seen in the numerical example below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "\n",
      "N    = 100000\n",
      "e1    = np.array([random.uniform(0.0, 1.0) for i in xrange(N)])\n",
      "e2    = np.array([random.uniform(0.0, 1.0) for i in xrange(N)])\n",
      "e      = np.minimum(e1,e2)\n",
      "print np.mean(e1),np.mean(e2),np.mean(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.499150838753 0.500549074073 0.332706820017\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"LossFunctionSelection\">Selecting</span> the loss functions\n",
      "\n",
      "In many practical cases the selection of the loss function is done on the basis of computational convenience. In particular we require, in many cases, that the loss function is convex (or the corresponding computation problem may be intractable computationaly). Another desired property of loss functions is smoothness which allows using [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) optimization methods. \n",
      "\n",
      "Sometimes it is possible to give a theoretical justification for selecting a specific loss functions. In the paper [Are Loss Functions All the Same?](http://web.mit.edu/lrosasco/www/publications/loss.pdf), the authors argue that the  hinge loss (presented below) is the recommended convex loss function for classification. Some loss functions [have meaning](http://hunch.net/?p=269) made explicit by a theorem in each case, see e.g. our discussion of the square loss below. Other loss functions are derived by plausible arguments, e.g. the hinge loss is derived by the margin maximization pronciple behind the [SVM](http://en.wikipedia.org/wiki/Support_vector_machine) model.\n",
      "\n",
      "Readers interested in more insight into characteristics of various loss function are referred to the work of **[Robert C. Williamson](http://users.cecs.anu.edu.au/~williams/)**, for example the paper entitled Loss Functions ([download PDF](http://users.cecs.anu.edu.au/~williams/papers/P193.pdf)).\n",
      "\n",
      "\n",
      "\n",
      "## <font color=\"brown\"><span id=\"PopularLossFunctionRegression\">Popular</span> loss functions for regression</font>\n",
      "\n",
      "The lost function is a function of one variable $t=w-y$. We consider the functions\n",
      "\n",
      "* Square loss  \n",
      "   >$L(w,y) = (w-y)^2$. \n",
      "   \n",
      "   In this case the optimal estimator is given by the conditional expectation $f_o(x) = E[Y \\mid X=x]$, also called a **regressor** in the statistics literature. The score loss have many nice properties and lead to well posed and easi-to-solve optimization problems. However, it is sensitive to **outliers** in tha data: isolated points that are far from the desired target function contribute heavilty to the square loss. As a result, outliers must be removed from the data set before trying to use this loss function.\n",
      "* Absolute loss \n",
      "> $L(w,y) = |w-y|$ \n",
      "  \n",
      "  In this case the optimal estimator is given by the conditional median $f_o(x) = \\tt{median}(Y \\mid X=x)$. This loss function is more robust to outliers than the square loss.\n",
      "* $\\epsilon$-insensitive loss \n",
      "  > $L(w,y) = \\max \\{ |w-y|-\\epsilon,0 \\}$\n",
      "\n",
      "All the above loss functions are convex.\n",
      "\n",
      "\n",
      "##<font color=\"brown\"><span id=\"PopularLossFunctionClassification\">Popular</span> loss functions for classification</font>\n",
      "\n",
      "We assume the labels $y$ to be from the set $\\{1,-1\\}$. The loss function is a function of one variable $t=wy$ and we consider the following functions\n",
      "* Square loss which is the convex loss function \n",
      "  > $L(w,y) = (w-y)^2 = (1-wy)^2$\n",
      "\n",
      "* [Hinge loss](http://en.wikipedia.org/wiki/Hinge_loss) is the loss function related to the maximal margin principle behing soft margin Support Vector Machines classifiers.\n",
      "  >$L(w,y) = \\max \\{ 1-wy,0 \\} =|1-wy|_{+}$\n",
      "  \n",
      "  The hinge loss is not smooth at one point but we still can use [subgadient methods](http://en.wikipedia.org/wiki/Subgradient_method)\n",
      "\n",
      "* [Logistic loss](http://www.hongliangjie.com/wp-content/uploads/2011/10/logistic.pdf), or cross entropy loss, is the convex loss functiona that leads to logistic regression\n",
      "  > $L(w,y) =  \\log(1+e^{-wy})$\n",
      "\n",
      "* [Exponential loss](http://en.wikipedia.org/wiki/AdaBoost) which is the loss of the boosting learning algorithm \n",
      "  > $L(w,y) = e^{-wy}$ \n",
      "\n",
      "* The 0-1 loss \n",
      "  > $L(w,y) = I(w \\neq y) = I(wy<0) $\n",
      "  \n",
      "  that counts the number of missclassified points. Here $I(\\cdot)$ denotes the indicator function that has value 1 if the condition is true (predicted value equal to data value) and zero otherwise. \n",
      "  \n",
      "  The optimal solution for the 0-1 loss function is $f_o ({\\mathbf x})=g_{k({\\bf x})}$ where $g_{k({\\bf x})}$ such that $ P(Y=g_{k({\\bf x})} \\mid X={\\mathbf x})= \\max_{g_i \\in \\{1,-1\\}} P(Y=g_i \\mid X={\\mathbf x})$. This solution is known as the **Bayes classifier**\n",
      "\n",
      "  This loss function is not smooth and not convex and leads to intactable optimization problems. Thus many times we use **surrogate loss function** that are convex approximation to the 0-1 loss.The hinge, logistic loss and exponential loss functions are all convex upper bounds on the 0-1 loss function ([download PDF](http://www.cs.utah.edu/~piyush/teaching/13-9-print.pdf)). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"CombatingOverFitting\">Combating</span> overfiting\n",
      "\n",
      "[Overfitting](http://en.wikipedia.org/wiki/Overfitting) is probably the most serious problem whne applying a machine learning technique to a real problem. This term is a **comparative** terms and refers to the situation where the in-sample of one alternative reduces while the out-of-sample error increases. Overfitting manifests itself when the number of samples does not match the complexity of the model. In terms of Bias-Variance analysis, overfitting is the case where the bias is decreased because of the flexibility of the model but the variance is very high. In terms of PAC learning, overfitting happen when the increase complexity of the model is not respected by suppying more data points. Some view overfitting as a phenomenom in which the model adapts itself to the noise in the data and it halucinates the target function instead of genealizing. \n",
      "\n",
      "In his course, Yaser Abu-Mostafa gives a rule of thumb to the minimal number of samples that we should have in order to provide generalization and avoid overfitting: \n",
      "> ## <font color=\"blue\">The number of samples should be $\\geq 10 \\cdot \\tt{VC}(\\mathcal{H})$</font>\n",
      "\n",
      "In practice, a great deal of though are invested in any machine learning algorithm to handle and minimize the risk of overfitting.\n",
      "\n",
      "The VC dimension of an hypothesis set depends linearly on the number of predictors which lead us to the [feature selection](http://en.wikipedia.org/wiki/Feature_selection) approach for reducing overfitting. This approach will be discussed in the next section that is concerned with dimentionality dimension. Next we discuss in more details the concept of regularization. \n",
      "\n",
      "We note that overfitting may represet itself in many ways, not all of them obvious. A nice post entitled [Clever Methods of Overfitting](http://hunch.net/?p=22) discuss some of the more subtle ways in which overfitting manifest itself and how to recover.\n",
      "\n",
      "\n",
      "\n",
      "## <font color=\"brown\"><span id=\"regularization\">Reducing</span> the complexity of the hypothesis set by regularization</font>\n",
      "\n",
      "[Regularization](http://en.wikipedia.org) is a common apparoach to tackle overfitting. We can arrive to regularization from many different angles: The classical regularization theory aims at correcting the inherent ill-poseness of the learning problem. In the present context, I like to view overfitting from the PAC learning view point that is to reduce the complexity of a model via reduction of its VC dimension. \n",
      "\n",
      "As an example we consider a linear predictors of the form $\\mathbf{w^Tx}$ where $\\mathbf{w}$ is a coefficient vector. We can reducing the VC dimension by requesting that $||\\mathbf{w}|| < C$ for some constant $C$. Small values of $C$ correspond to large regularization while large values of $C$ result is small regularization. There are two equivalent formulations for regularization in the linear case. The first is based directly on the desire to restrics the value of $\\mathbf{w}$\n",
      "\n",
      ">$\\min_{||\\mathbf{w}||^2 < C} ( X\\mathbf{w} - {\\mathbf y})^T( X\\mathbf{w} - {\\mathbf y})$\n",
      "\n",
      "We note that in the formulation above we can replace $||\\mathbf{w}||^2 = \\sum_{i=1}^{p} w_i^2$ by $||\\mathbf{w}||^2 = {\\mathbf \\beta^T \\mathbf{w}} = \\sum_{i=1}^{p} \\gamma_i w_i^2 = \\mathbf{w}^T \\Lambda \\mathbf{w}$ where $\\Lambda$ is a diagonal matrix or the more general [Tikhonov regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization) $\\mathbf{w}^T \\Gamma^T \\Gamma \\mathbf{w} = ||\\Gamma \\mathbf{w}||^2$\n",
      "\n",
      "The second for for the regularization problem, obtained from the first via Lagrange multipiers, is expressed as **unconditional** minimization problem taht, for Tikhonov regularization, is\n",
      "> $\\min ( X\\mathbf{w} - {\\mathbf y})^T( X\\mathbf{w} - {\\mathbf y}) +||\\Gamma \\mathbf{w}||^2$\n",
      "\n",
      "\n",
      "where, in th simplest formulation $\\Gamma = \\lambda I$ and  $\\lambda$ is inversly proportional to $C$. The above problem has a one step solution $\\mathbf{w} = (X^TX + \\Gamma^T \\Gamma)^{-1} X^T\\mathbf{y}$\n",
      "\n",
      "This approach to regularization is also called **weigh decay** (for neural networks) or **shrinkage** (in statistical literature).\n",
      "\n",
      "We note that we can regularize using other norms. In particular the $L_1$ norm, called [**lasso**](http://statweb.stanford.edu/~tibs/lasso.html) in the statistical literature is attractive as it produces sparse models and thus do also feature selection in the process of regularization. However, this penalty is not differentiable which complicates optimization algorithms."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"green\"><span id=\"regularizationExamplePolynomial\">Regularization</span> example - polynomial</font>\n",
      "Fit a polynomial of degree 15 (blue curve) over a sample of 12 points using ordinary least squares (red curve) and ridge regression learning algorithm with two choices of the smoothing parameter (solid and dashes green curves)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.grid_search  import GridSearchCV\n",
      "\n",
      "\n",
      "def createTrainingSet(deg,xsampled):\n",
      "    N = len(xsampled)\n",
      "    X = np.zeros((N,deg+1))\n",
      "    for n in xrange(N):\n",
      "        X[n] = np.array([ np.power(xsampled[n],i) for i in xrange(deg+1) ] )\n",
      "    return X\n",
      "\n",
      "def outError1(target,est,deg):\n",
      "    M = 10000\n",
      "    e = 0.0\n",
      "    for m in xrange(M):\n",
      "        x     = random.uniform(-1.,1.)\n",
      "        xp    = [np.power(x,i) for i in xrange(deg+1)]\n",
      "        ypred = est.predict(xp)\n",
      "        e    += (target(x)-ypred)**2\n",
      "    e /= float(M)\n",
      "    return e\n",
      "\n",
      "#==========================================================================\n",
      "# Generate train set\n",
      "#==========================================================================\n",
      "xtrain = np.linspace(-1, 1, 12)\n",
      "ytrain = poly15(xsampled)\n",
      "deg    = 10\n",
      "X      = createTrainingSet(deg,xtrain)\n",
      "\n",
      "#==========================================================================\n",
      "# Fit\n",
      "#==========================================================================\n",
      "#--------------------------------------------\n",
      "# Fit with linear regressor\n",
      "#--------------------------------------------\n",
      "est_lin   = LinearRegression(fit_intercept=False)\n",
      "est_lin.fit(X,ytrain)\n",
      "\n",
      "\n",
      "#--------------------------------------------\n",
      "# Fit with ridge regressor and auto determination\n",
      "# of alpha\n",
      "#--------------------------------------------\n",
      "print '\\nDetermine alpha by cross-validation\\n',50*'-'\n",
      "alphas_to_try = np.arange(0.1,10000,20)\n",
      "model = Ridge(fit_intercept=False)\n",
      "grid  = GridSearchCV(estimator=model,param_grid=dict(alpha=alphas_to_try))\n",
      "grid.fit(X,ytrain)\n",
      "print 'Best alpha found is ',grid.best_estimator_.alpha\n",
      "\n",
      "est_ridge_auto = Ridge(fit_intercept=False)\n",
      "est_ridge_auto.fit(X,ytrain,grid.best_estimator_.alpha)\n",
      "\n",
      "#==========================================================================\n",
      "# Print out-of-sample errors\n",
      "#==========================================================================\n",
      "print '\\nTrain set of 12 points. Fitting twelve order polynomial\\n',70*'-'\n",
      "print 'Out-of-sample error polyfit by linear regression                      ',\\\n",
      "       outError1(poly15,est_lin,deg)\n",
      "print 'Out-of-sample error polyfit by ridge-reg alpha=20 (cross-validation)  ',\\\n",
      "       outError1(poly15,est_ridge_auto,deg)\n",
      "\n",
      "\n",
      "#==========================================================================\n",
      "# Plot all\n",
      "#==========================================================================\n",
      "xp = np.linspace(-1, 1, 100)\n",
      "yp = poly15(xp)\n",
      "plt.plot(xp,poly15(xp),label=\"target\")                                 # Plot target function\n",
      "\n",
      "plt.scatter(xsampled,ytrain,c='b',marker='o',s=30)    # Plot training set\n",
      "\n",
      "\n",
      "yfit_lin = []\n",
      "for xx in xp:\n",
      "    yfit_lin.append(est_lin.predict([np.power(xx,i) for i in xrange(deg+1)]))\n",
      "plt.plot(xp,yfit_lin,'r',label=\"OLS\")\n",
      "\n",
      "\n",
      "yfit_ridge_auto = []\n",
      "for xx in xp:\n",
      "    yfit_ridge_auto.append(est_ridge_auto.predict([np.power(xx,i) for i in xrange(deg+1)]))\n",
      "plt.plot(xp,yfit_ridge_auto,'g',label=\"ridge\")\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.ylim([-2.,6.])\n",
      "plt.xlim([-1.5,2.])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Determine alpha by cross-validation\n",
        "--------------------------------------------------\n",
        "Best alpha found is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20.1\n",
        "\n",
        "Train set of 12 points. Fitting twelve order polynomial\n",
        "----------------------------------------------------------------------\n",
        "Out-of-sample error polyfit by linear regression                       "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8.93277859342\n",
        "Out-of-sample error polyfit by ridge-reg alpha=20 (cross-validation)   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.42047052379\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(-1.5, 2.0)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x15705780>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"brown\"><span id=\"ReducingNumberOfFeatures\">Reducing</span> the number of features</font>\n",
      "\n",
      "In many cases the VC dimension of an hypothesis space grows linearly with the number of features. Therefore it is advisable to reduce the number of features as much as possible without compromising the predictive power of the model. This it what [Feature selection](http://en.wikipedia.org/wiki/Feature_selection) is for. We discuss in more details feature selection in the context of dimensionality reduction techniques."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"CombatingCurseDim\">Combating</span> the curse of dimensionality\n",
      "The second major problem in learning models is the curse of dimentionality caused by the number of features. Large number of features also result in more complex models and is thus also also related to overfitting. So reducing the number of features is a good idea for most learning problems. We note that there are learning methods such as random forests in which feature selection is an inherent part of the learning procedure and thus we do not need to explicitly reduce the number of features.  \n",
      "\n",
      "## <font color=\"brown\"><span id=\"FeatureEng\">Feature</span> engineering</font>\n",
      "[Feature engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) is more an art than science. It is the process where one uses his best judgment about a particular domain to select a, small as possible, number of features that carry enough information so that models constructed based on those features provide good prediction power.\n",
      "\n",
      "Consider, for example, a computer vision application. The row data for such applications is images. Gray level image is represented as a matrix of integer values between 0 and 255. A very small 100x100 images have no less than **10,000** pixels. So if we use raw pixels as features then we have very high dimensional feature space. New methods known as [deep learning](http://en.wikipedia.org/wiki/Deep_learning) work on this high dimensional. More traditional  predictive models for computer vision are based on many sophisticated  [features](http://en.wikipedia.org/wiki/Feature_%28computer_vision%29) developed over the years which enable reducing considerably the dimension of the feature space.\n",
      "\n",
      "\n",
      "\n",
      "## <font color=\"brown\"><span id=\"FeatureSel\">Feature</span> selection</font>\n",
      "\n",
      "[Feature selection](http://en.wikipedia.org/wiki/Feature_selection) by only keeping the most relevant\n",
      "variables from the original dataset. In some cases, e.g. linear regression, there will be specific methods based on the special statistical properties of the estimator. In other cases we will use more generic techniques.\n",
      "\n",
      "#### Variable ranking\n",
      "We take and ranking function and remove all features with low rank. An example for a ranking function is the variance of a feature vector. If the variance is small than there is no information in that vector that help us predict the outcome and so this feature can be safely removed. There are, of course, many other ranking functions some general and some specific to a modelling method. For example in linear regression problems we can compute the **[$p$-value](http://en.wikipedia.org/wiki/P-value)** of each feature and remove features with high p-value.\n",
      "\n",
      "#### Subset selection\n",
      "In ideal case we would take some **ranking** measure of an variable (for example any measure used in model selection such as cross validation, MDL, AIC,BIC, etc will work) and then we search for the best subset of features that minimizes this measure. In practice this problem is not computationaly tractable so we use iterative methods. There are two main approaches:\n",
      "* **Forward selection**: Start from an empty set of features and add a feature at a time\n",
      "* **Backward selection**: Start from the full set of features and remove the least informative feature \n",
      "\n",
      "and many variants. \n",
      "\n",
      "The interested reader is encouraged to consider the survey papers [A review of feature selection techniques in bioinformatics (2007)](http://www.ncbi.nlm.nih.gov/pubmed/17720704) by Saeys Y1, Inza I, Larra\u00f1aga P. and [An Introduction to Variable and Feature Selection](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf) by [E. Guyon](http://clopinet.com/isabelle/) and [A. Elisseeff](http://ei.is.tuebingen.mpg.de/person/andre).\n",
      "\n",
      "## <font color=\"brown\"><span id=\"DimRed\">Dimensionality</span> reduction</font>\n",
      "\n",
      "[Dimensionality reduction](http://en.wikipedia.org/wiki/Dimensionality_reduction) by exploiting the redundancy of\n",
      "the input data and by finding a smaller set of new variables, each being a combination of the input variables containing basically the same information as the input variables. For more information, please look at [Dimension Reduction: A Guided Tour](http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf)(2010) by By [**Christopher J. C. Burges**](http://research.microsoft.com/en-us/people/cburges/) and [A survey of dimensionality reduction techniques](http://arxiv.org/ftp/arxiv/papers/1403/1403.2877.pdf) by [**C.O.S. Sorzano**](http://biocomp.cnb.csic.es/~coss/publications.htm), [**J. Vargas**](http://www.researchgate.net/profile/Javier_Vargas3/publications) and  [**A. Pascual\u2010Montano**](http://www.cnb.csic.es/~pascual/)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"AssesingModelQuality\">Assesing</span> model performance\n",
      "\n",
      "###<font color=\"red\">**This section is still under construction**</font>\n",
      "\n",
      "\n",
      "One of the interesting characteristics of practicing machine learning is that we are interested in a quantity that can not be measured - the test error. Thus we have to come with an idea of how to estimate the test error. The main idrea is to keep aside a **test set** randomly sampled from the data set. Once we have trained a model, we can then estimate its test error by evaluating on on the test set and comparing the value predicted by our model to the true label.\n",
      "\n",
      "\n",
      "[Classification Validation](http://inductivebias.com/Blog/classification-validation/)\n",
      "\n",
      "[Comparing the Bootstrap and Cross-Validation](http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0)\n",
      "\n",
      "http://scott.fortmann-roe.com/docs/MeasuringError.html\n",
      "\n",
      "[Evaluating classifiers: Precision, Recall, AUCs and ROCs](https://shapeofdata.wordpress.com/2015/01/05/precision-recall-aucs-and-rocs/)\n",
      "\n",
      "[Adding Cost Functions to ROCR performance objects](http://www.r-bloggers.com/adding-cost-functions-to-rocr-performance-objects/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29)\n",
      "\n",
      "[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)\n",
      "\n",
      "[General observations about the leave-one-error](https://books.google.co.il/books?id=WMfjDWTRGnYC&pg=PA113&lpg=PA113&dq=leave+one+out+estimator+for+error&source=bl&ots=LkpJkda2It&sig=XsvsKQvH72cdc6YyjPuG8gQijUk&hl=iw&sa=X&ei=b4aJVJC6Ks_naKjOgKgL&ved=0CFAQ6AEwBQ#v=onepage&q=leave%20one%20out%20estimator%20for%20error&f=false)\n",
      "\n",
      "[Cross validation](https://books.google.co.il/books?id=wzK8BAAAQBAJ&pg=PA27&lpg=PA27&dq=leave+one+out+estimator+for+error&source=bl&ots=7kQ6pMsHh-&sig=_d5g1C50h6RIjTYoQKAfLjZALJQ&hl=iw&sa=X&ei=4ImJVNy1MZbzaty2gpgC&ved=0CCQQ6AEwATgK#v=onepage&q=leave%20one%20out%20estimator%20for%20error&f=false)\n",
      "\n",
      "\n",
      "[An Overview of General Performance Metrics Binary Classifier Systems](http://sebastianraschka.com/PDFs/articles/performance_metrics.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"FeaturesScaling\">Features</span> scaling\n",
      "A data set may congtain many features. Thee features may be given at arbitrary scales and/or measuring units. For example consider health data composed of three features: height measured in meters,weight measured in Kilograms and blood pressure measured in millimeters of mercury units. Anothe example is when we  have two distance features, one measured in meters and the other in millimeters. We would like the result of our model to be independent on the measuring units. If we pick up a model that is sensitive to the measuring units then we better use **Feature scaling** before applying our modelling technique. For more information, consider the [Wikipedia entry](http://en.wikipedia.org/wiki/Feature_scaling) or [this post](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html) by Sebastian Raschka.\n",
      "\n",
      "Note that the scaling should be performed based **only** on the train/validation sets and we should not use in any way the data in the test set (for example to in derermination of the mean and variance for computing z-scores) for the scaling. Using data from the test set will bias our results to being toot optimistic, namely contiribute to overfitting. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#  <span id=\"EnsembleLearning\">Ensemble</span> learning\n",
      "\n",
      "###<font color=\"red\">**This section is still under construction**</font>\n",
      "\n",
      "The idea of [Ensemble learning](http://en.wikipedia.org/wiki/Ensemble_learning) is to combine, in clever ways, various models. If done correctly, the combined model can have vastly reduced variance at the cost of only slightly increasing the variance  of the combined model as compatred to the individual models that are the buildiong block of the ensembled model.\n",
      "\n",
      "Two theories to explain the success of ensemble methods:\n",
      "1. Ensemble increase margins (SVM ratioanle)\n",
      "2. Ensembles reduce variance at only stight increase of bias (??)\n",
      "\n",
      "[Bias-Variance analysis of ensemble methods](http://www-scf.usc.edu/~csci567/17-18-bias-variance.pdf)\n",
      "\n",
      "[Bias-Variance tradefoff in bootstrated ensembles](http://epubs.surrey.ac.uk/7133/2/smith.pdf)\n",
      "\n",
      "[Bias-Variance of SVM towars asemble methods](http://web.engr.oregonstate.edu/~tgd/publications/jmlr-valentini-bv.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"LearningTheoryResources\">References</span>  \n",
      "\n",
      "## <span id=\"ResourcesCourses\">Courses</span> \n",
      "* [Machine learning](https://www.coursera.org/course/machlearning) by [**Pedro Domingos**](http://homes.cs.washington.edu/~pedrod/) \n",
      "* [Learnind from data](http://work.caltech.edu/telecourse.html) by [**Yaser Abu-Mostafa**](https://work.caltech.edu/index.html)\n",
      "* [Introduction to statistical learning](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/) by [**Rob Tibshirani**](http://statweb.stanford.edu/~tibs/) and [**Trevor Hastie**](http://web.stanford.edu/~hastie/)\n",
      "* [Machine Learning](https://www.coursera.org/course/ml) and [CS229 (2007)](http://videolectures.net/stanfordcs229f07_machine_learning/) ([CS229 (2014)- problems sets and solutions](http://cs229.stanford.edu/)), by [**Andrew Ng**](http://cs.stanford.edu/people/ang/)\n",
      "* [Machine Learning and Computational Statistics](http://cs.nyu.edu/~dsontag/courses/ml14/)  by [**David Sontag**](http://cs.nyu.edu/~dsontag/) ([load slides](http://cs.nyu.edu/~dsontag/courses/ml14/slides/lecture4.pdf))\n",
      "* [CS229 - Machine Learning](http://videolectures.net/stanfordcs229f07_machine_learning/) by [**Andrew Ng**](http://cs.stanford.edu/people/ang/)\n",
      "* [Harvard CS109 Data Science Course](http://www.kdnuggets.com/2013/11/harvard-cs109-data-science-course-resources-free-online.html) ([GitHub directory with Python code](https://github.com/cs109/content))\n",
      "* [CS340 - Machine learning (2007)](http://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/) by [Kevin Murphy](http://www.cs.ubc.ca/~murphyk/)\n",
      "* [Feature extraction](http://clopinet.com/isabelle/Projects/ETH/) by [**E. Guyon**](http://clopinet.com/isabelle/)\n",
      "\n",
      "\n",
      "\n",
      "* [Machine learning courses online](http://fastml.com/machine-learning-courses-online/)\n",
      "* [Awsome courses](https://github.com/prakhar1989/awesome-courses) - List of awesome university courses for learning Computer Science!\n",
      "\n",
      "\n",
      "\n",
      "## <span id=\"ResourcesBooks\">Books</span> \n",
      "\n",
      "#### Theory of learning\n",
      "* [Foundations of Machine Learning](http://mitpress.mit.edu/books/foundations-machine-learning-0) by [**Mehryar Mohri**](http://cs.nyu.edu/~mohri/), [**Afshin Rostamizadeh**](http://research.google.com/pubs/author36233.html) and [**Ameet Talwalkar**](http://www.cs.ucla.edu/~ameet/)\n",
      "* [Understanding Machine Learning: From Theory to Algorithms](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html) by [**Shai Shalev-Shwartz**](http://www.cs.huji.ac.il/~shais/) and [**Shai Ben-David**](https://cs.uwaterloo.ca/~shai/)\n",
      "\n",
      "#### Feature extraction\n",
      "* [Feature Extraction, Foundations and Applications](http://clopinet.com/isabelle/Projects/NIPS2003/call-for-papers.html) - Collection of papers\n",
      "\n",
      "#### Text books\n",
      "* [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) by [**Trevor Hastie**](http://web.stanford.edu/~hastie/), [**Rob Tibshirani**](http://statweb.stanford.edu/~tibs/software.html) and [**Jerome H. Friedman**](http://statweb.stanford.edu/~jhf/) - free for download\n",
      "* [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/data.html)  by [**Trevor Hastie**](http://web.stanford.edu/~hastie/), [**Rob Tibshirani**](http://statweb.stanford.edu/~tibs/software.html), [**Gareth James**](http://www-bcf.usc.edu/~gareth/) and [**Daniela M. Witten**](http://faculty.washington.edu/dwitten/) - free for download\n",
      "* [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/PRML/) by [**Christopher M. Bishop**](http://research.microsoft.com/en-us/um/people/cmbishop/index.htm)\n",
      "* [Machine Learning: a Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/) by [**Kevin P Murphy**](http://www.cs.ubc.ca/~murphyk/)\n",
      "\n",
      "* [Understanding Machine Learning: From Theory to Algorithms](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html) by [**Shai Shalev-Shwartz**](http://www.cs.huji.ac.il/~shais/) and [**Shai Ben-David**](https://cs.uwaterloo.ca/~shai/)\n",
      "\n",
      "\n",
      "* [Introduction to Probability, Statistics, and Random Processes](http://www.probabilitycourse.com/) - An online book with some videos\n",
      "* [Statistical inference for everyone](http://web.bryant.edu/~bblais/statistical-inference-for-everyone-sie.html)\n",
      "* [Probabilistic Programming & Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) - The chapters of this book are actually IPYthon notebooks\n",
      "\n",
      "#### Optimization\n",
      "* [Convex optimization](http://stanford.edu/~boyd/cvxbook/) by [**Stephen P. Boyd**](http://web.stanford.edu/~boyd/) and [**Lieven Vandenberghe**](http://www.seas.ucla.edu/~vandenbe/) - free to download.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##  <span id=\"ResourcesPapersSlidesSites\">Papers,</span> Slides and Sites\n",
      "\n",
      "#### Collections\n",
      "* [Data Science ontology](http://www.datascienceontology.com/) - Beautiful visualization of many terms and methods in machine learning\n",
      "* [The Current State of Machine Intelligence](https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1) - start-ups related to data science in 2014\n",
      "* [Tutorials by Andrew M. Moore](http://www.cs.cmu.edu/~awm/tutorials.html)\n",
      "* [Pattern Classification](https://github.com/rasbt/pattern_classification#machine-learning-algorithms-and-classification-models) - A collection of tutorials and examples for solving and understanding machine learning and pattern classification tasks\n",
      "\n",
      "#### Theory of learning\n",
      " * [A Few Useful Things to Know about Machine Learning](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) by **Pedro Domingos**\n",
      " * [The Role of Occam\u2019s Razor in Knowledge Discovery](http://homes.cs.washington.edu/~pedrod/papers/dmkd99.pdf) by **Pedro Domingos**\n",
      " * [Occam\u2019s Razor in science](http://www.swarthmore.edu/Documents/academics/philosophy/Baker%20OR%20biogeography.pdf) by [**A. BAKER**](http://www.swarthmore.edu/philosophy/alan-baker-associate-professor-acting-chair)\n",
      " * [What the No Free Lunch Theorems really mean](http://www.santafe.edu/media/workingpapers/12-10-017.pdf) by [**David H. Wolpert**](http://www.santafe.edu/about/people/profile/David%20Wolpert)\n",
      " * [No Free Lunch Theorems site](http://www.no-free-lunch.org/)\n",
      " \n",
      "\n",
      "#### Bias-Variance\n",
      " * [A Unified Bias-Variance Decomposition for Zero-One and Squared Loss](http://homes.cs.washington.edu/~pedrod/papers/aaai00.pdf) by **Pedro Domingos**\n",
      " * [Variance and bias for general loss functions](http://www-bcf.usc.edu/~gareth/research/bv.pdf) by [**Gareth James**](http://www-bcf.usc.edu/~gareth/)\n",
      " * [Bias Plus Variance Decomposition for ZeroOne Loss Functions](http://robotics.stanford.edu/~ronnyk/biasVar.pdf) by [**Ron Kohavi**](http://ai.stanford.edu/~ronnyk/) and [**David H. Wolpert**](http://www.santafe.edu/about/people/profile/David%20Wolpert)\n",
      " * [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
      " * [Bias-Variance Analysis of Local Classification Methods](https://www.statistik.tu-dortmund.de/~bischl/mypapers/bias_variance_analysis_of_local_classification_methods.pdf)\n",
      " * [Practical Bias Variance Decomposition](http://www.cs.waikato.ac.nz/~remco/bias.pdf)\n",
      " * [A Bias-Variance Analysis of a Real World Learning Problem](http://www.liacs.nl/~putten/library/2004vdPvSBiasVariance.pdf)\n",
      "\n",
      "#### Comparison of algorithms\n",
      "* [Top 10 algorithms in data mining (2008)](http://ailab.arizona.edu/mis510/slides/15_data_mining.pdf) \n",
      "* [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? (2014)](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf) and [the followup post](http://www.r-bloggers.com/the-geometry-of-classifiers/)\n",
      "* [An Empirical Comparison of Supervised Learning Algorithms (2006)](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf), [Watch the talk ](http://videolectures.net/solomon_caruana_wslmw/)\n",
      "\n",
      "#### Cross validation and bootstrap\n",
      "* [A survey of cross-validation procedures for model selection](http://www.di.ens.fr/willow/pdfs/2010_Arlot_Celisse_SS.pdf) by [Sylvain Arlot](http://www.di.ens.fr/~arlot/).\n",
      "* [A Study of CrossValidation and Bootstrap for Accuracy Estimation and Model Selection (1995)](http://ai.stanford.edu/~ronnyk/accEst.pdf) by Ron Kohavi\n",
      "* [Bootstrap Techniques for Error Estimation (1987)](http://pdf.aminer.org/000/348/011/use_of_bootstrap_samples_in_quadratic_classifier_design.pdf)\n",
      "\n",
      "* [Loss functions](http://users.cecs.anu.edu.au/~williams/papers/P193.pdf) by **[Robert C. Williamson](http://users.cecs.anu.edu.au/~williams/)**\n",
      "\n",
      "#### Feature selection (extraction)\n",
      "* Feature selection: stability, algorithms, and evaluation - PhD thesis by **Pavel Krizek** ([download pdf](ftp://cmp.felk.cvut.cz/pub/cvl/articles/krizek/Krizek-TR-2008-16.pdf))\n",
      "* [A review of feature selection techniques in bioinformatics (2007)](http://www.ncbi.nlm.nih.gov/pubmed/17720704) by Saeys Y1, Inza I, Larra\u00f1aga P.\n",
      "* [An Introduction to Variable and Feature Selection](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf) by [E. Guyon](http://clopinet.com/isabelle/) and [A. Elisseeff](http://ei.is.tuebingen.mpg.de/person/andre)\n",
      "* [NIPS benchmark on feature selection](http://clopinet.com/isabelle/Projects/NIPS2003/)\n",
      "* [Feature selection algorithms](http://sebastianraschka.com/Articles/2014_sequential_sel_algos.html) (2014, with Python code) by [**Sebastian Raschka**](http://sebastianraschka.com/index.html)\n",
      "* [On Feature Selection, Bias-Variance, and Bagging](http://www.cs.cornell.edu/~mmunson/publications/docs/fs-bagging.pdf)\n",
      "\n",
      "#### Overfitting\n",
      "* [Clever Methods of Overfitting](http://hunch.net/?p=22)\n",
      "* [Common Pitfalls in Machine Learning](http://danielnee.com/?p=155)\n",
      "\n",
      "#### Dimension reduction\n",
      "* [Dimension Reduction: A Guided Tour](http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf)(2010) by By [**Christopher J. C. Burges**](http://research.microsoft.com/en-us/people/cburges/)\n",
      "* [Dimensionality Reduction - A Short Tutorial](http://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf) (2006) by [**Ali Ghodsi**](https://www.cs.berkeley.edu/~alig/**](https://www.cs.berkeley.edu/~alig/)\n",
      "* [A survey of dimensionality reduction techniques](http://arxiv.org/ftp/arxiv/papers/1403/1403.2877.pdf) by [**C.O.S. Sorzano**](http://biocomp.cnb.csic.es/~coss/publications.htm) (2010), [**J. Vargas**](http://www.researchgate.net/profile/Javier_Vargas3/publications) and  [**A. Pascual\u2010Montano**](http://www.cnb.csic.es/~pascual/)\n",
      "* [Implementing a PCA in Python step-by-step](http://sebastianraschka.com/Articles/2014_kernel_pca.html) (2014) by [**Sebastian Raschka**](http://sebastianraschka.com/index.html), see also [kernel PCA](http://sebastianraschka.com/Articles/2014_kernel_pca.html) article\n",
      "\n",
      "#### Assesing performance\n",
      "* [Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)\n",
      "* [An Overview of General Performance Metrics Binary Classifier Systems](http://sebastianraschka.com/PDFs/articles/performance_metrics.pdf) by [**Sebastian Raschka**](http://sebastianraschka.com/index.html)\n",
      "\n",
      "#### Other\n",
      "* [Leakage in Data Mining](http://dstillery.com/wp-content/uploads/2014/05/Leakage-in-Data-Mining-Formulation-Detection-and-Avoidance.pdf)\n",
      "* [Machine learning gremlins - a Video](https://www.youtube.com/watch?v=tleeC-KlsKA)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"ResourcesCompetitions\">Competitions</span> \n",
      " * [The CoIL Challenge 2000](http://www.liacs.nl/~putten/library/cc2000/) ([full report](http://www.liacs.nl/~putten/library/cc2000/report2.html)): Predicting and Explaining Caravan Policy Ownership\n",
      " * [Kaggle competitions](https://www.kaggle.com/competitions)\n",
      " * [DrivenData](http://www.drivendata.org/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"ResourcesSoftware\">Software</span> \n",
      "I will concentrate on C++ and Python libraries because that what I use. The reader should be aware that there are also very large and respectable libraries in other languages, especially R, Java, Julia and Mathlab.\n",
      "\n",
      "### Sites\n",
      "* [mloss](http://mloss.org/software/) - Machine Learning Open Source Software\n",
      "* [Awsome machine learning](https://github.com/josephmisiti/awesome-machine-learning) - A curated list of awesome Machine Learning frameworks, libraries and software.\n",
      "* [Abridged List of Machine Learning Topics](http://www.startup.ml/resources)\n",
      "* [Summary of machine learning libs available in python](http://fraka6.blogspot.co.il/2010/09/summary-of-machine-learning-libs.html)\n",
      "\n",
      "### Libraries\n",
      "\n",
      "### C/C++\n",
      "\n",
      "#### Machine learning\n",
      "* [OpenCv](http://docs.opencv.org/modules/ml/doc/ml.html) - OpenCv is the main library for computer vision applications with a large collection of machine learning procedures. Include binding to Python\n",
      "* [RT-Rank](http://research.engineering.wustl.edu/~amohan/)\n",
      "* [xgboost](https://github.com/tqchen/xgboost) General purpose gradient boosting library\n",
      "* [VFML](http://www.cs.washington.edu/dm/vfml/)\n",
      "* Classified by Quinlan: [C4.5](http://www.rulequest.com/Personal/), [C5.0](http://www.rulequest.com/see5-info.html)\n",
      "* [dlib](http://dlib.net/) ([paper](http://jmlr.org/papers/volume10/king09a/king09a.pdf))\n",
      "* [xgboost](https://github.com/tqchen/xgboost) General purpose gradient boosting library\n",
      "* [mlpack](http://www.mlpack.org/about.html) ([paper](http://www.mlpack.org/mlpack_biglearn.pdf),([design considerations](http://www.igglybob.com/mlpack_future.pdf))\n",
      "* [Wafles](http://waffles.sourceforge.net/) ([paper](http://jmlr.org/papers/volume12/gashler11a/gashler11a.pdf))\n",
      "* [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)fast out-of-core learning. [see also](http://www.startup.ml/blog/2014/9/21/what-every-machine-learning-package-can-learn-from-vowpal-wabbit)\n",
      "  \n",
      "#### Numerical linear algebra\n",
      "* [Armadillo](http://arma.sourceforge.net/)\n",
      "* [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page)\n",
      "\n",
      "#### Optimization\n",
      "* [NLopt](http://ab-initio.mit.edu/wiki/index.php/NLopt) - Have binding to many languages including Python, Julia, R and Mathlab\n",
      "* [MOE](https://github.com/Yelp/MOE) - Metric Optimization Engine with C++ and Python interfaces\n",
      "\n",
      "\n",
      "#### Python\n",
      "* [Scikitlearn](http://scikit-learn.org/stable/) - Machine learning in Python\n",
      "* [Statsmodels](http://statsmodels.sourceforge.net/) - Statistics in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data science sites and progarms\n",
      "* [startup.ml](http://www.startup.ml/) \n",
      "* [Bayes impact](http://www.bayesimpact.org/)\n",
      "* [DataKind](http://www.datakind.org/)\n",
      "* [Data science bowl](http://www.datasciencebowl.com/)\n",
      "* [DataCamp](https://www.datacamp.com/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 align=center>**[Back to top](#TOP)**</h3>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}