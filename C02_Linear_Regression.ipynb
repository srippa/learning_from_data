{
 "metadata": {
  "name": "",
  "signature": "sha256:48aa57b043ef2712155dd5e0ca7cb4c6d4ece236fba2d2be540102e3fcbd6739"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* <a href=\"#LinearRegressionMotivation\" data-ajax=\"false\">Introduction to linear regression</a>   \n",
      "* <a href=\"#LinearRegressionOLS\" data-ajax=\"false\">Square loss - Ordinary Least Squares</a>\n",
      "    * <a href=\"#LinearRegressionOLSCode\" data-ajax=\"false\">Code for OLS</a>\n",
      "    * <a href=\"#LinearRegressionOLSLibraries\" data-ajax=\"false\">Solving  OLS with statsmodels and scikitlearn</a>\n",
      "    * Code samples\n",
      "      * <a href=\"#AdvertiseDataSet\" data-ajax=\"false\">Advertise data set</a>\n",
      "      * <a href=\"#NormalizeDataSet\" data-ajax=\"false\">Normalize the data set</a>\n",
      "      * <a href=\"#ProstateDataSet\" data-ajax=\"false\">Prostate data set</a> normalize, split to test and train\n",
      "* Regularizsation\n",
      "  * <a href=\"#RidgeRegression\" data-ajax=\"false\">Ridge regression</a>\n",
      "    * $\\lambda$ and effective degrees of freedom\n",
      "    * Estimating $\\lambda$ by cross validation\n",
      "  * <a href=\"#LASSO\" data-ajax=\"false\">LASSO and LAR</a>\n",
      "  * Elastic-net\n",
      "* Features selection (dmensionality reduction)\n",
      "  * <a href=\"#FeatureSelectionOLS\" data-ajax=\"false\">Forward feature selection</a>\n",
      "  * <a href=\"#PCA\" data-ajax=\"false\">PCA</a>\n",
      "*  <a href=\"#RBF\" data-ajax=\"false\">Radial Basis Functions</a> - extension of linear regression that is able to model complex surfaces and yet remains a linear model\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enabe inline drawings, if you like\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"LinearRegressionMotivation\">Introduction</span>  to linear regression\n",
      "The derivation is based on the assumption that the target fynction is a linear combinations of functions of the predictors. Thus the hypothesios set consists of functions of the form\n",
      "$g(\\mathbf{x})=w_0 +w_1x_1, \\ldots, + w_Nx_p$. We note that the model is linear in the coefficients of the elements $\\{x_1,\\ldots,x_p\\}$ but those elements may be non linear functions of the predictors. \n",
      "\n",
      "For example suppose the sample space $\\mathcal{X} \\subset \\mathbb{R}^2$ and we consider predictors that are polynomials of second degree $g(\\mathbf{x})=w_0 +w_1x_1+w_2x_2 + w_3 x_1x_2 + w_4 x_1^2 + w_5x_2^2$. A popular class of non-linera transformation, that constantly achieves good regression results, is that of  <a href=\"#RBF\" data-ajax=\"false\">Radial Basis Functions</a> (RBF). \n",
      "\n",
      "Linear regression is attractive because of it leads to easily computable solutions which are often very interpretable.\n",
      "\n",
      "# <span id=\"LinearRegressionOLS\">Square</span> loss function - Ordinary Least Squares\n",
      "\n",
      "[OLS](http://en.wikipedia.org/wiki/Ordinary_least_squares) Determine the coefficients of the linear model under the square loss function, i.e. by minimizing $\\min_w ||\\bf{Xw}-y||_2^2$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$. The main problem with linear regression is that it relies heavily on the assumption of independence of the models terms $X_i$. If that assumption does not hold then the matrix $\\bf{X}$ have close to linear dependent columns and the solution becomes unstable. \n",
      "\n",
      "The simple structutre of OLS allows rigouros analysis and determination of many statistical properties such as p-values and standard errors for each coefficeint. This allows performing hypothesis testing and statistical interpretation of the coefficients and errors, see [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) or [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/data.html). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------\n",
      "# Add my modules and data directories to the path\n",
      "#------------------------------------------------------\n",
      "import os\n",
      "import sys\n",
      "\n",
      "myHome = %pwd                              # THe home of this notebook\n",
      "module_dir = os.path.join(myHome,'src')     # my python modues\n",
      "data_dir   = os.path.join(myHome,'data')    # my python modues\n",
      "sys.path.insert(0,module_dir)               # Add my python modeules to module pat\n",
      "\n",
      "print data_dir\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "D:\\Dropbox\\ipython\\notebooks\\learning_from_data\\data\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ipython_utils as iputils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  \n",
      "import numpy.linalg as la\n",
      "\n",
      "import scipy as sp\n",
      "import scipy.stats as st\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import matplotlib.pyplot as plt     ## matplotlib - plots\n",
      "\n",
      "from pandas import DataFrame\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <span id=\"LinearRegressionOLSCode\">Solution</span> of OLS \n",
      "\n",
      "Below is the code to computer OLS determine interesting statistical properties. The code is provided only for pedagogical purposes and is **not** of production quality.  The reader is referred to [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) and [ISSL](http://www-bcf.usc.edu/~gareth/ISL/data.html) for detauls on the derivation of the various quantities.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(iputils)\n",
      "iputils.print_python_file(module_dir+\"/sr_linreg.py\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style type=\"text/css\">.highlight .hll { background-color: #ffffcc }\n",
        ".highlight  { background: #f8f8f8; }\n",
        ".highlight .c { color: #408080; font-style: italic } /* Comment */\n",
        ".highlight .err { border: 1px solid #FF0000 } /* Error */\n",
        ".highlight .k { color: #008000; font-weight: bold } /* Keyword */\n",
        ".highlight .o { color: #666666 } /* Operator */\n",
        ".highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
        ".highlight .cp { color: #BC7A00 } /* Comment.Preproc */\n",
        ".highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
        ".highlight .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
        ".highlight .gd { color: #A00000 } /* Generic.Deleted */\n",
        ".highlight .ge { font-style: italic } /* Generic.Emph */\n",
        ".highlight .gr { color: #FF0000 } /* Generic.Error */\n",
        ".highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
        ".highlight .gi { color: #00A000 } /* Generic.Inserted */\n",
        ".highlight .go { color: #888888 } /* Generic.Output */\n",
        ".highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
        ".highlight .gs { font-weight: bold } /* Generic.Strong */\n",
        ".highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
        ".highlight .gt { color: #0044DD } /* Generic.Traceback */\n",
        ".highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
        ".highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
        ".highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
        ".highlight .kp { color: #008000 } /* Keyword.Pseudo */\n",
        ".highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
        ".highlight .kt { color: #B00040 } /* Keyword.Type */\n",
        ".highlight .m { color: #666666 } /* Literal.Number */\n",
        ".highlight .s { color: #BA2121 } /* Literal.String */\n",
        ".highlight .na { color: #7D9029 } /* Name.Attribute */\n",
        ".highlight .nb { color: #008000 } /* Name.Builtin */\n",
        ".highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
        ".highlight .no { color: #880000 } /* Name.Constant */\n",
        ".highlight .nd { color: #AA22FF } /* Name.Decorator */\n",
        ".highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
        ".highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
        ".highlight .nf { color: #0000FF } /* Name.Function */\n",
        ".highlight .nl { color: #A0A000 } /* Name.Label */\n",
        ".highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
        ".highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
        ".highlight .nv { color: #19177C } /* Name.Variable */\n",
        ".highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
        ".highlight .w { color: #bbbbbb } /* Text.Whitespace */\n",
        ".highlight .mf { color: #666666 } /* Literal.Number.Float */\n",
        ".highlight .mh { color: #666666 } /* Literal.Number.Hex */\n",
        ".highlight .mi { color: #666666 } /* Literal.Number.Integer */\n",
        ".highlight .mo { color: #666666 } /* Literal.Number.Oct */\n",
        ".highlight .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
        ".highlight .sc { color: #BA2121 } /* Literal.String.Char */\n",
        ".highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
        ".highlight .s2 { color: #BA2121 } /* Literal.String.Double */\n",
        ".highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
        ".highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
        ".highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
        ".highlight .sx { color: #008000 } /* Literal.String.Other */\n",
        ".highlight .sr { color: #BB6688 } /* Literal.String.Regex */\n",
        ".highlight .s1 { color: #BA2121 } /* Literal.String.Single */\n",
        ".highlight .ss { color: #19177C } /* Literal.String.Symbol */\n",
        ".highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
        ".highlight .vc { color: #19177C } /* Name.Variable.Class */\n",
        ".highlight .vg { color: #19177C } /* Name.Variable.Global */\n",
        ".highlight .vi { color: #19177C } /* Name.Variable.Instance */\n",
        ".highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>  \n",
        "<span class=\"kn\">import</span> <span class=\"nn\">numpy.linalg</span> <span class=\"kn\">as</span> <span class=\"nn\">la</span>\n",
        "\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy</span> <span class=\"kn\">as</span> <span class=\"nn\">sp</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">as</span> <span class=\"nn\">st</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_critval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_pval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">tstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ts</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">tstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">ts</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"mf\">2.0</span><span class=\"o\">*</span><span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_critval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_pval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">fstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">fs</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">fstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">fs</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"c\"># Residual sum of squares</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Residual standard error - estimation for the Standard deviation of the error terms</span>\n",
        "<span class=\"c\"># Also known as standard error</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n",
        "\n",
        "  \n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Rsquared</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">Rsqr</span>       <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"n\">TSS</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">Rsqr</span>\n",
        "\n",
        "<span class=\"c\"># Compute F score. High F score indicates that mnultiple linear regression</span>\n",
        "<span class=\"c\"># have at least one significant coefficient</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Fscore</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">p</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">a</span>          <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">TSS</span><span class=\"o\">-</span><span class=\"n\">rss_c</span><span class=\"p\">)</span><span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">b</span>          <span class=\"o\">=</span> <span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> \n",
        "    <span class=\"n\">F</span>          <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">b</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">F</span>\n",
        "\n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">AdjustedRsquared</span><span class=\"p\">(</span><span class=\"n\">rsqr</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">adj</span>        <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">rsqr</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mf\">1.</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">adj</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyLinearRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">Xin</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">column_stack</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)),</span> <span class=\"n\">Xin</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "        \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_</span>          <span class=\"o\">=</span> <span class=\"n\">X</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span>          <span class=\"o\">=</span> <span class=\"n\">y</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span>     <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>   <span class=\"c\"># Normal matrix</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span> <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span>      <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"c\"># Quantities that measure global quality of fit</span>\n",
        "        <span class=\"c\">#  RSE   - Global lack of fit</span>\n",
        "        <span class=\"c\">#  R**2  - The proportion of variablity in Y that is explained by the model</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>        <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rsqr_</span>       <span class=\"o\">=</span> <span class=\"n\">Rsquared</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span>          <span class=\"o\">=</span> <span class=\"n\">Fscore</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fprob_</span>      <span class=\"o\">=</span> <span class=\"n\">f_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span><span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"c\"># Estimating quality of fit for different predictors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>     <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">StdError_</span><span class=\"p\">())</span>      \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critval_</span>    <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"o\">/</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>              <span class=\"c\"># How important is a coefficient</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span><span class=\"p\">()</span>    \n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:])</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "    <span class=\"c\"># Compute confidence inbterval for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">confinterval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">CI</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"n\">critval</span> <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"p\">)</span>\n",
        "        <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">,</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span><span class=\"p\">):</span>\n",
        "            <span class=\"n\">ci_low</span>  <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">-</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">ci_high</span> <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">+</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">CI</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">ci_low</span><span class=\"p\">,</span><span class=\"n\">ci_high</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">CI</span>\n",
        "\n",
        "    <span class=\"c\"># Compute p-values for all predictors. Each p-value is the probability that we get the value of the preduictor</span>\n",
        "    <span class=\"c\"># that we got,under the hypothesis that the predictor value is zero (that is, the predictor is not</span>\n",
        "    <span class=\"c\"># impportant)</span>\n",
        "    <span class=\"c\"># If the value is small, e.g. &lt; 0.005 then we reject the null hypothesis and conclude that the coefficient</span>\n",
        "    <span class=\"c\"># is not neglible</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">pval_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">PV</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">PV</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">PV</span>\n",
        "            \n",
        "    <span class=\"c\"># Compute the standard errors for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">StdError_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"n\">Xinvdiag</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"p\">)</span>\n",
        "              \n",
        "        <span class=\"n\">SE</span>  <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">Xinvdiag</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">SE_i</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n",
        "            <span class=\"n\">SE</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">SE_i</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">return</span> <span class=\"n\">SE</span>\n",
        "\n",
        "\n",
        " \n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyRidgeRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">lamda</span><span class=\"p\">,</span><span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">p</span>                <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> \n",
        "        <span class=\"n\">Xlambda</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lamda</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">inv</span>              <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">Xlambda</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">if</span> <span class=\"n\">fit_intercept</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>  <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n",
        "        <span class=\"k\">else</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"n\">inv</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span><span class=\"p\">))</span>\n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "   \n",
        " \n",
        "<span class=\"c\"># Computing standard error (intercept and slope) for simple linear regression</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">StdError</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>         <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">n</span>     <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">print</span> <span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n",
        "    <span class=\"n\">x</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"s\">&#39;TV&#39;</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">xmean</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">xmean_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">xmean</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">xt</span>         <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">xmean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "    <span class=\"n\">SE_B0_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">];</span>\n",
        "    <span class=\"n\">SE_B1_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">];</span>\n",
        "    \n",
        "    <span class=\"n\">SE_B0_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">xmean_sqr</span><span class=\"o\">/</span><span class=\"n\">xt</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"n\">SE_B1_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">/</span><span class=\"n\">xt</span>\n",
        "\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B0 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B1 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr</span>\n",
        "    \n",
        "    <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B0_sqr</span><span class=\"p\">),</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B1_sqr</span><span class=\"p\">)]</span>\n",
        "  \n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "<IPython.core.display.HTML at 0x3a2f588>"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  \n",
      "import numpy.linalg as la\n",
      "\n",
      "import scipy as sp\n",
      "import scipy.stats as st\n",
      "\n",
      "def t_critval(df,confint=0.95):\n",
      "    ''' eturn critical value for a confidence interval for the t-distribution with df degrees of freedom '''\n",
      "    t_rv        = st.t(df)\n",
      "    alpha_h     = (1.-confint)/2.0\n",
      "    critval     = t_rv.ppf(1.0-alpha_h)  # Taking the poaitive value of critval\n",
      "    return critval\n",
      "\n",
      "def t_pval(df,tstat):\n",
      "    ''' Return probability of obtaiuning value of tstat or higher '''\n",
      "    t_rv        = st.t(df)\n",
      "    ts          = np.abs(tstat)\n",
      "    pval        = t_rv.sf(ts)  \n",
      "    return 2.0*pval\n",
      "\n",
      "def f_critval(n,p,confint=0.95):\n",
      "    ''' eturn critical value for a confidence interval for the t-distribution with df degrees of freedom '''\n",
      "    f_rv        = st.f(p,n-p-1)\n",
      "    alpha_h     = (1.-confint)/2.0\n",
      "    critval     = f_rv.ppf(1.0-alpha_h)  # Taking the poaitive value of critval\n",
      "    return critval\n",
      "\n",
      "def f_pval(n,p,fstat):\n",
      "    ''' Return probability of obtaiuning value of tstat or higher '''\n",
      "    f_rv        = st.f(p,n-p-1)\n",
      "    fs          = np.abs(fstat)\n",
      "    pval        = f_rv.sf(fs)  \n",
      "    return pval\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS(Y,ypred):\n",
      "    return np.sum((Y-ypred)**2)\n",
      "\n",
      "# Residual standard error - estimation for the Standard deviation of the error terms\n",
      "# Also known as standard error\n",
      "def RSE(Y,ypred,p):\n",
      "    n = len(Y)\n",
      "    return np.sqrt(RSS(Y,ypred)/float(n-p-1))\n",
      "\n",
      "  \n",
      "# Compute R^2 score for an estimator\n",
      "def Rsquared(estimator,X,Y):\n",
      "    ypred      = estimator.predict(X)\n",
      "    ypred_mean = np.mean(ypred)\n",
      "    rss_c      = RSS(Y,ypred)\n",
      "    TSS        = np.sum((Y-ypred_mean)**2)\n",
      "    Rsqr       = 1.0 - (rss_c/TSS)\n",
      "    return Rsqr\n",
      "\n",
      "# Compute F score. High F score indicates that mnultiple linear regression\n",
      "# have at least one significant coefficient\n",
      "def Fscore(estimator,X,Y):\n",
      "    n          = X.shape[0]\n",
      "    p          = X.shape[1]\n",
      "    ypred      = estimator.predict(X)\n",
      "    ypred_mean = np.mean(ypred)\n",
      "    rss_c      = RSS(Y,ypred)\n",
      "    TSS        = np.sum((Y-ypred_mean)**2)\n",
      "    a          = (TSS-rss_c)/ float(p)\n",
      "    b          = rss_c/float(n-p-1) \n",
      "    F          = a/b\n",
      "    return F\n",
      "\n",
      "# Compute R^2 score for an estimator\n",
      "def AdjustedRsquared(rsqr,n,p):\n",
      "    adj        = 1.0 - ( (1.0-rsqr)* (float(n)-1.)/(float(n)-float(p) ) )\n",
      "    return adj\n",
      "\n",
      "\n",
      "class MyLinearRegressor(object):\n",
      "    def fit(self, Xin, y):\n",
      "        \"\"\"Fits estimator to data. \"\"\"\n",
      "\n",
      "        X = np.column_stack( (np.ones(len(Xin)), Xin) )\n",
      " \n",
      "        self.n_          = Xin.shape[0]\n",
      "        self.p_          = Xin.shape[1]\n",
      "        \n",
      "        self.X_          = X\n",
      "        self.y_          = y\n",
      "        self.X_norm_     = X.transpose().dot(X)   # Normal matrix\n",
      "        self.X_norm_inv_ = la.inv(self.X_norm_)\n",
      "        self.coef_       = self.X_norm_inv_.dot(X.transpose().dot(y))\n",
      "        self.ypred_      = self.predict(Xin)\n",
      "\n",
      "        # Quantities that measure global quality of fit\n",
      "        #  RSE   - Global lack of fit\n",
      "        #  R**2  - The proportion of variablity in Y that is explained by the model\n",
      "        self.rse_        = RSE(self.y_,self.ypred_,self.p_+1)  # estimator for std of errors\n",
      "        self.rsqr_       = Rsquared(self,Xin,y)\n",
      "        self.F_          = Fscore(self,Xin,y)\n",
      "        self.fprob_      = f_pval(self.n_,self.p_,self.F_)\n",
      " \n",
      "        # Estimating quality of fit for different predictors\n",
      "        self.stderr_     = np.array(self.StdError_())      \n",
      "        self.critval_    = t_critval(self.n_-self.p_)\n",
      "        self.zscore_     = self.coef_/self.stderr_              # How important is a coefficient\n",
      "        self.pval_       = self.pval_()    \n",
      "\n",
      "        # set state of ``self``\n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict response of ``X``. \"\"\"\n",
      "        # compute predictions ``pred``\n",
      "        pred = X.dot(self.coef_[1:]) + self.coef_[0]\n",
      "        return pred\n",
      "\n",
      "    # Compute confidence inbterval for all predictors\n",
      "    def confinterval(self,confint=0.95):\n",
      "        CI = []\n",
      "        critval = t_critval(self.n_-self.p_,confint)\n",
      "        for (c,s) in zip(self.coef_,self.stderr_):\n",
      "            ci_low  = c - critval*s\n",
      "            ci_high = c + critval*s\n",
      "            CI.append( (ci_low,ci_high))\n",
      "        return CI\n",
      "\n",
      "    # Compute p-values for all predictors. Each p-value is the probability that we get the value of the preduictor\n",
      "    # that we got,under the hypothesis that the predictor value is zero (that is, the predictor is not\n",
      "    # impportant)\n",
      "    # If the value is small, e.g. < 0.005 then we reject the null hypothesis and conclude that the coefficient\n",
      "    # is not neglible\n",
      "    def pval_(self):\n",
      "        PV = []\n",
      "        for t in self.zscore_:\n",
      "            PV.append(t_pval(self.n_-self.p_,t))\n",
      "        return PV\n",
      "            \n",
      "    # Compute the standard errors for all predictors\n",
      "    def StdError_(self):\n",
      "        sigma_est     = self.rse_  # estimator for std of errors\n",
      "        Xinvdiag      = np.diag(self.X_norm_inv_)\n",
      "              \n",
      "        SE  = []\n",
      "        for v in Xinvdiag:\n",
      "            SE_i = sigma_est*np.sqrt(v)\n",
      "            SE.append(SE_i)\n",
      "\n",
      "        return SE\n",
      "\n",
      "\n",
      " \n",
      "class MyRidgeRegressor(object):\n",
      "    def fit(self, X, Y, lamda,fit_intercept=True):\n",
      "        \"\"\"Fits estimator to data. \"\"\"\n",
      "\n",
      "        p                = X.shape[1] \n",
      "        Xlambda          = X.transpose().dot(X) + lamda*np.eye(p)\n",
      "        inv              = la.inv(Xlambda)\n",
      "\n",
      "        if fit_intercept:\n",
      "            self.intercept_  = Y.mean()\n",
      "        else:\n",
      "            self.intercept_ = 0.0\n",
      "        self.coef_       = inv.dot(X.transpose().dot(Y-self.intercept_))\n",
      "\n",
      "        # set state of ``self``\n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict response of ``X``. \"\"\"\n",
      "        # compute predictions ``pred``\n",
      "        pred = X.dot(self.coef_) + self.intercept_\n",
      "        return pred\n",
      "\n",
      "   \n",
      " \n",
      "# Computing standard error (intercept and slope) for simple linear regression\n",
      "def StdError(estimator,X,Y):\n",
      "    ypred         = estimator.predict(X)\n",
      "    sigma_est     = RSE(Y,ypred)\n",
      "    sigma_sqr_est = sigma_est**2\n",
      "    n     = len(X)\n",
      "    print n,X.shape\n",
      "    x          = X['TV']\n",
      "    xmean      = np.mean(x)\n",
      "    xmean_sqr  = xmean**2\n",
      "    xt         = np.sum((x-xmean)**2)\n",
      "\n",
      "    SE_B0_sqr1 = sigma_sqr_est*Xinvdiag[0];\n",
      "    SE_B1_sqr1 = sigma_sqr_est*Xinvdiag[1];\n",
      "    \n",
      "    SE_B0_sqr  = sigma_sqr_est*( (1./float(n)) + (xmean_sqr/xt) )\n",
      "    SE_B1_sqr  = sigma_sqr_est /xt\n",
      "\n",
      "    print 'B0 ',SE_B0_sqr1,SE_B0_sqr\n",
      "    print 'B1 ',SE_B1_sqr1,SE_B1_sqr\n",
      "    \n",
      "    return [np.sqrt(SE_B0_sqr),np.sqrt(SE_B1_sqr)]\n",
      "  \n",
      "       \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <span id=\"LinearRegressionOLSLibraries\">Solution</span> of OLS using statsmodels and scikitlearn\n",
      "\n",
      "** [statsmodels](http://statsmodels.sourceforge.net/) **\n",
      "-----------------------\n",
      "A lentghty and interesting tutorial on using [statsmodels OLS function](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html)  for linear regression is discussed here. Also the statsmodels documentation have nice examples The advantage of using statsmodels rather than scikit-learn (which will be discussed later) is the rich set of statistical tests that can be performed. See a [nice example](http://mpastell.com/2013/04/19/python_regression/) of using OLS from statsmodels.\n",
      "\n",
      "** [Scikitlearn](http://scikit-learn.org/stable/index.html) **\n",
      "-------------------\n",
      "Scikitlearn is more a machine learning library than a statistical package and thus its [linear regression procedure](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) focuses more on prediction and less on statistical inference as done in statsmodels. From the results, it seems that both packages use the same underlying numerical procedures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------\n",
      "# Calling to the OLS code\n",
      "#------------------------------------\n",
      "def manualOLS(X,Y):\n",
      "    my_lin_est = MyLinearRegressor()\n",
      "    my_lin_est.fit(X,Y)\n",
      "\n",
      "    print my_lin_est.coef_\n",
      "    \n",
      "    print 'Y-mean        ',np.mean(Y)\n",
      "    print 'RSE(Std err)  ',my_lin_est.rse_ \n",
      "    print 'R**2          ',my_lin_est.rsqr_\n",
      "    print 'Adjusted R**2 ',AdjustedRsquared(my_lin_est.rsqr_,my_lin_est.n_,my_lin_est.p_+1)\n",
      "    print 'F-statistics  ',my_lin_est.F_\n",
      "    print 'F-prob        ',my_lin_est.fprob_\n",
      "    print '\\n'\n",
      "    print 'stderr ',my_lin_est.stderr_\n",
      "    print 'zscore ',my_lin_est.zscore_\n",
      "    print 'pval   ',my_lin_est.pval_\n",
      "    print 'confint',my_lin_est.confinterval()\n",
      "\n",
      "#------------------------------------\n",
      "# Calling to statsmodels OLS \n",
      "#------------------------------------\n",
      "def statsModelsOLS(Xin,Y):\n",
      "    import numpy as np\n",
      "    import statsmodels.api as sm\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # OLS using numpy\n",
      "    #--------------------------------------------\n",
      "    X  = Xin.copy()\n",
      "    X  = sm.add_constant(X)        # Add also constant (bias) term\n",
      "\n",
      "    # Fit regression model\n",
      "    linest_1d = sm.OLS(Y, X).fit()\n",
      "\n",
      "    # Inspect the results\n",
      "    print linest_1d.summary()\n",
      "    print linest_1d.params\n",
      "\n",
      "    rsqr          = Rsquared(linest_1d,X,Y)\n",
      "    n             = X.shape[0]\n",
      "    p_plus_one    = X.shape[1]\n",
      "    print 'RSE(Std err)  ',RSE(Y,linest_1d.predict(X),p_plus_one)\n",
      "    print 'R**2          ',rsqr\n",
      "    print 'Adjusted R**2 ',AdjustedRsquared(rsqr,n,p_plus_one)\n",
      "\n",
      "#------------------------------------\n",
      "# Calling to scikitlearn OLS \n",
      "#------------------------------------    \n",
      "def scikitledarnOLS(X,Y,normalize =False):\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # Define linear regressor\n",
      "    #--------------------------------------------\n",
      "    est = LinearRegression(fit_intercept=True,normalize=normalize)\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # Fit to data to get parameters\n",
      "    #--------------------------------------------\n",
      "    est.fit(X,Y)\n",
      "\n",
      "    print 'Intercept ',est.intercept_\n",
      "    print 'Coeff     ',est.coef_      ,type(est.coef_) # access coefficients\n",
      "\n",
      "    print type(X)\n",
      "\n",
      "    rsqr  = Rsquared(est,X,Y)\n",
      "    n     = X.shape[0]\n",
      "    p     = X.shape[1]\n",
      "\n",
      "    print 'RSE(Std err)   ',RSE(Y,est.predict(X),p+1)\n",
      "    print 'R**2           ',rsqr\n",
      "    print 'Variance score ',est.score(X,Y)\n",
      "    print 'Adjusted R**2  ',AdjustedRsquared(rsqr,n,p+1)\n",
      "\n",
      "\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <span id=\"AdvertiseDataSet\">Advertise</span>  data set\n",
      "\n",
      "Taken from [ISSL](http://www-bcf.usc.edu/~gareth/ISL/data.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import sys\n",
      "\n",
      "myHome = %pwd                              # THe home of this notebook\n",
      "module_dir = os.path.join(myHome,'src')    # my python modues\n",
      "data_dir   = os.path.join(myHome,'..','data','ML')    # my python modues\n",
      "sys.path.insert(0,module_dir)\n",
      "\n",
      "ad_file = os.path.join(data_dir,'Advertising.csv')\n",
      "ad      = pd.read_csv(ad_file,header=0,index_col=0)\n",
      "\n",
      "Y    = ad['Sales']                 # Response\n",
      "X    = ad.drop('Sales',axis=1)\n",
      "\n",
      "statsModelsOLS(X,Y)\n",
      "print '\\nmanual OLS \\n',30*'='\n",
      "manualOLS(X,Y)\n",
      "print '\\nscikitledarn OLS \\n',30*'='\n",
      "scikitledarnOLS(X,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  Sales   R-squared:                       0.897\n",
        "Model:                            OLS   Adj. R-squared:                  0.896\n",
        "Method:                 Least Squares   F-statistic:                     570.3\n",
        "Date:                Sun, 16 Nov 2014   Prob (F-statistic):           1.58e-96\n",
        "Time:                        21:06:59   Log-Likelihood:                -386.18\n",
        "No. Observations:                 200   AIC:                             780.4\n",
        "Df Residuals:                     196   BIC:                             793.6\n",
        "Df Model:                           3                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "const          2.9389      0.312      9.422      0.000         2.324     3.554\n",
        "TV             0.0458      0.001     32.809      0.000         0.043     0.049\n",
        "Radio          0.1885      0.009     21.893      0.000         0.172     0.206\n",
        "Newspaper     -0.0010      0.006     -0.177      0.860        -0.013     0.011\n",
        "==============================================================================\n",
        "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
        "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
        "Kurtosis:                       6.332   Cond. No.                         454.\n",
        "==============================================================================\n",
        "const        2.938889\n",
        "TV           0.045765\n",
        "Radio        0.188530\n",
        "Newspaper   -0.001037\n",
        "dtype: float64\n",
        "RSE(Std err)   1.68982666825\n",
        "R**2           0.897210638179\n",
        "Adjusted R**2  0.89563733162\n",
        "\n",
        "manual OLS \n",
        "==============================\n",
        "[  2.93888937e+00   4.57646455e-02   1.88530017e-01  -1.03749304e-03]\n",
        "Y-mean         14.0225\n",
        "RSE(Std err)   1.68982666825\n",
        "R**2           0.897210638179\n",
        "Adjusted R**2  0.89563733162\n",
        "F-statistics   570.270703659\n",
        "F-prob         1.57522725609e-96\n",
        "\n",
        "\n",
        "stderr  [ 0.31270698  0.00139847  0.00863329  0.00588604]\n",
        "zscore  [  9.39822125  32.72482193  21.83757388  -0.17626321]\n",
        "pval    [1.4379966154091513e-17, 1.3835731297936898e-81, 1.6516169260416499e-54, 0.8602682299787543]\n",
        "confint [(2.3222064938156235, 3.5555722451032112), (0.043006754274658762, 0.04852253663613644), (0.17150449496206541, 0.20555553887434355), (-0.012645237534232825, 0.010570251449279668)]\n",
        "\n",
        "scikitledarn OLS \n",
        "==============================\n",
        "Intercept "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.93888936946\n",
        "Coeff      [ 0.04576465  0.18853002 -0.00103749] <type 'numpy.ndarray'>\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RSE(Std err)    1.68982666825\n",
        "R**2            0.897210638179\n",
        "Variance score  0.897210638179\n",
        "Adjusted R**2   0.89563733162\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <span id=\"NormalizeDataSet\">Normalize</span>  data set\n",
      "\n",
      "Normalization is done to have the same range of values for each of the predictors and help improving the stability of the solution. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize input data frame. Assumes all predictors are quantitive variables\n",
      "# do not touch constant column. Return\n",
      "#   normalized data frame, vectpor of means, vector of standard deviations\n",
      "# so that future inputs can be normalize before prediction\n",
      "def normalize(X):\n",
      "    Xnorm = X.copy()\n",
      "    Xmean = []\n",
      "    Xstd  = []\n",
      "    # now iterate over the remaining columns and create a new zscore column\n",
      "    for col in X.columns:\n",
      "        col_std  = Xnorm[col].std(ddof=1)\n",
      "        col_mean = Xnorm[col].mean()\n",
      "        if col_std > 0.0:\n",
      "            Xmean.append(col_mean)\n",
      "            Xstd.append(col_std)\n",
      "            Xnorm[col] = (Xnorm[col] - col_mean )/col_std\n",
      "        else:\n",
      "            Xmean.append(0.0)\n",
      "            Xstd.append(1.0)\n",
      "    return Xnorm,Xmean,Xstd\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <span id=\"ProstateDataSet\">Prostate</span>  data set\n",
      "\n",
      "The prostate data set that is presdented in page 3 of [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/). This data came from a research that  examined the correlation between the level of prostate specific antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectomy. The goal is to predict the log of PSA (lpsa) from a number of measurements including log cancer volume (lcavol), log prostate weight lweight, age, log of benign prostatic hyperplasia amount lbph, seminal vesicle invasion svi, log of capsular penetration lcp, Gleason score gleason, and percent of Gleason scores 4 or 5 pgg45. **svi** and **gleason** are categorical values while the other are quantitive variables. The response is a quatitive variable which make this a **regression** problem.\n",
      "\n",
      "Note that the correlations and linear fit results displayed in page 50 of [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) relate to the train set consisting of 67 persons rather than on the whole 97 persons in this data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "import sys\n",
      "\n",
      "#-------------------------------------------\n",
      "# Read the data set\n",
      "#-------------------------------------------\n",
      "myHome = %pwd                              # THe home of this notebook\n",
      "module_dir = os.path.join(myHome,'src')    # my python modues\n",
      "data_dir   = os.path.join(myHome,'..','data','ML')    # my python modues\n",
      "sys.path.insert(0,module_dir)\n",
      "\n",
      "prostate_file = os.path.join(data_dir,'prostate.csv')\n",
      "prostate      = pd.read_csv(prostate_file,header=0,delim_whitespace=True,index_col=0)\n",
      "\n",
      "#-------------------------------------------\n",
      "# Normalize, split to train and test sets\n",
      "#-------------------------------------------\n",
      "T  = prostate['train']   # Get column that splits between terain and test sets\n",
      "Y  = prostate['lpsa']    # Get response\n",
      "\n",
      "\n",
      "prostate.drop(['train','lpsa'],axis=1,inplace=True)     # Remove the 'train column'\n",
      "prostate,ProstateMean,ProstateStd = normalize(prostate) # Normalize predictor columns\n",
      "\n",
      "# Divide into training and test set based on T\n",
      "Xtrain = prostate[T=='T']\n",
      "Ytrain = Y[T=='T']\n",
      "\n",
      "Xtest  = prostate[T=='F']\n",
      "Ytest  = Y[T=='F']\n",
      "\n",
      "print 'Shape of train: ',Xtrain.shape,Ytrain.shape\n",
      "print 'Shape of test:  ',Xtest.shape,Ytest.shape\n",
      "\n",
      "\n",
      "statsModelsOLS(Xtrain,Ytrain)\n",
      "print '\\nmanualOLS \\n',30*'='\n",
      "manualOLS(Xtrain,Ytrain)\n",
      "print '\\nscikitledarnOLS \\n',30*'='\n",
      "scikitledarnOLS(Xtrain,Ytrain,normalize =False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shape of train:  (67, 8) (67,)\n",
        "Shape of test:   (30, 8) (30,)\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                   lpsa   R-squared:                       0.694\n",
        "Model:                            OLS   Adj. R-squared:                  0.652\n",
        "Method:                 Least Squares   F-statistic:                     16.47\n",
        "Date:                Sun, 16 Nov 2014   Prob (F-statistic):           2.04e-12\n",
        "Time:                        21:30:29   Log-Likelihood:                -67.505\n",
        "No. Observations:                  67   AIC:                             153.0\n",
        "Df Residuals:                      58   BIC:                             172.9\n",
        "Df Model:                           8                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "const          2.4649      0.089     27.598      0.000         2.286     2.644\n",
        "lcavol         0.6795      0.127      5.366      0.000         0.426     0.933\n",
        "lweight        0.2631      0.096      2.751      0.008         0.072     0.454\n",
        "age           -0.1415      0.101     -1.396      0.168        -0.344     0.061\n",
        "lbph           0.2101      0.102      2.056      0.044         0.006     0.415\n",
        "svi            0.3052      0.124      2.469      0.017         0.058     0.553\n",
        "lcp           -0.2885      0.155     -1.867      0.067        -0.598     0.021\n",
        "gleason       -0.0213      0.145     -0.147      0.884        -0.312     0.269\n",
        "pgg45          0.2670      0.154      1.738      0.088        -0.041     0.574\n",
        "==============================================================================\n",
        "Omnibus:                        0.825   Durbin-Watson:                   1.690\n",
        "Prob(Omnibus):                  0.662   Jarque-Bera (JB):                0.389\n",
        "Skew:                          -0.164   Prob(JB):                        0.823\n",
        "Kurtosis:                       3.178   Cond. No.                         4.47\n",
        "=============================================================================="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "const      2.464933\n",
        "lcavol     0.679528\n",
        "lweight    0.263053\n",
        "age       -0.141465\n",
        "lbph       0.210147\n",
        "svi        0.305201\n",
        "lcp       -0.288493\n",
        "gleason   -0.021305\n",
        "pgg45      0.266956\n",
        "dtype: float64\n",
        "RSE(Std err)   0.718507034723\n",
        "R**2           0.694371179677\n",
        "Adjusted R**2  0.652215480322\n",
        "\n",
        "manualOLS \n",
        "==============================\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Y-mean         2.45234508507\n",
        "RSE(Std err)   0.718507034723\n",
        "R**2           0.694371179677\n",
        "Adjusted R**2  0.652215480322\n",
        "F-statistics   16.47158487\n",
        "F-prob         2.04232650854e-12\n",
        "\n",
        "\n",
        "stderr  [ 0.09009504  0.12773498  0.09646341  0.10222755  0.1031118   0.12467976\n",
        "  0.15587896  0.14651579  0.1549552 ]\n",
        "zscore  [ 27.35925314   5.31982819   2.72697258  -1.38382296   2.03804576\n",
        "   2.44787594  -1.85074859  -0.14541122   1.72279321]\n",
        "pval    [3.2326824673462138e-35, 1.6812568165799359e-06, 0.0084043487155551731, 0.17162571506279675, 0.046036390010150217, 0.017364063891657032, 0.069213805601909525, 0.88488185185053925, 0.090164139251352762]\n",
        "confint [(2.2846531697319858, 2.6452126745155033), (0.42393103922377062, 0.9351252432521785), (0.070030234595327562, 0.45607589686975303), (-0.34602169206201694, 0.063092024989673923), (0.0038203316719637725, 0.4164727827716912), (0.055716963877636694, 0.55468423037255921), (-0.60040585916220257, 0.023420314255110986), (-0.31448245321229712, 0.27187237560640126), (-0.043108872637428186, 0.57702039687727491)]\n",
        "\n",
        "scikitledarnOLS \n",
        "==============================\n",
        "Intercept  2.46493292212\n",
        "Coeff      [ 0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006  -0.28849277\n",
        " -0.02130504  0.26695576] <type 'numpy.ndarray'>\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RSE(Std err)    0.718507034723\n",
        "R**2            0.694371179677\n",
        "Variance score  0.694371179677\n",
        "Adjusted R**2   0.652215480322\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"RidgeRegression\">Ridge</span> regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ridge regression determine the coefficients of the linear model by solving the minimization problem $\\min_w ||\\bf{Xw}-y||_2^2 + \\lambda ||w||_2^2$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$ where $\\lambda$ is a free parameter that has to be determined. The larger $\\lambda$ is the more it **shrinks** the value of the coefficients and maker the solution more reguralized. It solved the problem of stability inherent in linear regression at the price of extra parameter that has to be detetmined. \n",
      "\n",
      "To do a proper ridge regression we need to normalize the input matrix and to estimate the intercept term as the mean of the right hand side vector $y$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.grid_search  import GridSearchCV\n",
      "\n",
      "X = Xtrain.copy()\n",
      "Y = Ytrain.copy()\n",
      "print X.shape\n",
      "\n",
      "def dfRidgeRegressor(lamda,D):\n",
      "    df = 0.0\n",
      "    for d in D:\n",
      "        d2 = d*d\n",
      "        df += d2/(d2+lamda)\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_score(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    return np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_std_err(estimator,Xtest,Ytest):\n",
      "    ypred     = estimator.predict(Xtest)\n",
      "    ysqr      = (Ytest-ypred)**2\n",
      "    y_std_err = np.std(ysqr)/np.sqrt(len(ysqr)-1)\n",
      "    return y_std_err\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print '\\nDegrees of freedom for different values of lambda\\n',50*'-'\n",
      "P, D, Q = np.linalg.svd(X, full_matrices=False)\n",
      "print 'Singular values ',D\n",
      "r = np.arange(0.0,5.0,0.5)\n",
      "for lamda in r:\n",
      "    print 'lamda ',lamda,' df ',dfRidgeRegressor(lamda,D=D)\n",
      "lamda = 24.0\n",
      "print 'lamda ',lamda,' df ',dfRidgeRegressor(lamda,D=D)\n",
      "\n",
      "print '\\nCompare my and scikitlearn  ridge regression\\n',50*'-'\n",
      "lamda = 0.5\n",
      "my_est = MyRidgeRegressor()\n",
      "my_est.fit(X,Y,lamda)\n",
      "print 'My ridge regression lamda=',lamda,'\\n  coef=',my_est.coef_\n",
      "print ' Intercept ',my_est.intercept_\n",
      "print ' Error     ',RSS_score(my_est,Xtest,Ytest)\n",
      "print ' Std Error ',RSS_std_err(my_est,Xtest,Ytest)\n",
      " \n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,Y)\n",
      "print 'Scikit ridge regression lamda=',lamda,'\\n  coef=',ridge_est.coef_\n",
      "print ' Intercept ',ridge_est.intercept_\n",
      "print ' Error     ',RSS_score(ridge_est,Xtest,Ytest)\n",
      "print ' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "\n",
      "    \n",
      "print '\\nRidge regression producing coefs ~ ESL pp 63\\n',50*'-'\n",
      "lamda = 24.0\n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,Y)\n",
      "print 'lamda ',lamda,'  Error ',RSS_score(ridge_est,Xtest,Ytest),' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_est.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n",
        "\n",
        "Degrees of freedom for different values of lambda\n",
        "--------------------------------------------------\n",
        "Singular values  [ 15.37564687  10.8584466    8.47008295   6.4917243    5.82635313\n",
        "   5.16077118   4.32731217   3.48856279]\n",
        "lamda  0.0  df  8.0\n",
        "lamda  0.5  df  7.87660748341\n",
        "lamda  1.0  df  7.75902336159\n",
        "lamda  1.5  df  7.64675613811\n",
        "lamda  2.0  df  7.53937410431\n",
        "lamda  2.5  df  7.43649601366\n",
        "lamda  3.0  df  7.33778350227\n",
        "lamda  3.5  df  7.24293487888\n",
        "lamda  4.0  df  7.15167999866\n",
        "lamda  4.5  df  7.0637760023\n",
        "lamda  24.0  df  5.0117601134\n",
        "\n",
        "Compare my and scikitlearn  ridge regression\n",
        "--------------------------------------------------\n",
        "My ridge regression lamda= 0.5 \n",
        "  coef= [ 0.66732936  0.26268005 -0.13581476  0.20837773  0.30251696 -0.27381514\n",
        " -0.01942982  0.26089392]\n",
        " Intercept  2.45234508507\n",
        " Error      0.519649308021\n",
        " Std Error  0.179720807509\n",
        "Scikit ridge regression lamda= 0.5 \n",
        "  coef= [ 0.66776889  0.2630757  -0.13801607  0.20889782  0.30201872 -0.27227083\n",
        " -0.01641616  0.25758736]\n",
        " Intercept  2.46536762536\n",
        " Error      0.5166754343\n",
        " Std Error  0.176343649302\n",
        "\n",
        "Ridge regression producing coefs ~ ESL pp 63\n",
        "--------------------------------------------------\n",
        "lamda  24.0   Error  0.490361320535  Std Error  0.162269156393\n",
        "lcavol     : 0.421\n",
        "lweight    : 0.239\n",
        "age        : -0.048\n",
        "lbph       : 0.162\n",
        "svi        : 0.227\n",
        "lcp        : -0.000\n",
        "gleason    : 0.041\n",
        "pgg45      : 0.132\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      " \n",
      "def constructSet(x1,x2):\n",
      "    #print x1\n",
      "    X = np.zeros([len(x1),8])\n",
      "    X[:,0] = 1\n",
      "    X[:,1] = x1\n",
      "    X[:,2] = x2\n",
      "    X[:,3] = x1*x1\n",
      "    X[:,4] = x2*x2\n",
      "    X[:,5] = x1*x2\n",
      "    X[:,6] = np.abs(x1-x2)\n",
      "    X[:,7] = np.abs(x1+x2)\n",
      "    return X\n",
      "   \n",
      " \n",
      "def misClassificationError(model,Xin,yin):\n",
      "    nerrs = 0\n",
      "    for i in xrange(len(yin)):\n",
      "        yp = model.predict(Xin[i])    \n",
      "        if np.sign(yp) != np.sign(yin[i]) : \n",
      "            #print yp,yin[i]\n",
      "            nerrs += 1\n",
      "    err = float(nerrs)/float(len(yin))\n",
      "    # print '--- nerr n err ',nerrs,len(yin),err\n",
      "    return err\n",
      "       \n",
      "\n",
      "train_file = os.path.join(data_dir,'train_data.txt')\n",
      "train_df   = pd.read_fwf(train_file,colspecs='infer',header=None,names=['x1','x2','y'])\n",
      " \n",
      "test_file = os.path.join(data_dir,'test_data.txt')\n",
      "test_df   = pd.read_fwf(test_file,colspecs='infer',header=None,names=['x1','x2','y'])\n",
      "train_df.head()\n",
      "print len(train_df)\n",
      "print len(test_df)\n",
      " \n",
      "x1 = train_df['x1']\n",
      "x2 = train_df['x2']\n",
      "y  = train_df['y']\n",
      "X  = constructSet(x1,x2)\n",
      " \n",
      "x1_test = test_df['x1']\n",
      "x2_test = test_df['x2']\n",
      "y_test  = test_df['y']\n",
      "X_test  = constructSet(x1_test,x2_test)\n",
      " \n",
      " \n",
      "clf = linear_model.LinearRegression(fit_intercept=False)\n",
      "clf.fit(X,y)\n",
      "print clf.coef_\n",
      " \n",
      "# Misclassification error\n",
      "print(\"Missclassification train error: %.5f\" % misClassificationError(clf,X,y))\n",
      "print(\"Missclassification test  error: %.5f\" % misClassificationError(clf,X_test,y_test))\n",
      " \n",
      "all = [-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10]\n",
      "all = [3,2,1,0,-1,-2,-3]\n",
      "#all = [3]\n",
      "for k in all:\n",
      "    lam         = 10**k\n",
      "    w           = lam/len(y)\n",
      "    w = lam\n",
      "    #print '\\n  k  lambda w ',k,lam,w,'\\n',50*'-'\n",
      "    clf_ridge   = linear_model.Ridge (alpha = w,fit_intercept=False)\n",
      "    clf_ridge.fit(X,y)\n",
      "    #print 'coef scikitl ',clf_ridge.coef_\n",
      "   \n",
      "    Xt = X.transpose()\n",
      "    I  = np.eye(8)\n",
      "    M  = np.linalg.inv(Xt.dot(X) + w*I)\n",
      "    coef =  M.dot(Xt.dot(y))\n",
      "    #print 'coef mine   ',clf_ridge.coef_\n",
      "    Err = np.sqrt( np.sum((coef-clf_ridge.coef_)**2 ))\n",
      "\n",
      "    # Misclassification error\n",
      "    #print '\\nk = ',k\n",
      "    #print(\"  Missclassification train error: %.5f\" % misClassificationError(clf_ridge,X,y))\n",
      "    #print(\"  Missclassification test  error: %.5f\" % misClassificationError(clf_ridge,X_test,y_test))\n",
      "    in_sample     = misClassificationError(clf_ridge,X,y)\n",
      "    out_of_sample = misClassificationError(clf_ridge,X_test,y_test)\n",
      "    print 'k in out (err)',k,in_sample,out_of_sample,Err\n",
      "\n",
      "\n",
      "\n",
      "#all = [-3]\n",
      "all = []\n",
      "for k in all:\n",
      "    lam         = 10**k\n",
      "    w           = lam/len(y)\n",
      "    # print '\\n  k  lambda w ',k,lam,w,'\\n',50*'-'\n",
      "    clf_ridge   = linear_model.Ridge (alpha = w,fit_intercept=False)\n",
      "    clf_ridge.fit(X,y)\n",
      "    #print 'coef scikitl ',clf_ridge.coef_\n",
      "   \n",
      "    Xt = X.transpose()\n",
      "    I  = np.eye(8)\n",
      "    M  = np.linalg.inv(Xt.dot(X) + w*I)\n",
      "    coef =  M.dot(Xt.dot(y))\n",
      "    #print 'coef mine   ',clf_ridge.coef_\n",
      "    print 'Err ',np.sqrt( np.sum((coef-clf_ridge.coef_)**2 ))\n",
      "\n",
      "    # Misclassification error\n",
      "    in_sample     = misClassificationError(clf_ridge,X,y)\n",
      "    out_of_sample = misClassificationError(clf_ridge,X_test,y_test)\n",
      "    print '\\nk = ',k\n",
      "    print 'k in out ',k,in_sample,out_of_sample\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35\n",
        "250\n",
        "[-1.64706706 -0.14505927  0.10154121 -2.03296844 -1.82804373  2.48152945\n",
        "  4.15893861  0.31651714]\n",
        "Missclassification train error: 0.02857\n",
        "Missclassification test  error: 0.08400\n",
        "k in out (err) 3 0.371428571429 0.436 1.85268548448e-18\n",
        "k in out (err) 2 0.2 0.228 5.33268781527e-17\n",
        "k in out (err) 1 0.0571428571429 0.124 1.86875698179e-16\n",
        "k in out (err)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 0.0 0.092 2.61573292223e-15\n",
        "k in out (err) -1 0.0285714285714 0.056 1.65525245195e-14\n",
        "k in out (err) -2 0.0285714285714 0.084 4.44242532938e-14\n",
        "k in out (err) -3 0.0285714285714 0.08 7.76259739527e-15\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Estimating $\\lambda$ by cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.grid_search  import GridSearchCV\n",
      "import math\n",
      "\n",
      "\n",
      "print '\\nDetermine lambda by cross-validation\\n',50*'-'\n",
      "lamdas_to_try = np.arange(0.0,30.0,0.5)\n",
      "model = Ridge()\n",
      "grid  = GridSearchCV(estimator=model,param_grid=dict(alpha=lamdas_to_try))\n",
      "grid.fit(X,Y)\n",
      "print grid.best_score_\n",
      "print grid.best_estimator_.alpha\n",
      "\n",
      "print '\\nRidge regression producing coefs ~ ESL pp 63\\n',50*'-'\n",
      "lamda = grid.best_estimator_.alpha\n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,Y)\n",
      "\n",
      "print 'lamda ',lamda,'  Error ',RSS_score(ridge_est,Xtest,Ytest),' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_est.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n",
      "\n",
      "    \n",
      "print '\\n\\nNEXT\\n'\n",
      "    \n",
      "# DOES NOT WORK. NEED TO CHECK WHY\n",
      "# Residual sum of squares\n",
      "def RSS_score1(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    score = np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "    if math.isnan(score):\n",
      "        print 'NaN detected '\n",
      "        score = 999999999999999.0\n",
      "    return score\n",
      "\n",
      "ridge_cv = RidgeCV(lamdas_to_try,scoring=RSS_score1)\n",
      "ridge_cv.fit(X,Y)\n",
      "print '\\nRidge CV found lambda ',ridge_cv.alpha_\n",
      "print '  Error ',RSS_score1(ridge_cv,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_cv.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Determine lambda by cross-validation\n",
        "--------------------------------------------------\n",
        "-6.00713177867"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29.5\n",
        "\n",
        "Ridge regression producing coefs ~ ESL pp 63\n",
        "--------------------------------------------------\n",
        "lamda  29.5   Error  0.494408576984  Std Error  0.165037098847\n",
        "lcavol     : 0.395\n",
        "lweight    : 0.232\n",
        "age        : -0.037\n",
        "lbph       : 0.154\n",
        "svi        : 0.218\n",
        "lcp        : 0.019\n",
        "gleason    : 0.044\n",
        "pgg45      : 0.126\n",
        "\n",
        "\n",
        "NEXT\n",
        "\n",
        "NaN detected \n",
        "\n",
        "Ridge CV found lambda  0.0\n",
        "  Error  NaN detected \n",
        "1e+15\n",
        "lcavol     : nan\n",
        "lweight    : nan\n",
        "age        : nan\n",
        "lbph       : nan\n",
        "svi        : nan\n",
        "lcp        : nan\n",
        "gleason    : nan\n",
        "pgg45      : nan\n"
       ]
      }
     ],
     "prompt_number": 263
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"LASSO\">Ridge</span>  and LAR\n",
      "\n",
      "\n",
      "The **lassso** replaces the $L_2$ penalty of riidge regression by $L_1$ penalty, that is we search for a coefficient vector such that \n",
      "$\\min_w ||\\bf{Xw}-y||_2^2 + \\lambda ||w||_1$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$ where $\\lambda$ is a free parameter that has to be determined. The larger $\\lambda$ is the more it **shrinks** the value of the coefficients and maker the solution more reguralized. It solved the problem of stability inherent in linear regression at the price of extra parameter that has to be detetmined. The particular property of the lasso solution is that some parameters are shrinked to zero.\n",
      "\n",
      "As in ridge regression, we should make sure to normalize the input matrix and to estimate the intercept term as the mean of the right hand side vector $y$.\n",
      "\n",
      "The [Least Angle Regression](http://arxiv.org/pdf/math/0406456.pdf) paper contains the algorithmic details on the efficient comptational procedures implementing LARS, lasso and other similar model selection methods. The diabetics data set from the paper cab be loaded from [this website](http://www4.stat.ncsu.edu/~boos/var.select/). A [C/C++ implementation](http://www.mlpack.org/doxygen.php?doc=classmlpack_1_1regression_1_1LARS.html) of LARS/lasso is available in the [mlpack package](http://www.mlpack.org/index.html)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "print '\\nDetermine alpha for lasso by cross-validation\\n',50*'-'\n",
      "lamdas_to_try = np.arange(0.1,0.3,0.005)\n",
      "model = linear_model.Lasso()\n",
      "grid  = GridSearchCV(estimator=model,param_grid=dict(alpha=lamdas_to_try))\n",
      "grid.fit(X,Y)\n",
      "print 'Final score CV ',grid.best_score_\n",
      "print 'Best estimation of alpha=',grid.best_estimator_.alpha\n",
      "\n",
      "# Compute Lasso regression with optimal alpha\n",
      "est_lasso = linear_model.Lasso(alpha=grid.best_estimator_.alpha,fit_intercept=True)\n",
      "est_lasso.fit(X,Y)\n",
      "\n",
      "\n",
      "print '\\nalpha ',grid.best_estimator_.alpha,'  Error ',RSS_score(est_lasso,Xtest,Ytest),' Std Error ',RSS_std_err(est_lasso,Xtest,Ytest)\n",
      "print 'Intercept ',est_lasso.intercept_\n",
      "for (name,c) in zip(X.columns,est_lasso.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Determine alpha for lasso by cross-validation\n",
        "--------------------------------------------------\n",
        "Final score CV "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -5.68611678798\n",
        "Best estimation of alpha= 0.125\n",
        "\n",
        "alpha  0.125   Error  0.453088546786  Std Error  0.153621633898\n",
        "Intercept  2.46603339212\n",
        "lcavol     : 0.546\n",
        "lweight    : 0.212\n",
        "age        : -0.000\n",
        "lbph       : 0.074\n",
        "svi        : 0.145\n",
        "lcp        : 0.000\n",
        "gleason    : 0.000\n",
        "pgg45      : 0.053\n"
       ]
      }
     ],
     "prompt_number": 321
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"FeatureSelectionOLS\">Forward</span> feature selection\n",
      "\n",
      "Forward selection of predictors by 10-fold cross validation and taking RSS error on test set/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn              import cross_validation\n",
      "\n",
      "X = Xtrain.copy()\n",
      "Y = Ytrain.copy()\n",
      "print X.shape\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_score(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    return np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_std_err(estimator,Xtest,Ytest):\n",
      "    ypred     = estimator.predict(Xtest)\n",
      "    ysqr      = (Ytest-ypred)**2\n",
      "    y_std_err = np.std(ysqr)/np.sqrt(len(ysqr)-1)\n",
      "    return y_std_err\n",
      "\n",
      "\n",
      "#--------------------------------------------\n",
      "# Define linear regressor\n",
      "#--------------------------------------------\n",
      "est = LinearRegression(fit_intercept=True)\n",
      "est.fit(X,Y)\n",
      "\n",
      "\n",
      "scores = cross_validation.cross_val_score(est, X, Y, scoring='mean_squared_error',cv=3)\n",
      "print 'Scores using scikitlearn ',scores, ' mean score ',scores.mean()\n",
      " \n",
      "scores = cross_validation.cross_val_score(est, X, Y, scoring=RSS_score,cv=3)\n",
      "print 'Scores using my score function ',scores, ' mean score ',scores.mean()\n",
      "\n",
      "names_of_predictors = list(X.columns)\n",
      "print 'Forward selection algorithm for predictors\\n ',names_of_predictors\n",
      "\n",
      "\n",
      "cnames = []\n",
      "while names_of_predictors:\n",
      "    scores            = []\n",
      "    # Add each of remaining predictors in turn and compute its cross-validation score\n",
      "    for p in names_of_predictors:\n",
      "        Xin    = Xfinal.copy()\n",
      "        Xin[p] = X[p]            # Add predictor p\n",
      "        est.fit(Xin,Y)\n",
      "        score = cross_validation.cross_val_score(est, Xin, Y, scoring=RSS_score,cv=10)\n",
      "        scores.append(score.mean())\n",
      "\n",
      "    # Add predictor with maximal score to 'active predictors'\n",
      "    min_index                 = np.argmin(scores)\n",
      "    selectedPredictor         = names_of_predictors[min_index]\n",
      "    cnames.append(selectedPredictor)\n",
      "  \n",
      "    # Compute estimator based on current active predictors\n",
      "    Xfinal = X[cnames]    \n",
      "    est.fit(Xfinal,Y)\n",
      "    \n",
      "    XtestFinal = Xtest[cnames]\n",
      "    \n",
      "    print ('Selected predictor {:10s} cv score {:.3f} Test error {:.3f} Std err {:.3f} R**2 {:.3f} '\\\n",
      "           .format(selectedPredictor,scores[min_index],\\\n",
      "                   RSS_score(est,XtestFinal,Ytest),\\\n",
      "                   RSS_std_err(est,XtestFinal,Ytest),\\\n",
      "                   est.score(Xfinal,Y)))\n",
      " \n",
      "    del names_of_predictors[min_index]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n",
        "Scores using scikitlearn  [-2.39132287 -0.9888438  -3.6037895 ]  mean score  -2.32798538942\n",
        "Scores using my score function  [ 2.39132287  0.9888438   3.6037895 ]  mean score  2.32798538942\n",
        "Forward selection algorithm for predictors\n",
        "  ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'Xfinal' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-b1d34b4a996b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Add each of remaining predictors in turn and compute its cross-validation score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames_of_predictors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mXin\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mXfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mXin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m            \u001b[1;31m# Add predictor p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'Xfinal' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"PCA\">PCA</span>  \n",
      "\n",
      "\n",
      "We normalize ol predictors.\n",
      "\n",
      "We consider the SVD decomposition of $X$, $X=UDV^T$ where $U$ is an orthnormal $n \\times p$ matrix that span the column space of $X$, $V^T$ is a $p \\times p$ matrix that span the row space of $X$ and $D = \\tt{diag} (d_1,\\ldots,d_p)$ is a diagonal $p \\times p$ matrix.\n",
      "\n",
      "The normal matrix $X^TX = VDU^TUDV^T=VD^2V^T$ and so $\\beta = (X^TX)^{-1}X^Ty = VD^{-1}U^Ty$ where $D^{-1} = \\tt{diag} (\\frac{1}{d_1},\\ldots,\\frac{1}{d_p})$\n",
      "\n",
      "We consider now the expantion of the solution $\\beta = \\sum_{i=1}^{p} \\alpha_i V_i$ then we have $\\hat{y} = X \\beta =  \\sum_{i=1}^{p} \\alpha_i d_i U_i$ and so $\\alpha_i = \\frac{(\\hat{y},U_i)}{d_i(U_i,U_i)}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyPCARegressor(object):\n",
      "    def __init__(self,X,Y):\n",
      "        self.intercept_  = Y.mean()\n",
      "        U, D, VT         = np.linalg.svd(X, full_matrices=False)\n",
      "        \n",
      "        self.p_     = X.shape[1]\n",
      "        self.VT_    = VT\n",
      "        self.sum_s_ = np.sum(D)\n",
      "        self.D_     = D\n",
      " \n",
      "        # solve with SVD\n",
      "        Dinv           = [1/d for d in D]\n",
      "        UT             = U.transpose()\n",
      "        UTy            = UT.dot(Y)\n",
      "        DinvUTy        = np.diag(Dinv).dot(UTy)\n",
      "        self.ls_sol_   = VT.transpose().dot(DinvUTy)\n",
      "\n",
      "        # Compute coefficients of the expantion of th eLS solution in terms of V[i]\n",
      "        ypred        = X.dot(self.ls_sol_) \n",
      "        self.alphas_ = [ (ypred.dot(UT[i]))/(D[i]) for i in xrange(len(self.ls_sol_))   ]     \n",
      "        \n",
      "    def fit(self, M):\n",
      "        \"\"\"Fits estimator to data. \"\"\"\n",
      "        if M > self.p_:\n",
      "            M = self.p_ \n",
      "        self.coef_  = np.zeros(self.p_)\n",
      "        self.sum_s_in_fit_ = 0.0\n",
      "        for i in xrange(M):\n",
      "            self.sum_s_in_fit_ += self.D_[i]\n",
      "            self.coef_  += self.alphas_[i]*self.VT_[i]\n",
      "\n",
      "        self.variance_explained_ =  self.sum_s_in_fit_/ self.sum_s_\n",
      "        # set state of ``self``\n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict response of ``X``. \"\"\"\n",
      "        # compute predictions ``pred``\n",
      "        pred = X.dot(self.coef_) \n",
      "        return pred\n",
      "\n",
      "   \n",
      "Xone   = np.column_stack( (np.ones(len(X)), X) )\n",
      "XtestO = np.column_stack( (np.ones(len(Xtest)), Xtest) )\n",
      "\n",
      "PCAest = MyPCARegressor(Xone,Y)\n",
      "for i in xrange(Xone.shape[1]):\n",
      "    PCAest.fit(i+1)\n",
      "    print ('{:2d} Test error {:.3f} Standard error {:.3f} % Variance explained  {:.3f} ').format(i,RSS_score(PCAest,XtestO,Ytest),\\\n",
      "                                                                           RSS_std_err(PCAest,XtestO,Ytest),\\\n",
      "                                                                           PCAest.variance_explained_)\n",
      "\n",
      "\n",
      "xnorm      = Xone.transpose().dot(Xone)\n",
      "xninv      = np.linalg.inv(xnorm)\n",
      "sol        = xninv.dot(Xone.transpose().dot(Y))\n",
      "print '\\nCoefs of LS solution by ninverting normal equations\\n',sol\n",
      "\n",
      "print 'Coefs of LS solution by Using SVD expantion\\n',PCAest.ls_sol_\n",
      "\n",
      "PCAest.fit(Xone.shape[1])\n",
      "print 'Coefs of LS solution by Using full PCA expantion\\n',PCAest.coef_\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 Test error 7.082 Standard error 0.770 % Variance explained  0.226 \n",
        " 1 Test error 7.330 Standard error 0.973 % Variance explained  0.385 \n",
        " 2 Test error 6.532 Standard error 1.692 % Variance explained  0.512 \n",
        " 3 Test error 0.969 Standard error 0.301 % Variance explained  0.632 \n",
        " 4 Test error 0.711 Standard error 0.187 % Variance explained  0.725 \n",
        " 5 Test error 0.714 Standard error 0.167 % Variance explained  0.811 \n",
        " 6 Test error 0.516 Standard error 0.149 % Variance explained  0.886 \n",
        " 7 Test error 0.464 Standard error 0.131 % Variance explained  0.950 \n",
        " 8 Test error 0.521 Standard error 0.179 % Variance explained  1.000 \n",
        "\n",
        "Coefs of LS solution by ninverting normal equations\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Coefs of LS solution by Using SVD expantion\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Coefs of LS solution by Using full PCA expantion\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n"
       ]
      }
     ],
     "prompt_number": 414
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X.shape\n",
      "print U.shape\n",
      "print D.shape\n",
      "print VT.shape\n",
      "\n",
      "for i in xrange(len(UT)):\n",
      "    diff = X.dot(V[i]) - UT[i]*D[i]\n",
      "    print i,np.sum(np.abs(diff))/float(len(diff))\n",
      "\n",
      "\n",
      "beta = np.zeros(8)\n",
      "for i in xrange(8):\n",
      "    beta += V[i]\n",
      "    \n",
      "ypred = X.dot(beta)\n",
      "\n",
      "ypred1 = np.zeros(len(ypred))\n",
      "for i in xrange(8):\n",
      "    ypred1 += D[i]*UT[i]\n",
      " \n",
      "alphas = [ (ypred.dot(UT[i]))/(D[i]) for i in xrange(len(sol2))   ]\n",
      "print alphas\n",
      "print ypred1-ypred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n",
        "(67L, 8L)\n",
        "(8L,)\n",
        "(8L, 8L)\n",
        "0 7.04453079618e-16\n",
        "1 2.30656082957e-15\n",
        "2 7.44481176541e-16\n",
        "3 7.07456481457e-16\n",
        "4 5.99204558277e-16\n",
        "5 3.66078436221e-16\n",
        "6 3.81389618695e-16\n",
        "7 6.46611702822e-16\n",
        "[0.99999999999999989, 1.0000000000000011, 0.99999999999999878, 1.0000000000000002, 1.0000000000000007, 1.0000000000000009, 0.99999999999999956, 0.99999999999999978]\n",
        "1    -2.609024e-15\n",
        "2    -3.552714e-15\n",
        "3    -5.773160e-15\n",
        "4     8.881784e-16\n",
        "5    -2.220446e-15\n",
        "6    -2.553513e-15\n",
        "8    -3.330669e-16\n",
        "11    4.440892e-16\n",
        "12    0.000000e+00\n",
        "13   -4.662937e-15\n",
        "14   -4.440892e-15\n",
        "16   -2.664535e-15\n",
        "17   -4.440892e-16\n",
        "18   -8.881784e-16\n",
        "19   -1.332268e-15\n",
        "...\n",
        "79    1.776357e-15\n",
        "81   -4.857226e-15\n",
        "82   -3.996803e-15\n",
        "83    4.662937e-15\n",
        "85   -8.881784e-16\n",
        "86    1.776357e-15\n",
        "87    2.775558e-16\n",
        "88    0.000000e+00\n",
        "89    8.881784e-15\n",
        "90    2.664535e-15\n",
        "91    1.776357e-15\n",
        "92    1.776357e-15\n",
        "93    2.220446e-15\n",
        "94    1.776357e-15\n",
        "96    8.881784e-16\n",
        "Length: 67, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 359
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"RBF\">Radial</span>  Basis Functions (RBF)\n",
      "\n",
      "Linear regression is not restricted to just hyperplanes. The same setting also can be applied to any **basis functions** representation of the form\n",
      "> $\\sum_{k=1}^K w_k \\Phi_k(x) + b$ \n",
      "\n",
      "In that case the data matrix $A$ is defined by $A_{ij}=\\Phi_{j-1}(\\mathbf{x_i}), \\quad i = 1,\\ldots,N , j = 2,\\ldots,K+1$ and the first colums of $A$ is the column vector $\\mathbf{1}$. Popular selection for basis functions are **[radial basis functions](http://en.wikipedia.org/wiki/Radial_basis_function)** in which \n",
      "> $\\Phi_{j}(\\mathbf{x}) = \\Phi( ||\\mathbf{c_j - x}||)$\n",
      "\n",
      "where $c_j \\in \\mathbb{R}^d$ are called **centers** of the RBF. A popular choice for RBF is the **Gaussian**\n",
      "> $\\Phi( r ) = \\exp (-\\gamma r^2)$ where $\\gamma$ is a positive constant.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}