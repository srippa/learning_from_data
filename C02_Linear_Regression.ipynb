{
 "metadata": {
  "name": "",
  "signature": "sha256:63b1cdeb6f33904345a3e18a561a49823c9d534ca7ee893ff6603b4fa7abd163"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Linear regression\n",
      "\n",
      "In [Linear regression](http://en.wikipedia.org/wiki/Linear_regression), the learning model is represented as a **linear combination** of the predictors. This formulation allows developing fast and reliable learning algorithms and, also, enables deep theoretical investigation of the statistical properties of the learning model. Linear regression is central in texts books that focus on learning from a statistical point of view, e.g. [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/). \n",
      "\n",
      "The purpose of this chapter is to describe implementations of linear regression learning model using Python. In particular I will introduce the basic formulation and learning algorithm, regularization techniques, methods for feature selection and extensions using non-linear transformations. I will demonstrate solving linear regression problems using two Python packages:  The [linear regression module of scikitlearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and the , more statistically oriented, implementation [in the statsmodels module](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html). I will also present a short python code that I wrote to solve some linear regression problems using [numpy](http://www.numpy.org/) . This code is meant to be used only to reinforce the understanding of the method. It is far from the quality needed from a production code.\n",
      "\n",
      "I recommend also reading the very accessible  \"[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\" book that provides lots of examples in [R](http://www.r-project.org/). I will use some of the data sets of the two books discussed above in the examples below.\n",
      "\n",
      "\n",
      "# Contents\n",
      "* [The hypothesis set](#LinearRegressionMotivation)\n",
      "* [Square loss and the Ordinary Least Squares (OLS) problem](#LinearRegressionOLS) \n",
      "* <a href=\"#LinearRegressionOLSLibraries\" data-ajax=\"false\"><font color=\"green\">Solving OLS problems in Python</font></a>\n",
      "    * [<font color=\"green\">OLS in StatsModels</font>](#StatsModelsLinReg)\n",
      "    * [<font color=\"green\">OLS in ScikitLearn</font>](#ScikitLearnLinReg)\n",
      "    * [<font color=\"green\">OLS using numpy</font>](#MyLinReg)    \n",
      "    * [<font color=\"green\">Numerical examples</font>](#NumericalExamplesLinReg)\n",
      "      * [<font color=\"brown\">Advertise data set</forn>](#AdvertiseDataSet)\n",
      "      * [<font color=\"brown\">Feature scaling/standardization</forn>](#NormalizeDataSet)\n",
      "      * [<font color=\"brown\">Prostate data set</forn>](#ProstateDataSet)\n",
      "* [Regularizsation in linear regression](#RegularizationLinReg)\n",
      "  * <a href=\"#RidgeRegression\" data-ajax=\"false\">Ridge regression</a>\n",
      "    * $\\lambda$ and effective degrees of freedom\n",
      "    * Estimating $\\lambda$ by cross validation\n",
      "  * <a href=\"#LASSO\" data-ajax=\"false\">LASSO and LAR</a>\n",
      "  * Elastic-net\n",
      "* [Dimensionality reduction](#DimReductionLinReg)\n",
      "  * <a href=\"#FeatureSelectionOLS\" data-ajax=\"false\">Forward feature selection</a>\n",
      "  * <a href=\"#PCA\" data-ajax=\"false\">PCA</a>\n",
      "*  <a href=\"#RBF\" data-ajax=\"false\">Radial Basis Functions</a> - extension of linear regression that is able to model complex surfaces and yet remains a linear model\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"LinearRegressionMotivation\">The</span>  hypothesis set\n",
      "The hypothesios set consists of functions of the form\n",
      "$g(\\mathbf{x})=w_0 +w_1x_1, \\ldots, + w_Nx_d=\\mathbf{w^Tx}$. We note that the model is linear in the coefficients of the elements $\\{1,x_1,\\ldots,x_d\\}$ but those elements may be non linear functions of the predictors. \n",
      "\n",
      "For example suppose the sample space $\\mathcal{X} \\subset \\mathbb{R}^2$ and we consider predictors that are polynomials of second degree $g(\\mathbf{x})=w_0 +w_1x_1+w_2x_2 + w_3 x_1x_2 + w_4 x_1^2 + w_5x_2^2$. A popular class of non-linear transformation, that constantly achieves good regression results, is that of  <a href=\"#RBF\" data-ajax=\"false\">Radial Basis Functions</a> (RBF). \n",
      "\n",
      "Linear regression is attractive because of it leads to easily computable solutions which are often very interpretable.\n",
      "\n",
      "# <span id=\"LinearRegressionOLS\">Square</span> loss function and the Ordinary Least Squares problem\n",
      "\n",
      "[OLS](http://en.wikipedia.org/wiki/Ordinary_least_squares) Determine the coefficients of the linear model under the square loss function, i.e. by minimizing $\\min_w ||\\bf{Xw}-y||_2^2$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$. The main problem with linear regression is that it relies heavily on the assumption of independence of the models terms $X_i$. If that assumption does not hold then the matrix $\\bf{X}$ have close to linear dependent columns and the solution becomes unstable. \n",
      "\n",
      "The simple structutre of OLS allows rigouros analysis and determination of many statistical properties such as p-values and standard errors for each coefficeint. This allows performing hypothesis testing and statistical interpretation of the coefficients and errors, see [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) or [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/data.html). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"green\">Pyton preamble</font>\n",
      "Run the Cell below to set properly pathes and drawing options."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------------------------\n",
      "# Add my modules and data directories to the path\n",
      "#------------------------------------------------------\n",
      "import os\n",
      "import sys\n",
      "\n",
      "myHome = %pwd                              # THe home of this notebook\n",
      "module_dir = os.path.join(myHome,'src')     # my python modues\n",
      "data_dir   = os.path.join(myHome,'data')    # my python modues\n",
      "sys.path.insert(0,module_dir)               # Add my python modeules to module pat\n",
      "\n",
      "# Enabe inline drawings, if you like\n",
      "%pylab inline                       \n",
      "import matplotlib.pyplot as plt     # matplotlib - plots"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  \n",
      "import numpy.linalg as la\n",
      "\n",
      "import scipy as sp\n",
      "import scipy.stats as st\n",
      "\n",
      "import sr_linreg            # My code - for educational purposes ONLY\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import matplotlib.pyplot as plt     ## matplotlib - plots\n",
      "\n",
      "from pandas import DataFrame\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <font color=\"green\"><span id=\"LinearRegressionOLSLibraries\">Solving</span> OLS in Python</font>\n",
      "In the program sniplets below I show three procedures that solve the OLS problem given an inpit data matrix <code>Xin</code> and a vector <code>y</code> of the values of the target function. Each procedure will solve the OLS problem and compute the quantity known as [R-squared](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit). \n",
      "\n",
      "We note that we should be careful to how the data matrix $\\mathbf{X}$ is constructed. In some cases the matrix contains a column of ones corresponding to the constant term $w_0$. In other cases, we do not include this term in the data matrix. In such cases we should add an appropriate command to tell the package that we use to add the constant term. If we use **statsmodels** then we should invoke the command <code>add_constant(X)</code>. In **scikitlearn** we will create a <code>LinearRegression</code> object with the setting <code>fit_intercept=True</code>. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##  <a id=\"StatsModelsLinReg\"></a><font color=\"green\">OLS in statsmodels</font>\n",
      "\n",
      "A lentghty and interesting tutorial on using [statsmodels OLS function](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html)  for linear regression is discussed here. Also the statsmodels documentation have nice examples. The main difference between the OLS implementation of statsmodels rather and that of  scikit-learn is the rich set of statistical tests that are performed performed. See a [nice example](http://mpastell.com/2013/04/19/python_regression/) of using OLS from statsmodels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------\n",
      "# Calling to statsmodels OLS \n",
      "#------------------------------------\n",
      "def statsModelsOLS(Xin,Y):\n",
      "    import numpy as np\n",
      "    import statsmodels.api as sm\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # OLS using numpy\n",
      "    #--------------------------------------------\n",
      "    X  = Xin.copy()\n",
      "    X  = sm.add_constant(X)        # Add also constant (bias) term\n",
      "\n",
      "    # Fit regression model\n",
      "    linest_1d = sm.OLS(Y, X).fit()\n",
      "\n",
      "    # Inspect the results\n",
      "    print linest_1d.summary()\n",
      "    print linest_1d.params\n",
      "\n",
      "    rsqr          = sr_linreg.Rsquared(linest_1d,X,Y)\n",
      "    n             = X.shape[0]\n",
      "    d_plus_one    = X.shape[1]\n",
      "    print 'RSE(Std err)  ',sr_linreg.RSE(Y,linest_1d.predict(X),d_plus_one)\n",
      "    print 'R**2          ',rsqr\n",
      "    print 'Adjusted R**2 ',sr_linreg.AdjustedRsquared(rsqr,n,d_plus_one)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##  <a id=\"ScikitLearnLinReg\"></a><font color=\"green\">OLS in Scikitlearn</font>\n",
      "\n",
      "Scikitlearn is more a machine learning library than a statistical package and thus its [linear regression procedure](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) focuses more on prediction and less on statistical inference as done in statsmodels. From the results, it seems that both packages use the same underlying numerical procedures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#------------------------------------\n",
      "# Calling to scikitlearn OLS \n",
      "#------------------------------------    \n",
      "def scikitledarnOLS(X,Y,normalize =False):\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # Define linear regressor\n",
      "    #--------------------------------------------\n",
      "    est = LinearRegression(fit_intercept=True,normalize=normalize)\n",
      "\n",
      "    #--------------------------------------------\n",
      "    # Fit to data to get parameters\n",
      "    #--------------------------------------------\n",
      "    est.fit(X,Y)\n",
      "\n",
      "    print 'Intercept ',est.intercept_\n",
      "    print 'Coeff     ',est.coef_      ,type(est.coef_) # access coefficients\n",
      "\n",
      "    print type(X)\n",
      "\n",
      "    rsqr  = sr_linreg.Rsquared(est,X,Y)\n",
      "    n     = X.shape[0]\n",
      "    d     = X.shape[1]\n",
      "\n",
      "    print 'RSE(Std err)   ',sr_linreg.RSE(Y,est.predict(X),d+1)\n",
      "    print 'R**2           ',rsqr\n",
      "    print 'Variance score ',est.score(X,Y)\n",
      "    print 'Adjusted R**2  ',sr_linreg.AdjustedRsquared(rsqr,n,d+1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "##  <a id=\"MyLinReg\"></a><font color=\"green\">OLS using numpy</font>\n",
      "\n",
      "\n",
      "Below is the code to computer OLS determine interesting statistical properties. The code is provided only for pedagogical purposes and is **not** of production quality.  The reader is referred to [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) and [ISSL](http://www-bcf.usc.edu/~gareth/ISL/data.html) for details on the derivation and the meaning of the various quantities that are printed below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sr_linreg \n",
      "\n",
      "#------------------------------------\n",
      "# Calling to the OLS code\n",
      "#------------------------------------\n",
      "def manualOLS(X,Y):\n",
      "    my_lin_est = sr_linreg.MyLinearRegressor()\n",
      "    my_lin_est.fit(X,Y)\n",
      "\n",
      "    print my_lin_est.coef_\n",
      "    \n",
      "    print 'Y-mean        ',np.mean(Y)\n",
      "    print 'RSE(Std err)  ',my_lin_est.rse_ \n",
      "    print 'R**2          ',my_lin_est.rsqr_\n",
      "    print 'Adjusted R**2 ',sr_linreg.AdjustedRsquared(my_lin_est.rsqr_,my_lin_est.n_,my_lin_est.p_+1)\n",
      "    print 'F-statistics  ',my_lin_est.F_\n",
      "    print 'F-prob        ',my_lin_est.fprob_\n",
      "    print '\\n'\n",
      "    print 'stderr ',my_lin_est.stderr_\n",
      "    print 'zscore ',my_lin_est.zscore_\n",
      "    print 'pval   ',my_lin_est.pval_\n",
      "    print 'confint',my_lin_est.confinterval()\n",
      " \n",
      "#------------------------------------------------\n",
      "# Print the source code of my OLS implementation\n",
      "#------------------------------------------------\n",
      "import ipython_utils as iputils\n",
      "iputils.print_python_file(module_dir+\"/sr_linreg.py\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style type=\"text/css\">.highlight .hll { background-color: #ffffcc }\n",
        ".highlight  { background: #f8f8f8; }\n",
        ".highlight .c { color: #408080; font-style: italic } /* Comment */\n",
        ".highlight .err { border: 1px solid #FF0000 } /* Error */\n",
        ".highlight .k { color: #008000; font-weight: bold } /* Keyword */\n",
        ".highlight .o { color: #666666 } /* Operator */\n",
        ".highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
        ".highlight .cp { color: #BC7A00 } /* Comment.Preproc */\n",
        ".highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
        ".highlight .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
        ".highlight .gd { color: #A00000 } /* Generic.Deleted */\n",
        ".highlight .ge { font-style: italic } /* Generic.Emph */\n",
        ".highlight .gr { color: #FF0000 } /* Generic.Error */\n",
        ".highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
        ".highlight .gi { color: #00A000 } /* Generic.Inserted */\n",
        ".highlight .go { color: #888888 } /* Generic.Output */\n",
        ".highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
        ".highlight .gs { font-weight: bold } /* Generic.Strong */\n",
        ".highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
        ".highlight .gt { color: #0044DD } /* Generic.Traceback */\n",
        ".highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
        ".highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
        ".highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
        ".highlight .kp { color: #008000 } /* Keyword.Pseudo */\n",
        ".highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
        ".highlight .kt { color: #B00040 } /* Keyword.Type */\n",
        ".highlight .m { color: #666666 } /* Literal.Number */\n",
        ".highlight .s { color: #BA2121 } /* Literal.String */\n",
        ".highlight .na { color: #7D9029 } /* Name.Attribute */\n",
        ".highlight .nb { color: #008000 } /* Name.Builtin */\n",
        ".highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
        ".highlight .no { color: #880000 } /* Name.Constant */\n",
        ".highlight .nd { color: #AA22FF } /* Name.Decorator */\n",
        ".highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
        ".highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
        ".highlight .nf { color: #0000FF } /* Name.Function */\n",
        ".highlight .nl { color: #A0A000 } /* Name.Label */\n",
        ".highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
        ".highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
        ".highlight .nv { color: #19177C } /* Name.Variable */\n",
        ".highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
        ".highlight .w { color: #bbbbbb } /* Text.Whitespace */\n",
        ".highlight .mf { color: #666666 } /* Literal.Number.Float */\n",
        ".highlight .mh { color: #666666 } /* Literal.Number.Hex */\n",
        ".highlight .mi { color: #666666 } /* Literal.Number.Integer */\n",
        ".highlight .mo { color: #666666 } /* Literal.Number.Oct */\n",
        ".highlight .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
        ".highlight .sc { color: #BA2121 } /* Literal.String.Char */\n",
        ".highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
        ".highlight .s2 { color: #BA2121 } /* Literal.String.Double */\n",
        ".highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
        ".highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
        ".highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
        ".highlight .sx { color: #008000 } /* Literal.String.Other */\n",
        ".highlight .sr { color: #BB6688 } /* Literal.String.Regex */\n",
        ".highlight .s1 { color: #BA2121 } /* Literal.String.Single */\n",
        ".highlight .ss { color: #19177C } /* Literal.String.Symbol */\n",
        ".highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
        ".highlight .vc { color: #19177C } /* Name.Variable.Class */\n",
        ".highlight .vg { color: #19177C } /* Name.Variable.Global */\n",
        ".highlight .vi { color: #19177C } /* Name.Variable.Instance */\n",
        ".highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>  \n",
        "<span class=\"kn\">import</span> <span class=\"nn\">numpy.linalg</span> <span class=\"kn\">as</span> <span class=\"nn\">la</span>\n",
        "\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy</span> <span class=\"kn\">as</span> <span class=\"nn\">sp</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">as</span> <span class=\"nn\">st</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_critval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_pval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">tstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ts</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">tstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">ts</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"mf\">2.0</span><span class=\"o\">*</span><span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_critval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_pval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">fstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">fs</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">fstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">fs</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"c\"># Residual sum of squares</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Residual standard error - estimation for the Standard deviation of the error terms</span>\n",
        "<span class=\"c\"># Also known as standard error</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n",
        "\n",
        "  \n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Rsquared</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">Rsqr</span>       <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"n\">TSS</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">Rsqr</span>\n",
        "\n",
        "<span class=\"c\"># Compute F score. High F score indicates that mnultiple linear regression</span>\n",
        "<span class=\"c\"># have at least one significant coefficient</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Fscore</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">p</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">a</span>          <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">TSS</span><span class=\"o\">-</span><span class=\"n\">rss_c</span><span class=\"p\">)</span><span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">b</span>          <span class=\"o\">=</span> <span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> \n",
        "    <span class=\"n\">F</span>          <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">b</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">F</span>\n",
        "\n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">AdjustedRsquared</span><span class=\"p\">(</span><span class=\"n\">rsqr</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">adj</span>        <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">rsqr</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mf\">1.</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">adj</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyLinearRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">Xin</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">column_stack</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)),</span> <span class=\"n\">Xin</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "        \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_</span>          <span class=\"o\">=</span> <span class=\"n\">X</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span>          <span class=\"o\">=</span> <span class=\"n\">y</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span>     <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>   <span class=\"c\"># Normal matrix</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span> <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span>      <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"c\"># Quantities that measure global quality of fit</span>\n",
        "        <span class=\"c\">#  RSE   - Global lack of fit</span>\n",
        "        <span class=\"c\">#  R**2  - The proportion of variablity in Y that is explained by the model</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>        <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rsqr_</span>       <span class=\"o\">=</span> <span class=\"n\">Rsquared</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span>          <span class=\"o\">=</span> <span class=\"n\">Fscore</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fprob_</span>      <span class=\"o\">=</span> <span class=\"n\">f_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span><span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"c\"># Estimating quality of fit for different predictors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>     <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">StdError_</span><span class=\"p\">())</span>      \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critval_</span>    <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"o\">/</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>              <span class=\"c\"># How important is a coefficient</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span><span class=\"p\">()</span>    \n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:])</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "    <span class=\"c\"># Compute confidence inbterval for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">confinterval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">CI</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"n\">critval</span> <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"p\">)</span>\n",
        "        <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">,</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span><span class=\"p\">):</span>\n",
        "            <span class=\"n\">ci_low</span>  <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">-</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">ci_high</span> <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">+</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">CI</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">ci_low</span><span class=\"p\">,</span><span class=\"n\">ci_high</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">CI</span>\n",
        "\n",
        "    <span class=\"c\"># Compute p-values for all predictors. Each p-value is the probability that we get the value of the preduictor</span>\n",
        "    <span class=\"c\"># that we got,under the hypothesis that the predictor value is zero (that is, the predictor is not</span>\n",
        "    <span class=\"c\"># impportant)</span>\n",
        "    <span class=\"c\"># If the value is small, e.g. &lt; 0.005 then we reject the null hypothesis and conclude that the coefficient</span>\n",
        "    <span class=\"c\"># is not neglible</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">pval_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">PV</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">PV</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">PV</span>\n",
        "            \n",
        "    <span class=\"c\"># Compute the standard errors for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">StdError_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"n\">Xinvdiag</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"p\">)</span>\n",
        "              \n",
        "        <span class=\"n\">SE</span>  <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">Xinvdiag</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">SE_i</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n",
        "            <span class=\"n\">SE</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">SE_i</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">return</span> <span class=\"n\">SE</span>\n",
        "\n",
        "\n",
        " \n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyRidgeRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">lamda</span><span class=\"p\">,</span><span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">p</span>                <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> \n",
        "        <span class=\"n\">Xlambda</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lamda</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">inv</span>              <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">Xlambda</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">if</span> <span class=\"n\">fit_intercept</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>  <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n",
        "        <span class=\"k\">else</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"n\">inv</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span><span class=\"p\">))</span>\n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "   \n",
        " \n",
        "<span class=\"c\"># Computing standard error (intercept and slope) for simple linear regression</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">StdError</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>         <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">n</span>     <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">print</span> <span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n",
        "    <span class=\"n\">x</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"s\">&#39;TV&#39;</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">xmean</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">xmean_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">xmean</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">xt</span>         <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">xmean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "    <span class=\"n\">SE_B0_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">];</span>\n",
        "    <span class=\"n\">SE_B1_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">];</span>\n",
        "    \n",
        "    <span class=\"n\">SE_B0_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">xmean_sqr</span><span class=\"o\">/</span><span class=\"n\">xt</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"n\">SE_B1_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">/</span><span class=\"n\">xt</span>\n",
        "\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B0 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B1 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr</span>\n",
        "    \n",
        "    <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B0_sqr</span><span class=\"p\">),</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B1_sqr</span><span class=\"p\">)]</span>\n",
        "  \n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<IPython.core.display.HTML at 0xa1ba400>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id=\"NumericalExamplesLinReg\"></a><font color=\"green\">Numerical examples</font>\n",
      "\n",
      "## <font color=\"brown\"><span id=\"AdvertiseDataSet\">Advertise</span>  data set</font>\n",
      "\n",
      "The advertise data set, taken from [ISSL, page 14](http://www-bcf.usc.edu/~gareth/ISL/data.html), consists of the sales of a product along with advertising budgets for the product for three different media: **TV**, **radio** and **newspaper**. We have the relevant data for a total of 200 markets. Based on this result we want to know what is the relation between advertising budgets and sales. Linear regression is discussed in depth in **chapter 3** of [ISSL, pages 59-126](http://www-bcf.usc.edu/~gareth/ISL/data.html) along with detailed numerical analysis of the advertise data set. Below we replicate some of this demonstration using Python libraries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "def getAdvertisingData():\n",
      "    ad_file = os.path.join(data_dir,'Advertising.csv')\n",
      "    ad      = pd.read_csv(ad_file,header=0,index_col=0)\n",
      "\n",
      "    y    = ad['Sales']                 # Response\n",
      "    X    = ad.drop('Sales',axis=1)\n",
      "    \n",
      "    return X,y\n",
      "\n",
      "X,y = getAdvertisingData()\n",
      "\n",
      "print '\\nOLS - stastmodels \\n',30*'-'\n",
      "statsModelsOLS(X,y)\n",
      "print '\\nOLS - scikitledarn\\n',30*'-'\n",
      "scikitledarnOLS(X,y)\n",
      "print '\\nOLS - manual  \\n',30*'-'\n",
      "manualOLS(X,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "OLS - stastmodels \n",
        "------------------------------\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  Sales   R-squared:                       0.897\n",
        "Model:                            OLS   Adj. R-squared:                  0.896\n",
        "Method:                 Least Squares   F-statistic:                     570.3\n",
        "Date:                Sat, 17 Jan 2015   Prob (F-statistic):           1.58e-96\n",
        "Time:                        16:28:35   Log-Likelihood:                -386.18\n",
        "No. Observations:                 200   AIC:                             780.4\n",
        "Df Residuals:                     196   BIC:                             793.6\n",
        "Df Model:                           3                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "const          2.9389      0.312      9.422      0.000         2.324     3.554\n",
        "TV             0.0458      0.001     32.809      0.000         0.043     0.049\n",
        "Radio          0.1885      0.009     21.893      0.000         0.172     0.206\n",
        "Newspaper     -0.0010      0.006     -0.177      0.860        -0.013     0.011\n",
        "==============================================================================\n",
        "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
        "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
        "Kurtosis:                       6.332   Cond. No.                         454.\n",
        "==============================================================================\n",
        "const        2.938889\n",
        "TV           0.045765\n",
        "Radio        0.188530\n",
        "Newspaper   -0.001037\n",
        "dtype: float64\n",
        "RSE(Std err)   1.68982666825\n",
        "R**2           0.897210638179\n",
        "Adjusted R**2  0.89563733162\n",
        "\n",
        "OLS - scikitledarn\n",
        "------------------------------\n",
        "Intercept  2.93888936946\n",
        "Coeff      [ 0.04576465  0.18853002 -0.00103749] <type 'numpy.ndarray'>\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RSE(Std err)    1.68982666825\n",
        "R**2            0.897210638179\n",
        "Variance score  0.897210638179\n",
        "Adjusted R**2   0.89563733162\n",
        "\n",
        "OLS - manual  \n",
        "------------------------------\n",
        "[  2.93888937e+00   4.57646455e-02   1.88530017e-01  -1.03749304e-03]\n",
        "Y-mean         14.0225\n",
        "RSE(Std err)   1.68982666825\n",
        "R**2           0.897210638179\n",
        "Adjusted R**2  0.89563733162\n",
        "F-statistics   570.270703659\n",
        "F-prob         1.57522725609e-96\n",
        "\n",
        "\n",
        "stderr  [ 0.31270698  0.00139847  0.00863329  0.00588604]\n",
        "zscore  [  9.39822125  32.72482193  21.83757388  -0.17626321]\n",
        "pval    [1.4379966154089449e-17, 1.3835731297938475e-81, 1.6516169260419794e-54, 0.86026822997881336]\n",
        "confint [(2.3222064938156306, 3.5555722451032183), (0.043006754274658714, 0.048522536636136392), (0.17150449496206518, 0.20555553887434333), (-0.012645237534232379, 0.010570251449280111)]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"brown\"><span id=\"NormalizeDataSet\">Feature</span> scaling/standardization</font>\n",
      "\n",
      "\n",
      "Normalization is done to have the same range of values for each of the predictors and help improving the stability of the solution. The scikitlearn package has a nice [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) module that perfroms various forms of normalization transformations. We should do some bookeeping operations so we can apply correctly the transformation both when learning from the trainuing data and when predicting new instances. \n",
      "\n",
      "A subtle point on computing standard deviations: Most Python functions, in particular the [std](http://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html) function,  will use the formula\n",
      "> $\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (v_i - \\mu)^2} \\quad, \\quad \\mu = \\frac{1}{N} \\sum_{i=1}^{N} v_i$\n",
      "\n",
      "which provides a maximum likelihood estimate of the variance for normally distributed variables while most R implementations will use the formula\n",
      "> $\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (v_i - \\mu)^2} $\n",
      "\n",
      "that provides an unbiased estimator of the variance of the infinite population. Some Python functions, e.g. [std](http://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html), allows specifying a parameter <code>ddof</code> in the formula \n",
      "> $\\sqrt{\\frac{1}{N-\\tt{ddof}} \\sum_{i=1}^N (v_i - \\mu)^2} $\n",
      "\n",
      "but others, like the [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale) function of scikitlearn do not provide such parameter and will compute the standrad deviation parameter using the default value of <code>ddof=0</code>. This should be of little concern in practice except when comparing the results with papers of R code that use the formula with <code>ddof=1</code>.\n",
      "\n",
      "In the examples below I will use the function <code>normalize</code> below with the default value of <code>ddof=1</code> so that we can compare the results to those of the [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/)/[ISL](http://www-bcf.usc.edu/~gareth/ISL/) books that use R code for the computation.\n",
      "\n",
      "http://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/\n",
      "\n",
      "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize input data frame. Assumes all predictors are quantitive variables\n",
      "# do not touch constant column. Return\n",
      "#   normalized data frame, vector of means, vector of standard deviations\n",
      "# so that future inputs can be normalize before prediction\n",
      "def normalize(X,ddof=0):\n",
      "    Xnorm = X.copy()\n",
      "    Xmean = []\n",
      "    Xstd  = []\n",
      "    # now iterate over the remaining columns and create a new zscore column\n",
      "    for col in X.columns:\n",
      "        col_std  = Xnorm[col].std(ddof=ddof)\n",
      "        col_mean = Xnorm[col].mean()\n",
      "        if col_std > 0.0:\n",
      "            Xmean.append(col_mean)\n",
      "            Xstd.append(col_std)\n",
      "            Xnorm[col] = (Xnorm[col] - col_mean )/col_std\n",
      "        else:\n",
      "            Xmean.append(0.0)\n",
      "            Xstd.append(1.0)\n",
      "    return Xnorm,Xmean,Xstd\n",
      "\n",
      "from sklearn import preprocessing\n",
      "X , y = getAdvertisingData()\n",
      "\n",
      "ddof = 1\n",
      "X_scaled, x_mean, x_std = normalize(X,ddof)\n",
      "\n",
      "scaler = preprocessing.StandardScaler().fit(X)\n",
      "print 'Mean ',scaler.mean_,x_mean\n",
      "print 'Std  ',scaler.std_ ,x_std\n",
      "X_scaled_scikitlearn = scaler.transform(X)\n",
      "\n",
      "print X_scaled.mean(axis=0),X_scaled.std(axis=0)\n",
      "print X_scaled_scikitlearn.mean(axis=0), X_scaled_scikitlearn.std(axis=0)\n",
      "\n",
      "print np.linalg.norm(X_scaled - X_scaled_scikitlearn,axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean  [ 147.0425   23.264    30.554 ] [147.04249999999999, 23.264000000000006, 30.553999999999995]\n",
        "Std   [ 85.63933176  14.80964564  21.72410606] [85.854236314908064, 14.846809176168712, 21.77862083852283]\n",
        "TV           1.093570e-16\n",
        "Radio       -4.241052e-16\n",
        "Newspaper    2.264855e-16\n",
        "dtype: float64 TV           1\n",
        "Radio        1\n",
        "Newspaper    1\n",
        "dtype: float64\n",
        "[  1.22124533e-16  -4.52970994e-16   2.22044605e-16] [ 1.  1.  1.]\n",
        "[ 0.03539964  0.03539964  0.03539964]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color=\"brown\"><span id=\"ProstateDataSet\">Prostate</span> data set</font>\n",
      "\n",
      "The prostate data set that is presdented in page 3 of [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/). This data came from a research that  examined the correlation between the level of prostate specific antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectomy. The goal is to predict the log of PSA (lpsa) from a number of measurements including log cancer volume (lcavol), log prostate weight lweight, age, log of benign prostatic hyperplasia amount lbph, seminal vesicle invasion svi, log of capsular penetration lcp, Gleason score gleason, and percent of Gleason scores 4 or 5 pgg45. **svi** and **gleason** are categorical values while the other are quantitive variables. The response is a quatitive variable which make this a **regression** problem.\n",
      "\n",
      "Note that the correlations and linear fit results displayed in page 50 of [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/) relate to the train set consisting of 67 persons rather than on the whole 97 persons in this data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#-------------------------------------------\n",
      "# Read the data set\n",
      "#-------------------------------------------\n",
      "prostate_file = os.path.join(data_dir,'prostate.csv')\n",
      "prostate      = pd.read_csv(prostate_file,header=0,delim_whitespace=True,index_col=0)\n",
      "\n",
      "#-------------------------------------------\n",
      "# Normalize, split to train and test sets\n",
      "#-------------------------------------------\n",
      "T  = prostate['train']   # Get column that splits between terain and test sets\n",
      "Y  = prostate['lpsa']    # Get response\n",
      "\n",
      "\n",
      "prostate.drop(['train','lpsa'],axis=1,inplace=True)     # Remove the 'train column'\n",
      "prostate,ProstateMean,ProstateStd = normalize(prostate) # Normalize predictor columns\n",
      "\n",
      "# Divide into training and test set based on T\n",
      "Xtrain = prostate[T=='T']\n",
      "Ytrain = Y[T=='T']\n",
      "\n",
      "Xtest  = prostate[T=='F']\n",
      "Ytest  = Y[T=='F']\n",
      "\n",
      "print 'Shape of train: ',Xtrain.shape,Ytrain.shape\n",
      "print 'Shape of test:  ',Xtest.shape,Ytest.shape\n",
      "\n",
      "\n",
      "statsModelsOLS(Xtrain,Ytrain)\n",
      "print '\\nmanualOLS \\n',30*'='\n",
      "manualOLS(Xtrain,Ytrain)\n",
      "print '\\nscikitledarnOLS \\n',30*'='\n",
      "scikitledarnOLS(Xtrain,Ytrain,normalize =False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shape of train:  (67, 8) (67,)\n",
        "Shape of test:   (30, 8) (30,)\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                   lpsa   R-squared:                       0.694\n",
        "Model:                            OLS   Adj. R-squared:                  0.652\n",
        "Method:                 Least Squares   F-statistic:                     16.47\n",
        "Date:                Sun, 11 Jan 2015   Prob (F-statistic):           2.04e-12\n",
        "Time:                        13:42:45   Log-Likelihood:                -67.505\n",
        "No. Observations:                  67   AIC:                             153.0\n",
        "Df Residuals:                      58   BIC:                             172.9\n",
        "Df Model:                           8                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "const          2.4649      0.089     27.598      0.000         2.286     2.644\n",
        "lcavol         0.6795      0.127      5.366      0.000         0.426     0.933\n",
        "lweight        0.2631      0.096      2.751      0.008         0.072     0.454\n",
        "age           -0.1415      0.101     -1.396      0.168        -0.344     0.061\n",
        "lbph           0.2101      0.102      2.056      0.044         0.006     0.415\n",
        "svi            0.3052      0.124      2.469      0.017         0.058     0.553\n",
        "lcp           -0.2885      0.155     -1.867      0.067        -0.598     0.021\n",
        "gleason       -0.0213      0.145     -0.147      0.884        -0.312     0.269\n",
        "pgg45          0.2670      0.154      1.738      0.088        -0.041     0.574\n",
        "==============================================================================\n",
        "Omnibus:                        0.825   Durbin-Watson:                   1.690\n",
        "Prob(Omnibus):                  0.662   Jarque-Bera (JB):                0.389\n",
        "Skew:                          -0.164   Prob(JB):                        0.823\n",
        "Kurtosis:                       3.178   Cond. No.                         4.47\n",
        "==============================================================================\n",
        "const      2.464933\n",
        "lcavol     0.679528\n",
        "lweight    0.263053\n",
        "age       -0.141465\n",
        "lbph       0.210147\n",
        "svi        0.305201\n",
        "lcp       -0.288493\n",
        "gleason   -0.021305\n",
        "pgg45      0.266956\n",
        "dtype: float64\n",
        "RSE(Std err)   0.718507034723\n",
        "R**2           0.694371179677\n",
        "Adjusted R**2  0.652215480322\n",
        "\n",
        "manualOLS \n",
        "==============================\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Y-mean         2.45234508507\n",
        "RSE(Std err)   0.718507034723\n",
        "R**2           0.694371179677\n",
        "Adjusted R**2  0.652215480322\n",
        "F-statistics   16.47158487\n",
        "F-prob         2.04232650854e-12\n",
        "\n",
        "\n",
        "stderr  [ 0.09009504  0.12773498  0.09646341  0.10222755  0.1031118   0.12467976\n",
        "  0.15587896  0.14651579  0.1549552 ]\n",
        "zscore  [ 27.35925314   5.31982819   2.72697258  -1.38382296   2.03804576\n",
        "   2.44787594  -1.85074859  -0.14541122   1.72279321]\n",
        "pval    [3.2326824673462138e-35, 1.6812568165799175e-06, 0.0084043487155551731, 0.17162571506279692, 0.046036390010150058, 0.017364063891657074, 0.069213805601909748, 0.8848818518505408, 0.090164139251352957]\n",
        "confint [(2.2846531697319858, 2.6452126745155033), (0.42393103922377096, 0.93512524325217883), (0.070030234595327562, 0.45607589686975303), (-0.34602169206201688, 0.063092024989673978), (0.0038203316719639946, 0.41647278277169142), (0.055716963877636583, 0.5546842303725591), (-0.60040585916220235, 0.023420314255111208), (-0.31448245321229695, 0.27187237560640165), (-0.043108872637428297, 0.57702039687727491)]\n",
        "\n",
        "scikitledarnOLS \n",
        "==============================\n",
        "Intercept  2.46493292212\n",
        "Coeff      [ 0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006  -0.28849277\n",
        " -0.02130504  0.26695576] <type 'numpy.ndarray'>\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RSE(Std err)    0.718507034723\n",
        "R**2            0.694371179677\n",
        "Variance score  0.694371179677\n",
        "Adjusted R**2   0.652215480322\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id=\"RegularizationLinReg\"></a> Regularization in linear regression\n",
      "\n",
      "## <font color=\"brown\"><span id=\"RidgeRegression\">Ridge</span> regression</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ridge regression determine the coefficients of the linear model by solving the minimization problem $\\min_w ||\\bf{Xw}-y||_2^2 + \\lambda ||w||_2^2$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$ where $\\lambda$ is a free parameter that has to be determined. The larger $\\lambda$ is the more it **shrinks** the value of the coefficients and maker the solution more reguralized. It solved the problem of stability inherent in linear regression at the price of extra parameter that has to be detetmined. \n",
      "\n",
      "To do a proper ridge regression we need to normalize the input matrix and to estimate the intercept term as the mean of the right hand side vector $y$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.grid_search  import GridSearchCV\n",
      "\n",
      "X = Xtrain.copy()\n",
      "Y = Ytrain.copy()\n",
      "print X.shape\n",
      "\n",
      "def dfRidgeRegressor(lamda,D):\n",
      "    df = 0.0\n",
      "    for d in D:\n",
      "        d2 = d*d\n",
      "        df += d2/(d2+lamda)\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_score(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    return np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_std_err(estimator,Xtest,Ytest):\n",
      "    ypred     = estimator.predict(Xtest)\n",
      "    ysqr      = (Ytest-ypred)**2\n",
      "    y_std_err = np.std(ysqr)/np.sqrt(len(ysqr)-1)\n",
      "    return y_std_err\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print '\\nDegrees of freedom for different values of lambda\\n',50*'-'\n",
      "P, D, Q = np.linalg.svd(X, full_matrices=False)\n",
      "print 'Singular values ',D\n",
      "r = np.arange(0.0,5.0,0.5)\n",
      "for lamda in r:\n",
      "    print 'lamda ',lamda,' df ',dfRidgeRegressor(lamda,D=D)\n",
      "lamda = 24.0\n",
      "print 'lamda ',lamda,' df ',dfRidgeRegressor(lamda,D=D)\n",
      "\n",
      "print '\\nCompare my and scikitlearn  ridge regression\\n',50*'-'\n",
      "lamda = 0.5\n",
      "my_est = sr_linreg.MyRidgeRegressor()\n",
      "my_est.fit(X,Y,lamda)\n",
      "print 'My ridge regression lamda=',lamda,'\\n  coef=',my_est.coef_\n",
      "print ' Intercept ',my_est.intercept_\n",
      "print ' Error     ',RSS_score(my_est,Xtest,Ytest)\n",
      "print ' Std Error ',RSS_std_err(my_est,Xtest,Ytest)\n",
      " \n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,Y)\n",
      "print 'Scikit ridge regression lamda=',lamda,'\\n  coef=',ridge_est.coef_\n",
      "print ' Intercept ',ridge_est.intercept_\n",
      "print ' Error     ',RSS_score(ridge_est,Xtest,Ytest)\n",
      "print ' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "\n",
      "    \n",
      "print '\\nRidge regression producing coefs ~ ESL pp 63\\n',50*'-'\n",
      "lamda = 24.0\n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,Y)\n",
      "print 'lamda ',lamda,'  Error ',RSS_score(ridge_est,Xtest,Ytest),' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_est.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n",
        "\n",
        "Degrees of freedom for different values of lambda\n",
        "--------------------------------------------------\n",
        "Singular values  [ 15.37564687  10.8584466    8.47008295   6.4917243    5.82635313\n",
        "   5.16077118   4.32731217   3.48856279]\n",
        "lamda  0.0  df  8.0\n",
        "lamda  0.5  df  7.87660748341\n",
        "lamda  1.0  df  7.75902336159\n",
        "lamda  1.5  df  7.64675613811\n",
        "lamda  2.0  df  7.53937410431\n",
        "lamda  2.5  df  7.43649601366\n",
        "lamda  3.0  df  7.33778350227\n",
        "lamda  3.5  df  7.24293487888\n",
        "lamda  4.0  df  7.15167999866\n",
        "lamda  4.5  df  7.0637760023\n",
        "lamda  24.0  df  5.0117601134\n",
        "\n",
        "Compare my and scikitlearn  ridge regression\n",
        "--------------------------------------------------\n",
        "My ridge regression lamda= 0.5 \n",
        "  coef= [ 0.66732936  0.26268005 -0.13581476  0.20837773  0.30251696 -0.27381514\n",
        " -0.01942982  0.26089392]\n",
        " Intercept  2.45234508507\n",
        " Error      0.519649308021\n",
        " Std Error  0.179720807509\n",
        "Scikit ridge regression lamda= 0.5 \n",
        "  coef= [ 0.66776889  0.2630757  -0.13801607  0.20889782  0.30201872 -0.27227083\n",
        " -0.01641616  0.25758736]\n",
        " Intercept  2.46536762536\n",
        " Error      0.5166754343\n",
        " Std Error  0.176343649302\n",
        "\n",
        "Ridge regression producing coefs ~ ESL pp 63\n",
        "--------------------------------------------------\n",
        "lamda  24.0   Error  0.490361320535  Std Error  0.162269156393\n",
        "lcavol     : 0.421\n",
        "lweight    : 0.239\n",
        "age        : -0.048\n",
        "lbph       : 0.162\n",
        "svi        : 0.227\n",
        "lcp        : -0.000\n",
        "gleason    : 0.041\n",
        "pgg45      : 0.132\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      " \n",
      "def constructSet(x1,x2):\n",
      "    #print x1\n",
      "    X = np.zeros([len(x1),8])\n",
      "    X[:,0] = 1\n",
      "    X[:,1] = x1\n",
      "    X[:,2] = x2\n",
      "    X[:,3] = x1*x1\n",
      "    X[:,4] = x2*x2\n",
      "    X[:,5] = x1*x2\n",
      "    X[:,6] = np.abs(x1-x2)\n",
      "    X[:,7] = np.abs(x1+x2)\n",
      "    return X\n",
      "   \n",
      " \n",
      "def misClassificationError(model,Xin,yin):\n",
      "    nerrs = 0\n",
      "    for i in xrange(len(yin)):\n",
      "        yp = model.predict(Xin[i])    \n",
      "        if np.sign(yp) != np.sign(yin[i]) : \n",
      "            #print yp,yin[i]\n",
      "            nerrs += 1\n",
      "    err = float(nerrs)/float(len(yin))\n",
      "    # print '--- nerr n err ',nerrs,len(yin),err\n",
      "    return err\n",
      "       \n",
      "\n",
      "train_file = os.path.join(data_dir,'train_data.txt')\n",
      "train_df   = pd.read_fwf(train_file,colspecs='infer',header=None,names=['x1','x2','y'])\n",
      " \n",
      "test_file = os.path.join(data_dir,'test_data.txt')\n",
      "test_df   = pd.read_fwf(test_file,colspecs='infer',header=None,names=['x1','x2','y'])\n",
      "train_df.head()\n",
      "print len(train_df)\n",
      "print len(test_df)\n",
      " \n",
      "x1 = train_df['x1']\n",
      "x2 = train_df['x2']\n",
      "y  = train_df['y']\n",
      "X  = constructSet(x1,x2)\n",
      " \n",
      "x1_test = test_df['x1']\n",
      "x2_test = test_df['x2']\n",
      "y_test  = test_df['y']\n",
      "X_test  = constructSet(x1_test,x2_test)\n",
      " \n",
      " \n",
      "clf = linear_model.LinearRegression(fit_intercept=False)\n",
      "clf.fit(X,y)\n",
      "print clf.coef_\n",
      " \n",
      "# Misclassification error\n",
      "print(\"Missclassification train error: %.5f\" % misClassificationError(clf,X,y))\n",
      "print(\"Missclassification test  error: %.5f\" % misClassificationError(clf,X_test,y_test))\n",
      " \n",
      "all = [-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10]\n",
      "all = [3,2,1,0,-1,-2,-3]\n",
      "#all = [3]\n",
      "for k in all:\n",
      "    lam         = 10**k\n",
      "    w           = lam/len(y)\n",
      "    w = lam\n",
      "    #print '\\n  k  lambda w ',k,lam,w,'\\n',50*'-'\n",
      "    clf_ridge   = linear_model.Ridge (alpha = w,fit_intercept=False)\n",
      "    clf_ridge.fit(X,y)\n",
      "    #print 'coef scikitl ',clf_ridge.coef_\n",
      "   \n",
      "    Xt = X.transpose()\n",
      "    I  = np.eye(8)\n",
      "    M  = np.linalg.inv(Xt.dot(X) + w*I)\n",
      "    coef =  M.dot(Xt.dot(y))\n",
      "    #print 'coef mine   ',clf_ridge.coef_\n",
      "    Err = np.sqrt( np.sum((coef-clf_ridge.coef_)**2 ))\n",
      "\n",
      "    # Misclassification error\n",
      "    #print '\\nk = ',k\n",
      "    #print(\"  Missclassification train error: %.5f\" % misClassificationError(clf_ridge,X,y))\n",
      "    #print(\"  Missclassification test  error: %.5f\" % misClassificationError(clf_ridge,X_test,y_test))\n",
      "    in_sample     = misClassificationError(clf_ridge,X,y)\n",
      "    out_of_sample = misClassificationError(clf_ridge,X_test,y_test)\n",
      "    print 'k in out (err)',k,in_sample,out_of_sample,Err\n",
      "\n",
      "\n",
      "\n",
      "#all = [-3]\n",
      "all = []\n",
      "for k in all:\n",
      "    lam         = 10**k\n",
      "    w           = lam/len(y)\n",
      "    # print '\\n  k  lambda w ',k,lam,w,'\\n',50*'-'\n",
      "    clf_ridge   = linear_model.Ridge (alpha = w,fit_intercept=False)\n",
      "    clf_ridge.fit(X,y)\n",
      "    #print 'coef scikitl ',clf_ridge.coef_\n",
      "   \n",
      "    Xt = X.transpose()\n",
      "    I  = np.eye(8)\n",
      "    M  = np.linalg.inv(Xt.dot(X) + w*I)\n",
      "    coef =  M.dot(Xt.dot(y))\n",
      "    #print 'coef mine   ',clf_ridge.coef_\n",
      "    print 'Err ',np.sqrt( np.sum((coef-clf_ridge.coef_)**2 ))\n",
      "\n",
      "    # Misclassification error\n",
      "    in_sample     = misClassificationError(clf_ridge,X,y)\n",
      "    out_of_sample = misClassificationError(clf_ridge,X_test,y_test)\n",
      "    print '\\nk = ',k\n",
      "    print 'k in out ',k,in_sample,out_of_sample\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35\n",
        "250\n",
        "[-1.64706706 -0.14505927  0.10154121 -2.03296844 -1.82804373  2.48152945\n",
        "  4.15893861  0.31651714]\n",
        "Missclassification train error: 0.02857\n",
        "Missclassification test  error: 0.08400\n",
        "k in out (err) 3 0.371428571429 0.436 1.30104260698e-18\n",
        "k in out (err) 2 0.2 0.228 5.974101546e-17\n",
        "k in out (err) 1 0.0571428571429 0.124 1.79617192479e-16\n",
        "k in out (err) 0 0.0 0.092 2.82848245336e-15\n",
        "k in out (err) -1 0.0285714285714 0.056 8.48005514301e-15\n",
        "k in out (err) -2 0.0285714285714 0.084 4.50151285501e-14\n",
        "k in out (err) -3 0.0285714285714 0.08 1.64163378221e-14\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Estimating $\\lambda$ by cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.grid_search  import GridSearchCV\n",
      "import math\n",
      "\n",
      "\n",
      "print '\\nDetermine lambda by cross-validation\\n',50*'-'\n",
      "lamdas_to_try = np.arange(0.0,30.0,0.5)\n",
      "model = Ridge()\n",
      "grid  = GridSearchCV(estimator=model,param_grid=dict(alpha=lamdas_to_try))\n",
      "grid.fit(X,y)\n",
      "print grid.best_score_\n",
      "print grid.best_estimator_.alpha\n",
      "\n",
      "print '\\nRidge regression producing coefs ~ ESL pp 63\\n',50*'-'\n",
      "lamda = grid.best_estimator_.alpha\n",
      "ridge_est = Ridge(lamda,fit_intercept =True)\n",
      "ridge_est.fit(X,y)\n",
      "\n",
      "print 'lamda ',lamda,'  Error ',RSS_score(ridge_est,Xtest,Ytest),' Std Error ',RSS_std_err(ridge_est,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_est.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n",
      "\n",
      "    \n",
      "print '\\n\\nNEXT\\n'\n",
      "    \n",
      "# DOES NOT WORK. NEED TO CHECK WHY\n",
      "# Residual sum of squares\n",
      "def RSS_score1(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    score = np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "    if math.isnan(score):\n",
      "        print 'NaN detected '\n",
      "        score = 999999999999999.0\n",
      "    return score\n",
      "\n",
      "ridge_cv = RidgeCV(lamdas_to_try,scoring=RSS_score1)\n",
      "ridge_cv.fit(X,y)\n",
      "print '\\nRidge CV found lambda ',ridge_cv.alpha_\n",
      "print '  Error ',RSS_score1(ridge_cv,Xtest,Ytest)\n",
      "for (name,c) in zip(X.columns,ridge_cv.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Determine lambda by cross-validation\n",
        "--------------------------------------------------\n",
        "0.610439241395"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0\n",
        "\n",
        "Ridge regression producing coefs ~ ESL pp 63\n",
        "--------------------------------------------------\n",
        "lamda  0.0   Error  40.8307847327  Std Error  7.6788004628\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'columns'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-aabf390b0b2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'lamda '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'  Error '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRSS_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mridge_est\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' Std Error '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRSS_std_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mridge_est\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mridge_est\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'{:10s} : {:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"LASSO\">Ridge</span>  and LAR\n",
      "\n",
      "\n",
      "The **lassso** replaces the $L_2$ penalty of riidge regression by $L_1$ penalty, that is we search for a coefficient vector such that \n",
      "$\\min_w ||\\bf{Xw}-y||_2^2 + \\lambda ||w||_1$ for all observations $(x_i,y_i) \\quad,\\quad i=1,\\ldots,N$ where $\\lambda$ is a free parameter that has to be determined. The larger $\\lambda$ is the more it **shrinks** the value of the coefficients and maker the solution more reguralized. It solved the problem of stability inherent in linear regression at the price of extra parameter that has to be detetmined. The particular property of the lasso solution is that some parameters are shrinked to zero.\n",
      "\n",
      "As in ridge regression, we should make sure to normalize the input matrix and to estimate the intercept term as the mean of the right hand side vector $y$.\n",
      "\n",
      "The [Least Angle Regression](http://arxiv.org/pdf/math/0406456.pdf) paper contains the algorithmic details on the efficient comptational procedures implementing LARS, lasso and other similar model selection methods. The diabetics data set from the paper cab be loaded from [this website](http://www4.stat.ncsu.edu/~boos/var.select/). A [C/C++ implementation](http://www.mlpack.org/doxygen.php?doc=classmlpack_1_1regression_1_1LARS.html) of LARS/lasso is available in the [mlpack package](http://www.mlpack.org/index.html)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "print '\\nDetermine alpha for lasso by cross-validation\\n',50*'-'\n",
      "lamdas_to_try = np.arange(0.1,0.3,0.005)\n",
      "model = linear_model.Lasso()\n",
      "grid  = GridSearchCV(estimator=model,param_grid=dict(alpha=lamdas_to_try))\n",
      "grid.fit(X,y)\n",
      "print 'Final score CV ',grid.best_score_\n",
      "print 'Best estimation of alpha=',grid.best_estimator_.alpha\n",
      "\n",
      "# Compute Lasso regression with optimal alpha\n",
      "est_lasso = linear_model.Lasso(alpha=grid.best_estimator_.alpha,fit_intercept=True)\n",
      "est_lasso.fit(X,y)\n",
      "\n",
      "\n",
      "print '\\nalpha ',grid.best_estimator_.alpha,'  Error ',RSS_score(est_lasso,Xtest,Ytest),' Std Error ',RSS_std_err(est_lasso,Xtest,Ytest)\n",
      "print 'Intercept ',est_lasso.intercept_\n",
      "for (name,c) in zip(X.columns,est_lasso.coef_):\n",
      "    print ('{:10s} : {:.3f}').format(name,c)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Determine alpha for lasso by cross-validation\n",
        "--------------------------------------------------\n",
        "Final score CV "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.618898500211\n",
        "Best estimation of alpha= 0.1\n",
        "\n",
        "alpha  0.1   Error  11.9188516274  Std Error  1.47480028497\n",
        "Intercept  -0.769199716439\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'columns'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-850afaa099cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\nalpha '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'  Error '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRSS_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest_lasso\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' Std Error '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRSS_std_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest_lasso\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Intercept '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mest_lasso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mest_lasso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'{:10s} : {:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <span id=\"DimReductionLinReg\">Dimensionality</span> reduction\n",
      "\n",
      "\n",
      "## <span id=\"FeatureSelectionOLS\">Forward</span> feature selection\n",
      "\n",
      "Forward selection of predictors by 10-fold cross validation and taking RSS error on test set/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn              import cross_validation\n",
      "\n",
      "X = Xtrain.copy()\n",
      "Y = Ytrain.copy()\n",
      "print X.shape\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_score(estimator,Xtest,Ytest):\n",
      "    ypred = estimator.predict(Xtest)\n",
      "    return np.sum((Ytest-ypred)**2)/float(len(Ytest))\n",
      "\n",
      "# Residual sum of squares\n",
      "def RSS_std_err(estimator,Xtest,Ytest):\n",
      "    ypred     = estimator.predict(Xtest)\n",
      "    ysqr      = (Ytest-ypred)**2\n",
      "    y_std_err = np.std(ysqr)/np.sqrt(len(ysqr)-1)\n",
      "    return y_std_err\n",
      "\n",
      "\n",
      "#--------------------------------------------\n",
      "# Define linear regressor\n",
      "#--------------------------------------------\n",
      "est = LinearRegression(fit_intercept=True)\n",
      "est.fit(X,y)\n",
      "\n",
      "\n",
      "scores = cross_validation.cross_val_score(est, X, Y, scoring='mean_squared_error',cv=3)\n",
      "print 'Scores using scikitlearn ',scores, ' mean score ',scores.mean()\n",
      " \n",
      "scores = cross_validation.cross_val_score(est, X, Y, scoring=RSS_score,cv=3)\n",
      "print 'Scores using my score function ',scores, ' mean score ',scores.mean()\n",
      "\n",
      "names_of_predictors = list(X.columns)\n",
      "print 'Forward selection algorithm for predictors\\n ',names_of_predictors\n",
      "\n",
      "\n",
      "cnames = []\n",
      "while names_of_predictors:\n",
      "    scores            = []\n",
      "    # Add each of remaining predictors in turn and compute its cross-validation score\n",
      "    for p in names_of_predictors:\n",
      "        Xin    = Xfinal.copy()\n",
      "        Xin[p] = X[p]            # Add predictor p\n",
      "        est.fit(Xin,y)\n",
      "        score = cross_validation.cross_val_score(est, Xin, y, scoring=RSS_score,cv=10)\n",
      "        scores.append(score.mean())\n",
      "\n",
      "    # Add predictor with maximal score to 'active predictors'\n",
      "    min_index                 = np.argmin(scores)\n",
      "    selectedPredictor         = names_of_predictors[min_index]\n",
      "    cnames.append(selectedPredictor)\n",
      "  \n",
      "    # Compute estimator based on current active predictors\n",
      "    Xfinal = X[cnames]    \n",
      "    est.fit(Xfinal,y)\n",
      "    \n",
      "    XtestFinal = Xtest[cnames]\n",
      "    \n",
      "    print ('Selected predictor {:10s} cv score {:.3f} Test error {:.3f} Std err {:.3f} R**2 {:.3f} '\\\n",
      "           .format(selectedPredictor,scores[min_index],\\\n",
      "                   RSS_score(est,XtestFinal,Ytest),\\\n",
      "                   RSS_std_err(est,XtestFinal,Ytest),\\\n",
      "                   est.score(Xfinal,y)))\n",
      " \n",
      "    del names_of_predictors[min_index]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "incompatible dimensions",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-f7a8c506ed45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#--------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Shmuel\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, n_jobs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidues_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Shmuel\\Anaconda\\lib\\site-packages\\scipy\\linalg\\basic.pyc\u001b[0m in \u001b[0;36mlstsq\u001b[1;34m(a, b, cond, overwrite_a, overwrite_b, check_finite)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mnrhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'incompatible dimensions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[0mgelss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gelss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: incompatible dimensions"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"PCA\">PCA</span>  \n",
      "\n",
      "\n",
      "We normalize ol predictors.\n",
      "\n",
      "We consider the SVD decomposition of $X$, $X=UDV^T$ where $U$ is an orthnormal $n \\times p$ matrix that span the column space of $X$, $V^T$ is a $p \\times p$ matrix that span the row space of $X$ and $D = \\tt{diag} (d_1,\\ldots,d_p)$ is a diagonal $p \\times p$ matrix.\n",
      "\n",
      "The normal matrix $X^TX = VDU^TUDV^T=VD^2V^T$ and so $\\beta = (X^TX)^{-1}X^Ty = VD^{-1}U^Ty$ where $D^{-1} = \\tt{diag} (\\frac{1}{d_1},\\ldots,\\frac{1}{d_p})$\n",
      "\n",
      "We consider now the expantion of the solution $\\beta = \\sum_{i=1}^{p} \\alpha_i V_i$ then we have $\\hat{y} = X \\beta =  \\sum_{i=1}^{p} \\alpha_i d_i U_i$ and so $\\alpha_i = \\frac{(\\hat{y},U_i)}{d_i(U_i,U_i)}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyPCARegressor(object):\n",
      "    def __init__(self,X,Y):\n",
      "        self.intercept_  = Y.mean()\n",
      "        U, D, VT         = np.linalg.svd(X, full_matrices=False)\n",
      "        \n",
      "        self.p_     = X.shape[1]\n",
      "        self.VT_    = VT\n",
      "        self.sum_s_ = np.sum(D)\n",
      "        self.D_     = D\n",
      " \n",
      "        # solve with SVD\n",
      "        Dinv           = [1/d for d in D]\n",
      "        UT             = U.transpose()\n",
      "        UTy            = UT.dot(Y)\n",
      "        DinvUTy        = np.diag(Dinv).dot(UTy)\n",
      "        self.ls_sol_   = VT.transpose().dot(DinvUTy)\n",
      "\n",
      "        # Compute coefficients of the expantion of th eLS solution in terms of V[i]\n",
      "        ypred        = X.dot(self.ls_sol_) \n",
      "        self.alphas_ = [ (ypred.dot(UT[i]))/(D[i]) for i in xrange(len(self.ls_sol_))   ]     \n",
      "        \n",
      "    def fit(self, M):\n",
      "        \"\"\"Fits estimator to data. \"\"\"\n",
      "        if M > self.p_:\n",
      "            M = self.p_ \n",
      "        self.coef_  = np.zeros(self.p_)\n",
      "        self.sum_s_in_fit_ = 0.0\n",
      "        for i in xrange(M):\n",
      "            self.sum_s_in_fit_ += self.D_[i]\n",
      "            self.coef_  += self.alphas_[i]*self.VT_[i]\n",
      "\n",
      "        self.variance_explained_ =  self.sum_s_in_fit_/ self.sum_s_\n",
      "        # set state of ``self``\n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict response of ``X``. \"\"\"\n",
      "        # compute predictions ``pred``\n",
      "        pred = X.dot(self.coef_) \n",
      "        return pred\n",
      "\n",
      "   \n",
      "Xone   = np.column_stack( (np.ones(len(X)), X) )\n",
      "XtestO = np.column_stack( (np.ones(len(Xtest)), Xtest) )\n",
      "\n",
      "PCAest = MyPCARegressor(Xone,Y)\n",
      "for i in xrange(Xone.shape[1]):\n",
      "    PCAest.fit(i+1)\n",
      "    print ('{:2d} Test error {:.3f} Standard error {:.3f} % Variance explained  {:.3f} ').format(i,RSS_score(PCAest,XtestO,Ytest),\\\n",
      "                                                                           RSS_std_err(PCAest,XtestO,Ytest),\\\n",
      "                                                                           PCAest.variance_explained_)\n",
      "\n",
      "\n",
      "xnorm      = Xone.transpose().dot(Xone)\n",
      "xninv      = np.linalg.inv(xnorm)\n",
      "sol        = xninv.dot(Xone.transpose().dot(Y))\n",
      "print '\\nCoefs of LS solution by ninverting normal equations\\n',sol\n",
      "\n",
      "print 'Coefs of LS solution by Using SVD expantion\\n',PCAest.ls_sol_\n",
      "\n",
      "PCAest.fit(Xone.shape[1])\n",
      "print 'Coefs of LS solution by Using full PCA expantion\\n',PCAest.coef_\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 Test error 7.082 Standard error 0.770 % Variance explained  0.226 \n",
        " 1 Test error 7.330 Standard error 0.973 % Variance explained  0.385 \n",
        " 2 Test error 6.532 Standard error 1.692 % Variance explained  0.512 \n",
        " 3 Test error 0.969 Standard error 0.301 % Variance explained  0.632 \n",
        " 4 Test error 0.711 Standard error 0.187 % Variance explained  0.725 \n",
        " 5 Test error 0.714 Standard error 0.167 % Variance explained  0.811 \n",
        " 6 Test error 0.516 Standard error 0.149 % Variance explained  0.886 \n",
        " 7 Test error 0.464 Standard error 0.131 % Variance explained  0.950 \n",
        " 8 Test error 0.521 Standard error 0.179 % Variance explained  1.000 \n",
        "\n",
        "Coefs of LS solution by ninverting normal equations\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Coefs of LS solution by Using SVD expantion\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n",
        "Coefs of LS solution by Using full PCA expantion\n",
        "[ 2.46493292  0.67952814  0.26305307 -0.14146483  0.21014656  0.3052006\n",
        " -0.28849277 -0.02130504  0.26695576]\n"
       ]
      }
     ],
     "prompt_number": 414
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X.shape\n",
      "print U.shape\n",
      "print D.shape\n",
      "print VT.shape\n",
      "\n",
      "for i in xrange(len(UT)):\n",
      "    diff = X.dot(V[i]) - UT[i]*D[i]\n",
      "    print i,np.sum(np.abs(diff))/float(len(diff))\n",
      "\n",
      "\n",
      "beta = np.zeros(8)\n",
      "for i in xrange(8):\n",
      "    beta += V[i]\n",
      "    \n",
      "ypred = X.dot(beta)\n",
      "\n",
      "ypred1 = np.zeros(len(ypred))\n",
      "for i in xrange(8):\n",
      "    ypred1 += D[i]*UT[i]\n",
      " \n",
      "alphas = [ (ypred.dot(UT[i]))/(D[i]) for i in xrange(len(sol2))   ]\n",
      "print alphas\n",
      "print ypred1-ypred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(67, 8)\n",
        "(67L, 8L)\n",
        "(8L,)\n",
        "(8L, 8L)\n",
        "0 7.04453079618e-16\n",
        "1 2.30656082957e-15\n",
        "2 7.44481176541e-16\n",
        "3 7.07456481457e-16\n",
        "4 5.99204558277e-16\n",
        "5 3.66078436221e-16\n",
        "6 3.81389618695e-16\n",
        "7 6.46611702822e-16\n",
        "[0.99999999999999989, 1.0000000000000011, 0.99999999999999878, 1.0000000000000002, 1.0000000000000007, 1.0000000000000009, 0.99999999999999956, 0.99999999999999978]\n",
        "1    -2.609024e-15\n",
        "2    -3.552714e-15\n",
        "3    -5.773160e-15\n",
        "4     8.881784e-16\n",
        "5    -2.220446e-15\n",
        "6    -2.553513e-15\n",
        "8    -3.330669e-16\n",
        "11    4.440892e-16\n",
        "12    0.000000e+00\n",
        "13   -4.662937e-15\n",
        "14   -4.440892e-15\n",
        "16   -2.664535e-15\n",
        "17   -4.440892e-16\n",
        "18   -8.881784e-16\n",
        "19   -1.332268e-15\n",
        "...\n",
        "79    1.776357e-15\n",
        "81   -4.857226e-15\n",
        "82   -3.996803e-15\n",
        "83    4.662937e-15\n",
        "85   -8.881784e-16\n",
        "86    1.776357e-15\n",
        "87    2.775558e-16\n",
        "88    0.000000e+00\n",
        "89    8.881784e-15\n",
        "90    2.664535e-15\n",
        "91    1.776357e-15\n",
        "92    1.776357e-15\n",
        "93    2.220446e-15\n",
        "94    1.776357e-15\n",
        "96    8.881784e-16\n",
        "Length: 67, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 359
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <span id=\"RBF\">Radial</span>  Basis Functions (RBF)\n",
      "\n",
      "Linear regression is not restricted to just hyperplanes. The same setting also can be applied to any **basis functions** representation of the form\n",
      "> $\\sum_{k=1}^K w_k \\Phi_k(x) + b$ \n",
      "\n",
      "In that case the data matrix $A$ is defined by $A_{ij}=\\Phi_{j-1}(\\mathbf{x_i}), \\quad i = 1,\\ldots,N , j = 2,\\ldots,K+1$ and the first colums of $A$ is the column vector $\\mathbf{1}$. Popular selection for basis functions are **[radial basis functions](http://en.wikipedia.org/wiki/Radial_basis_function)** in which \n",
      "> $\\Phi_{j}(\\mathbf{x}) = \\Phi( ||\\mathbf{c_j - x}||)$\n",
      "\n",
      "where $c_j \\in \\mathbb{R}^d$ are called **centers** of the RBF. A popular choice for RBF is the **Gaussian**\n",
      "> $\\Phi( r ) = \\exp (-\\gamma r^2)$ where $\\gamma$ is a positive constant.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a id=\"CodeLinReg\"></a>Code for linear regression\n",
      "Execute the Cell bellow to view full code of the ordinay least square procedure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ipython_utils as iputils\n",
      "iputils.print_python_file(module_dir+\"/sr_linreg.py\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style type=\"text/css\">.highlight .hll { background-color: #ffffcc }\n",
        ".highlight  { background: #f8f8f8; }\n",
        ".highlight .c { color: #408080; font-style: italic } /* Comment */\n",
        ".highlight .err { border: 1px solid #FF0000 } /* Error */\n",
        ".highlight .k { color: #008000; font-weight: bold } /* Keyword */\n",
        ".highlight .o { color: #666666 } /* Operator */\n",
        ".highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
        ".highlight .cp { color: #BC7A00 } /* Comment.Preproc */\n",
        ".highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
        ".highlight .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
        ".highlight .gd { color: #A00000 } /* Generic.Deleted */\n",
        ".highlight .ge { font-style: italic } /* Generic.Emph */\n",
        ".highlight .gr { color: #FF0000 } /* Generic.Error */\n",
        ".highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
        ".highlight .gi { color: #00A000 } /* Generic.Inserted */\n",
        ".highlight .go { color: #888888 } /* Generic.Output */\n",
        ".highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
        ".highlight .gs { font-weight: bold } /* Generic.Strong */\n",
        ".highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
        ".highlight .gt { color: #0044DD } /* Generic.Traceback */\n",
        ".highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
        ".highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
        ".highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
        ".highlight .kp { color: #008000 } /* Keyword.Pseudo */\n",
        ".highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
        ".highlight .kt { color: #B00040 } /* Keyword.Type */\n",
        ".highlight .m { color: #666666 } /* Literal.Number */\n",
        ".highlight .s { color: #BA2121 } /* Literal.String */\n",
        ".highlight .na { color: #7D9029 } /* Name.Attribute */\n",
        ".highlight .nb { color: #008000 } /* Name.Builtin */\n",
        ".highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
        ".highlight .no { color: #880000 } /* Name.Constant */\n",
        ".highlight .nd { color: #AA22FF } /* Name.Decorator */\n",
        ".highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
        ".highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
        ".highlight .nf { color: #0000FF } /* Name.Function */\n",
        ".highlight .nl { color: #A0A000 } /* Name.Label */\n",
        ".highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
        ".highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
        ".highlight .nv { color: #19177C } /* Name.Variable */\n",
        ".highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
        ".highlight .w { color: #bbbbbb } /* Text.Whitespace */\n",
        ".highlight .mf { color: #666666 } /* Literal.Number.Float */\n",
        ".highlight .mh { color: #666666 } /* Literal.Number.Hex */\n",
        ".highlight .mi { color: #666666 } /* Literal.Number.Integer */\n",
        ".highlight .mo { color: #666666 } /* Literal.Number.Oct */\n",
        ".highlight .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
        ".highlight .sc { color: #BA2121 } /* Literal.String.Char */\n",
        ".highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
        ".highlight .s2 { color: #BA2121 } /* Literal.String.Double */\n",
        ".highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
        ".highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
        ".highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
        ".highlight .sx { color: #008000 } /* Literal.String.Other */\n",
        ".highlight .sr { color: #BB6688 } /* Literal.String.Regex */\n",
        ".highlight .s1 { color: #BA2121 } /* Literal.String.Single */\n",
        ".highlight .ss { color: #19177C } /* Literal.String.Symbol */\n",
        ".highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
        ".highlight .vc { color: #19177C } /* Name.Variable.Class */\n",
        ".highlight .vg { color: #19177C } /* Name.Variable.Global */\n",
        ".highlight .vi { color: #19177C } /* Name.Variable.Instance */\n",
        ".highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>  \n",
        "<span class=\"kn\">import</span> <span class=\"nn\">numpy.linalg</span> <span class=\"kn\">as</span> <span class=\"nn\">la</span>\n",
        "\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy</span> <span class=\"kn\">as</span> <span class=\"nn\">sp</span>\n",
        "<span class=\"kn\">import</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">as</span> <span class=\"nn\">st</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_critval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">t_pval</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span><span class=\"n\">tstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">t_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ts</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">tstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">t_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">ts</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"mf\">2.0</span><span class=\"o\">*</span><span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_critval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; eturn critical value for a confidence interval for the t-distribution with df degrees of freedom &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">alpha_h</span>     <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">-</span><span class=\"n\">confint</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">2.0</span>\n",
        "    <span class=\"n\">critval</span>     <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">ppf</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">alpha_h</span><span class=\"p\">)</span>  <span class=\"c\"># Taking the poaitive value of critval</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">critval</span>\n",
        "\n",
        "<span class=\"k\">def</span> <span class=\"nf\">f_pval</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">fstat</span><span class=\"p\">):</span>\n",
        "    <span class=\"sd\">&#39;&#39;&#39; Return probability of obtaiuning value of tstat or higher &#39;&#39;&#39;</span>\n",
        "    <span class=\"n\">f_rv</span>        <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"o\">.</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">fs</span>          <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">fstat</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">pval</span>        <span class=\"o\">=</span> <span class=\"n\">f_rv</span><span class=\"o\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"n\">fs</span><span class=\"p\">)</span>  \n",
        "    <span class=\"k\">return</span> <span class=\"n\">pval</span>\n",
        "\n",
        "<span class=\"c\"># Residual sum of squares</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "<span class=\"c\"># Residual standard error - estimation for the Standard deviation of the error terms</span>\n",
        "<span class=\"c\"># Also known as standard error</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n",
        "\n",
        "  \n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Rsquared</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">Rsqr</span>       <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"n\">TSS</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">Rsqr</span>\n",
        "\n",
        "<span class=\"c\"># Compute F score. High F score indicates that mnultiple linear regression</span>\n",
        "<span class=\"c\"># have at least one significant coefficient</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">Fscore</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">n</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">p</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">ypred</span>      <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">ypred_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">rss_c</span>      <span class=\"o\">=</span> <span class=\"n\">RSS</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">TSS</span>        <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">ypred_mean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">a</span>          <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">TSS</span><span class=\"o\">-</span><span class=\"n\">rss_c</span><span class=\"p\">)</span><span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">b</span>          <span class=\"o\">=</span> <span class=\"n\">rss_c</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> \n",
        "    <span class=\"n\">F</span>          <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">/</span><span class=\"n\">b</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">F</span>\n",
        "\n",
        "<span class=\"c\"># Compute R^2 score for an estimator</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">AdjustedRsquared</span><span class=\"p\">(</span><span class=\"n\">rsqr</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">adj</span>        <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"o\">-</span><span class=\"n\">rsqr</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mf\">1.</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span> <span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"k\">return</span> <span class=\"n\">adj</span>\n",
        "\n",
        "\n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyLinearRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">Xin</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">column_stack</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)),</span> <span class=\"n\">Xin</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span>          <span class=\"o\">=</span> <span class=\"n\">Xin</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
        "        \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_</span>          <span class=\"o\">=</span> <span class=\"n\">X</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span>          <span class=\"o\">=</span> <span class=\"n\">y</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span>     <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>   <span class=\"c\"># Normal matrix</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span> <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span>      <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">Xin</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"c\"># Quantities that measure global quality of fit</span>\n",
        "        <span class=\"c\">#  RSE   - Global lack of fit</span>\n",
        "        <span class=\"c\">#  R**2  - The proportion of variablity in Y that is explained by the model</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>        <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ypred_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rsqr_</span>       <span class=\"o\">=</span> <span class=\"n\">Rsquared</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span>          <span class=\"o\">=</span> <span class=\"n\">Fscore</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">Xin</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fprob_</span>      <span class=\"o\">=</span> <span class=\"n\">f_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">F_</span><span class=\"p\">)</span>\n",
        " \n",
        "        <span class=\"c\"># Estimating quality of fit for different predictors</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>     <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">StdError_</span><span class=\"p\">())</span>      \n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critval_</span>    <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">)</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"o\">/</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span>              <span class=\"c\"># How important is a coefficient</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span>       <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pval_</span><span class=\"p\">()</span>    \n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:])</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "    <span class=\"c\"># Compute confidence inbterval for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">confinterval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">CI</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"n\">critval</span> <span class=\"o\">=</span> <span class=\"n\">t_critval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">confint</span><span class=\"p\">)</span>\n",
        "        <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">,</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stderr_</span><span class=\"p\">):</span>\n",
        "            <span class=\"n\">ci_low</span>  <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">-</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">ci_high</span> <span class=\"o\">=</span> <span class=\"n\">c</span> <span class=\"o\">+</span> <span class=\"n\">critval</span><span class=\"o\">*</span><span class=\"n\">s</span>\n",
        "            <span class=\"n\">CI</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">ci_low</span><span class=\"p\">,</span><span class=\"n\">ci_high</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">CI</span>\n",
        "\n",
        "    <span class=\"c\"># Compute p-values for all predictors. Each p-value is the probability that we get the value of the preduictor</span>\n",
        "    <span class=\"c\"># that we got,under the hypothesis that the predictor value is zero (that is, the predictor is not</span>\n",
        "    <span class=\"c\"># impportant)</span>\n",
        "    <span class=\"c\"># If the value is small, e.g. &lt; 0.005 then we reject the null hypothesis and conclude that the coefficient</span>\n",
        "    <span class=\"c\"># is not neglible</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">pval_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">PV</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">zscore_</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">PV</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">t_pval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">p_</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">))</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">PV</span>\n",
        "            \n",
        "    <span class=\"c\"># Compute the standard errors for all predictors</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">StdError_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
        "        <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rse_</span>  <span class=\"c\"># estimator for std of errors</span>\n",
        "        <span class=\"n\">Xinvdiag</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_norm_inv_</span><span class=\"p\">)</span>\n",
        "              \n",
        "        <span class=\"n\">SE</span>  <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
        "        <span class=\"k\">for</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">Xinvdiag</span><span class=\"p\">:</span>\n",
        "            <span class=\"n\">SE_i</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n",
        "            <span class=\"n\">SE</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">SE_i</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">return</span> <span class=\"n\">SE</span>\n",
        "\n",
        "\n",
        " \n",
        "<span class=\"k\">class</span> <span class=\"nc\">MyRidgeRegressor</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
        "    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">lamda</span><span class=\"p\">,</span><span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Fits estimator to data. &quot;&quot;&quot;</span>\n",
        "\n",
        "        <span class=\"n\">p</span>                <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> \n",
        "        <span class=\"n\">Xlambda</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lamda</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n",
        "        <span class=\"n\">inv</span>              <span class=\"o\">=</span> <span class=\"n\">la</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">Xlambda</span><span class=\"p\">)</span>\n",
        "\n",
        "        <span class=\"k\">if</span> <span class=\"n\">fit_intercept</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>  <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n",
        "        <span class=\"k\">else</span><span class=\"p\">:</span>\n",
        "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n",
        "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span>       <span class=\"o\">=</span> <span class=\"n\">inv</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span><span class=\"p\">))</span>\n",
        "\n",
        "        <span class=\"c\"># set state of ``self``</span>\n",
        "        <span class=\"k\">return</span> <span class=\"bp\">self</span>\n",
        "            \n",
        "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n",
        "        <span class=\"sd\">&quot;&quot;&quot;Predict response of ``X``. &quot;&quot;&quot;</span>\n",
        "        <span class=\"c\"># compute predictions ``pred``</span>\n",
        "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">intercept_</span>\n",
        "        <span class=\"k\">return</span> <span class=\"n\">pred</span>\n",
        "\n",
        "   \n",
        " \n",
        "<span class=\"c\"># Computing standard error (intercept and slope) for simple linear regression</span>\n",
        "<span class=\"k\">def</span> <span class=\"nf\">StdError</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">):</span>\n",
        "    <span class=\"n\">ypred</span>         <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_est</span>     <span class=\"o\">=</span> <span class=\"n\">RSE</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">ypred</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">=</span> <span class=\"n\">sigma_est</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">n</span>     <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
        "    <span class=\"k\">print</span> <span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n",
        "    <span class=\"n\">x</span>          <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"s\">&#39;TV&#39;</span><span class=\"p\">]</span>\n",
        "    <span class=\"n\">xmean</span>      <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
        "    <span class=\"n\">xmean_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">xmean</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n",
        "    <span class=\"n\">xt</span>         <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"n\">xmean</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n",
        "\n",
        "    <span class=\"n\">SE_B0_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">];</span>\n",
        "    <span class=\"n\">SE_B1_sqr1</span> <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"n\">Xinvdiag</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">];</span>\n",
        "    \n",
        "    <span class=\"n\">SE_B0_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span><span class=\"o\">*</span><span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">xmean_sqr</span><span class=\"o\">/</span><span class=\"n\">xt</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n",
        "    <span class=\"n\">SE_B1_sqr</span>  <span class=\"o\">=</span> <span class=\"n\">sigma_sqr_est</span> <span class=\"o\">/</span><span class=\"n\">xt</span>\n",
        "\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B0 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B0_sqr</span>\n",
        "    <span class=\"k\">print</span> <span class=\"s\">&#39;B1 &#39;</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr1</span><span class=\"p\">,</span><span class=\"n\">SE_B1_sqr</span>\n",
        "    \n",
        "    <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B0_sqr</span><span class=\"p\">),</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">SE_B1_sqr</span><span class=\"p\">)]</span>\n",
        "  \n",
        "</pre></div>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<IPython.core.display.HTML at 0x16e62ba8>"
       ]
      }
     ],
     "prompt_number": 23
    }
   ],
   "metadata": {}
  }
 ]
}