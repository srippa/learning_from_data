{
 "metadata": {
  "name": "",
  "signature": "sha256:4371ff03022318254a07cf326796971455bac15fba4526d90268215f93192cc4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning from data notebooks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "During the past few years I got studied topics in machine learning and I decided to summarize the lessons I learned in a serie of  [IPython notebooks](http://ipython.org/notebook.html) containing selected parts of the theory, short code sniplets and few plots to demonstrate and highlight selected parts. In most cases I use the standard Python data stack including: [numpy](http://www.numpy.org/),[matplotlib](http://matplotlib.org/), [statsmodels](http://statsmodels.sourceforge.net/), [pandas](http://pandas.pydata.org/) and [scikit-learn](http://scikit-learn.org/stable/). Sometimes I will add new interesting libraries like, e.g. [seaborn](http://stanford.edu/~mwaskom/software/seaborn/). \n",
      "\n",
      "In few occasions I will implement a method from scratch. In such cases, I always prefer clarity of presentation over performance or numerical stability. For example when solving least-square problems I invert the normal equations which is e seriously flawed approach from numerical stability point of view.  Thus my homemade implementations are **not** recommended for practical use. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Index\n",
      "\n",
      "* **[C01 - Introduction](C01_learning_from_data_introduction.ipynb)**:  ** <font color='green'>Problem formulation, No-free-lunch theorems, Bias-Variance, PAC-learning, Overfitting, Curse of dimensionality, Model selection, Feature selection, Dimensionality reduction, Cost functions, Feature scaling, Assesing performance of models, Ensemble methods, Resources: courses, sites, competitions, books</font>**\n",
      "\n",
      "\n",
      "## In preparation\n",
      "* **Linear Regression** - Ordinary least sequares, ridge regression, LASSO, feature selection, PCA, radial basis functions\n",
      "* **Linear classification** - Logistic regression, Optimization using Newton iterations, Gradient descent, perceptron , Stochastic Gradient Descent\n",
      "* **SVM and RBF** - Linear separable case, Non linear separable case, non-separable case\n",
      "* **Naive Bayes **\n",
      "* **Decision trees** -- impurity measures, tree growing, prunning, bagging, random forests, boosting, feature selection with trees\n",
      "* **Resources**) - Tools, Books, Courses, Software, Links"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}