{
 "metadata": {
  "name": "",
  "signature": "sha256:5eb2091e216b4507273dbc6b13a97710a3ad3b557eb5d339044e4dbbbd322397"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning from data notebooks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "During past years I got interested in learning various data science topics and I decided to summarize the lessons I learned as serie of [IPYthon notebooks](http://ipython.org/notebook.html) with added code sniplets and plots to demonstrate and highlight selected parts. In most cases I use the standard Python libraries for data science computations: [numpy](http://www.numpy.org/),[matplotlib](http://matplotlib.org/), [statsmodels](http://statsmodels.sourceforge.net/), [pandas](http://pandas.pydata.org/) and [scikit-learn](http://scikit-learn.org/stable/). Sometimes I will add new interesting libraries like, e.g. [seaborn](http://stanford.edu/~mwaskom/software/seaborn/). \n",
      "\n",
      "In few occasions I will implement a method from scratch. In such cases, I always prefer clarity of presentation over performance ornumerical stability. For example I invert the normal equations whensolving least-square problems which is e seriously flawed approach from numerical stability point of view.  Thus my homemade implementations e **not** recommended for practicalk usage - use code from a respected library like one of those mensionned above. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Index\n",
      "\n",
      "* **[Section 1 - Introduction](C01_learning_from_data_introduction.ipynb)**:  ** <font color='green'>Problem formulation, No-free-lunch theorems, Bias-Variance, PAC-learning, Overfitting, Curse of dimensionality, Model selection Cost functions</font>**\n",
      "\n",
      "\n",
      "## In preparation\n",
      "* **Measuring performance** - Test error , Bias-Variance\n",
      "* **Dimensionality reduction** Feature engineering, feature selection, PCA\n",
      "* **Linear Regression** - Ordinary least sequares, ridge regression, LASSO, feature selection, PCA, radial basis functions\n",
      "* **Linear classification** - Logistic regression, Optimization using Newton iterations, Gradient descent, perceptron , Stochastic Gradient Descent\n",
      "* **SVM and RBF** - Linear separable case, Non linear separable case, non-separable case\n",
      "* **Naive Bayes **\n",
      "* **Decision trees** -- impurity measures, tree growing, prunning, bagging, random forests, boosting, feature selection with trees\n",
      "* **Resources**) - Tools, Books, Courses, Software, Links"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}